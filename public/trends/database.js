var database = {
  header: ['System','Domain','Task','Organization(s)','Organization Categorization','Author(s)','Publication date','Year','Reference','Link','Citations','Inclusion criteria','Parameters','Training compute (FLOPs)','Training dataset','Training dataset size (datapoints)','Hidden layers','Inference compute (FLOPs)','Equivalent training time (hours)','Inference time (ms)','Training dataset size (GB)','Approach','Dense or sparse model','Training objective','Training cost (2020 USD)','Compute Sponsor Categorization'],
  rows: [
    {
      'System': '',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'University of Oxford',
      'Organization Categorization': 'Academia',
      'Author(s)': 'S Wu, C Rupprecht, A Vedaldi',
      'Publication date': '25/11/2019',
      'Year': '2019',
      'Reference': 'Unsupervised Learning of Probably Symmetric Deformable 3D Objects From Images in the Wild',
      'Link': 'https://arxiv.org/abs/1911.11130',
      'Citations': '6.60E+01',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Theseus',
      'Domain': 'Other',
      'Task': 'Maze solving',
      'Organization(s)': 'Bell Laboratories',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Claude Shannon',
      'Publication date': '02/07/1950',
      'Year': '1950',
      'Reference': 'Mighty Mouse',
      'Link': 'https://www.technologyreview.com/2018/12/19/138508/mighty-mouse/',
      'Citations': 'N/A',
      'Inclusion criteria': 'Historical relevance',
      'Parameters': '4.00E+01',
      'Training compute (FLOPs)': '4.0E+01',
      'Training dataset': '',
      'Training dataset size (datapoints)': '4.00E+01',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'SNARC',
      'Domain': 'Other',
      'Task': 'Maze solving',
      'Organization(s)': 'Harvard University Psychological Laboratories',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Marvin Minsky',
      'Publication date': '08/01/1952',
      'Year': '1952',
      'Reference': 'A Neural-Analogue Calculator Based upon a Probability Model of Reinforcement',
      'Link': 'https://en.wikipedia.org/wiki/Stochastic_neural_analog_reinforcement_calculator',
      'Citations': '3.30E+01',
      'Inclusion criteria': 'Historical relevance',
      'Parameters': '4.00E+01',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Institute for Advanced Study',
      'Organization Categorization': '',
      'Author(s)': 'NA Barricelli',
      'Publication date': '02/07/1954',
      'Year': '1954',
      'Reference': 'Esempi numerici di processi di evoluzione',
      'Link': 'https://link.springer.com/article/10.1007/BF01556771',
      'Citations': '2.66E+02',
      'Inclusion criteria': 'Historical relevance',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': '',
    },
    {
      'System': 'Self Organizing System',
      'Domain': 'Vision',
      'Task': 'Pattern recognition',
      'Organization(s)': 'Massachusetts Institute of Technology',
      'Organization Categorization': 'Academia',
      'Author(s)': 'W. A. Clark and B. G. Farley',
      'Publication date': '01/03/1955',
      'Year': '1955',
      'Reference': 'Generalization of pattern recognition in a self-organizing system',
      'Link': 'https://dl.acm.org/doi/10.1145/1455292.1455309',
      'Citations': '8.30E+01',
      'Inclusion criteria': 'Historical relevance',
      'Parameters': '2.25E+02',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '2.56E+02',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Character recognition',
      'Organization(s)': 'Massachusetts Institute of Technology',
      'Organization Categorization': 'Academia',
      'Author(s)': 'O. G. Selfridge',
      'Publication date': '01/03/1955',
      'Year': '1955',
      'Reference': 'Pattern recognition and learning',
      'Link': 'https://dl.acm.org/doi/10.1145/1455292.1455310',
      'Citations': '2.90E+02',
      'Inclusion criteria': 'Historical relevance',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'Princeton University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'AM Uttley',
      'Publication date': '01/07/1956',
      'Year': '1956',
      'Reference': 'Conditional probability machines',
      'Link': 'https://www.moma.org/collection/works/illustratedbooks/16252?locale=es',
      'Citations': '8.40E+01',
      'Inclusion criteria': 'Historical relevance',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Perceptron Mark I',
      'Domain': 'Vision',
      'Task': 'Binary classification',
      'Organization(s)': 'Cornell Aeronautical Laboratory',
      'Organization Categorization': 'Industry',
      'Author(s)': 'F Rosenblatt',
      'Publication date': '01/01/1957',
      'Year': '1957',
      'Reference': 'The Perceptron—a perceiving and recognizing automaton',
      'Link': 'https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf',
      'Citations': '1.61E+03',
      'Inclusion criteria': 'Historical relevance',
      'Parameters': '4.00E+02',
      'Training compute (FLOPs)': '6.9E+05',
      'Training dataset': '',
      'Training dataset size (datapoints)': '6.00E+00',
      'Hidden layers': 'N/A',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': 'N/A',
      'Training cost (2020 USD)': 'N/A',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Samuel Neural Checkers',
      'Domain': 'Games',
      'Task': 'Checkers',
      'Organization(s)': 'IBM',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Arthur L. Samuel',
      'Publication date': '01/07/1959',
      'Year': '1959',
      'Reference': 'Some studies in machine learning using the game of checkers',
      'Link': 'https://ieeexplore.ieee.org/abstract/document/5392560',
      'Citations': '4.15E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.60E+01',
      'Training compute (FLOPs)': '4.3E+08',
      'Training dataset': '',
      'Training dataset size (datapoints)': '5.30E+04',
      'Hidden layers': 'N/A',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '9',
      'Inference time (ms)': '7.5',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Character recognition',
      'Organization(s)': 'Sandia Corporation',
      'Organization Categorization': 'Industry',
      'Author(s)': 'W. W. Bledsoe, I. Browning',
      'Publication date': '01/12/1959',
      'Year': '1959',
      'Reference': 'Pattern recognition and reading by machine',
      'Link': 'https://www.computer.org/csdl/proceedings-article/afips/1959/50550225/12OmNqN6R43',
      'Citations': '5.53E+02',
      'Inclusion criteria': 'Historical relevance',
      'Parameters': '2.63E+03',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Pandemonium (morse)',
      'Domain': 'Other',
      'Task': 'Morse translation',
      'Organization(s)': 'Massachusetts Institute of Technology',
      'Organization Categorization': 'Academia',
      'Author(s)': 'OG Selfridge',
      'Publication date': '01/02/1959',
      'Year': '1959',
      'Reference': 'Pandemonium: A Paradigm for Learning',
      'Link': 'https://aitopics.org/doc/classics:504E1BAC/',
      'Citations': '1.45E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '3.00E+03',
      'Training compute (FLOPs)': '6.0E+08',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'ADALINE',
      'Domain': 'Vision',
      'Task': 'Pattern recognition',
      'Organization(s)': 'Standford University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Widrow and Hoff',
      'Publication date': '30/06/1960',
      'Year': '1960',
      'Reference': 'Adaptive switching circuits',
      'Link': 'https://isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf',
      'Citations': '6.33E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.70E+01',
      'Training compute (FLOPs)': '9.9E+03',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.00E+02',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '3.30E+01',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'LMS',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Standford University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Widrow and Hoff',
      'Publication date': '30/06/1960',
      'Year': '1960',
      'Reference': 'Adaptive switching circuits (technical report)',
      'Link': 'https://www.scirp.org/(S(351jmbntvnsjt1aadkposzje))/reference/ReferencesPapers.aspx?ReferenceID=547230',
      'Citations': '6.33E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Massachusetts Institute of Technology (MIT)',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Marvin Minsky',
      'Publication date': '01/01/1961',
      'Year': '1961',
      'Reference': 'Steps Toward Artificial Intelligence',
      'Link': 'https://ieeexplore.ieee.org/abstract/document/4066245',
      'Citations': '2.43E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Massachusetts Institute of Technology (MIT)',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Marvin Minsky and Oliver G. Selfridge',
      'Publication date': '01/07/1961',
      'Year': '1961',
      'Reference': 'Learning in random nets',
      'Link': 'https://stacks.stanford.edu/file/druid:yr384hg3073/yr384hg3073.pdf',
      'Citations': '4.70E+01',
      'Inclusion criteria': 'Historical relevance',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': 'Binary classification',
      'Organization(s)': 'The University of Genoa',
      'Organization Categorization': 'Academia',
      'Author(s)': 'A Gamba, L Gamberini, G Palmieri, R Sanna',
      'Publication date': '01/09/1961',
      'Year': '1961',
      'Reference': 'Further experiments with PAPA',
      'Link': 'https://www.semanticscholar.org/paper/Further-experiments-with-PAPA-Gamba-Gamberini/c3a20b9aa86033cec29f08e69f4bc81e8b329ae2',
      'Citations': '2.10E+01',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'MADALINE I',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Standford University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'William Combs Ridgway',
      'Publication date': '01/07/1962',
      'Year': '1962',
      'Reference': 'An adaptive logic system with generalizing properties',
      'Link': 'https://www.proquest.com/openview/7898314db50a218b58052ac91e3bde1e/1?',
      'Citations': '7.50E+01',
      'Inclusion criteria': 'Historical relevance',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'STeLLA',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'University of Canterbury',
      'Organization Categorization': 'Academia',
      'Author(s)': 'J.H. Andreae and Peter L. Joyce',
      'Publication date': '01/06/1963',
      'Year': '1963',
      'Reference': 'STeLLA: A Scheme for a Learning Machine',
      'Link': 'https://www.researchgate.net/publication/252919025_STELLA_A_scheme_for_a_learning_machine',
      'Citations': '3.40E+01',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'MENACE',
      'Domain': 'Games',
      'Task': 'Tic Tac Toe',
      'Organization(s)': 'University of Edinburgh',
      'Organization Categorization': 'Academia',
      'Author(s)': 'University of Edinburgh',
      'Publication date': '01/11/1963',
      'Year': '1963',
      'Reference': 'Experiments on the Mechanization of Game-Learning Part I. Characterization of the Model and its parameters',
      'Link': 'https://academic.oup.com/comjnl/article/6/3/232/360077',
      'Citations': '4.60E+01',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Samuel Neural Checkers II',
      'Domain': 'Games',
      'Task': 'Checkers',
      'Organization(s)': 'University of Geneva',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Palmieri, G. and R. Sanna',
      'Publication date': '01/11/1967',
      'Year': '1967',
      'Reference': 'Some studies in machine learning using the game of checkers. Part II',
      'Link': 'https://www.cs.virginia.edu/~evans/greatworks/samuel.pdf',
      'Citations': '7.47E+02',
      'Inclusion criteria': '',
      'Parameters': '4.00E+01',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': 'N/A',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'GLEE',
      'Domain': 'Games',
      'Task': 'Tic Tac Toe',
      'Organization(s)': 'University of Edinburgh',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Michie and Chambers',
      'Publication date': '01/07/1968',
      'Year': '1968',
      'Reference': 'Boxes: An Experiment in Adaptive Control',
      'Link': 'https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.474.2430',
      'Citations': '5.90E+02',
      'Inclusion criteria': 'Historical relevance',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'BOXES',
      'Domain': 'Games',
      'Task': 'Pole balancing',
      'Organization(s)': 'University of Edinburgh',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Michie and Chambers',
      'Publication date': '01/07/1968',
      'Year': '1968',
      'Reference': 'Boxes: An Experiment in Adaptive Control',
      'Link': 'https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.474.2430',
      'Citations': '5.90E+02',
      'Inclusion criteria': 'Historical relevance',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Massachusetts Institute of Technology',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Patrick Winston',
      'Publication date': '01/09/1970',
      'Year': '1970',
      'Reference': 'Learning Structural Definitions from Examples',
      'Link': 'https://dspace.mit.edu/handle/1721.1/6884',
      'Citations': '1.81E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Naive Bayes',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'Massachusetts Institute of Technology',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Duda and Hart',
      'Publication date': '01/09/1974',
      'Year': '1974',
      'Reference': 'Pattern Classification and Scene Analysis',
      'Link': 'https://www.semanticscholar.org/paper/Pattern-classification-and-scene-analysis-Duda-Hart/b07ce649d6f6eb636872527104b0209d3edc8188',
      'Citations': '2.31E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Bootstrap Adaptation',
      'Domain': 'Games',
      'Task': 'Blackjack',
      'Organization(s)': 'IEEE',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Widrow, Gupta, and Maitra',
      'Publication date': '09/1973',
      'Year': '1973',
      'Reference': 'Punish/Reward: Learning with a Critic in Adaptive Threshold Systems',
      'Link': 'https://ieeexplore.ieee.org/document/4309272',
      'Citations': '3.82E+02',
      'Inclusion criteria': '',
      'Parameters': '2.10E+01',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Cognitron',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Biological Cybernetics',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Kunihiko Fukushima',
      'Publication date': '09/1975',
      'Year': '1975',
      'Reference': 'Cognitron: a self-organizing multilayered neural network',
      'Link': 'https://link.springer.com/article/10.1007%2FBF00342633',
      'Citations': '7.91E+02',
      'Inclusion criteria': 'Historical relevance',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'TD(0)',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'University of Essex',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Ian Witten',
      'Publication date': '01/08/1977',
      'Year': '1977',
      'Reference': 'An adaptive optimal controller for discrete-time Markov environments',
      'Link': 'https://www.sciencedirect.com/science/article/pii/S0019995877903540',
      'Citations': '2.44E+02',
      'Inclusion criteria': 'Historical relevance',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'Utrecht University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Koenderink & van Doom',
      'Publication date': '5/1979',
      'Year': '1979',
      'Reference': 'The internal representation of solid shape with respect to vision',
      'Link': 'https://link.springer.com/article/10.1007/BF00337644',
      'Citations': '9.81E+02',
      'Inclusion criteria': 'Historical relevance',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Neocognitron',
      'Domain': 'Vision',
      'Task': 'Character recognition',
      'Organization(s)': 'NHK Broadcasting Science Research Laboratories',
      'Organization Categorization': 'Industry',
      'Author(s)': 'K Fukushima, S Miyake',
      'Publication date': '04/1980',
      'Year': '1980',
      'Reference': 'Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position',
      'Link': 'https://link.springer.com/article/10.1007/BF00344251',
      'Citations': '5.78E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.14E+06',
      'Training compute (FLOPs)': '2.3E+08',
      'Training dataset': '',
      'Training dataset size (datapoints)': '5.00E+00',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Kohonen network',
      'Domain': 'Other',
      'Task': 'Dimensionality reduction',
      'Organization(s)': 'Helsinki University of Technology',
      'Organization Categorization': 'Academia',
      'Author(s)': 'T Kohonen',
      'Publication date': '25/07/1981',
      'Year': '1981',
      'Reference': 'Self-organized formation of topologically correct feature maps',
      'Link': 'https://link.springer.com/article/10.1007/BF00337288',
      'Citations': '1.18E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '4.10E+03',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Hopfield network',
      'Domain': 'Other',
      'Task': 'Sequence memorization',
      'Organization(s)': 'California Institute of Technology',
      'Organization Categorization': 'Academia',
      'Author(s)': 'JJ Hopfield',
      'Publication date': '01/04/1982',
      'Year': '1982',
      'Reference': 'Neural networks and physical systems with emergent collective computational abilities',
      'Link': 'https://www.pnas.org/doi/10.1073/pnas.79.8.2554',
      'Citations': '2.33E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '9.90E+03',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': 'N/A',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'ASE+ACE',
      'Domain': 'Games',
      'Task': 'Pole balancing',
      'Organization(s)': 'Stanford',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson',
      'Publication date': '09/1983',
      'Year': '1983',
      'Reference': 'Neuronlike adaptive elements that can solve difficult learning control problems',
      'Link': 'https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6313077',
      'Citations': '4.30E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '3.24E+02',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'MIT',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Steven Pinker',
      'Publication date': '01/07/1984',
      'Year': '1984',
      'Reference': 'Language learnability and language development.',
      'Link': 'https://psycnet.apa.org/record/1985-97439-000',
      'Citations': '4.73E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'University of California and University of Carnegie Mellon',
      'Organization Categorization': 'Academia',
      'Author(s)': 'D. E. Rumelhart, G. E. Hinton, and R. J. Williams',
      'Publication date': '01/1986',
      'Year': '1986',
      'Reference': 'Learning internal representations by error propagation',
      'Link': 'https://dl.acm.org/doi/10.5555/104279.104293',
      'Citations': '2.73E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'Massachusetts Institute of Technology',
      'Organization Categorization': 'Academia',
      'Author(s)': 'John Canny',
      'Publication date': '11/1986',
      'Year': '1986',
      'Reference': 'A Computational Approach To Edge Detection',
      'Link': 'https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4767851',
      'Citations': '3.79E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'University of California',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Jordan, M.I.',
      'Publication date': '01/05/1986',
      'Year': '1986',
      'Reference': 'Serial order: A parallel distributed processing approach',
      'Link': 'https://www.osti.gov/biblio/6910294',
      'Citations': '1.50E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Back-propagation',
      'Domain': 'Other',
      'Task': 'Learning to complete triples',
      'Organization(s)': 'University of California',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Rumelhart, David E.; Hinton, Geoffrey E.; Williams, Ronald J.',
      'Publication date': '01/10/1986',
      'Year': '1986',
      'Reference': 'Learning representations by back-propagating errors',
      'Link': 'https://www.semanticscholar.org/paper/Learning-representations-by-back-propagating-errors-Rumelhart-Hinton/052b1d8ce63b07fec3de9dbb583772d860b7c769',
      'Citations': '2.53E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.44E+02',
      'Training compute (FLOPs)': '1.2E+08',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.44E+02',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '2.88E+02',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': 'Unsupervised',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': 'Verb conjugation',
      'Organization(s)': 'Stanford',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Rumelhart, D. E., & McClelland, J. L',
      'Publication date': '03/01/1986',
      'Year': '1986',
      'Reference': 'Learning the past tenses of English verbs: Implicit rules or parallel distributed processing?',
      'Link': 'https://www.semanticscholar.org/paper/On-learning-the-past-tenses-of-English-verbs%3A-rules-Rumelhart-McClelland/4fa569625b5ab35e955a8d5be11a4aa9f59ca424',
      'Citations': '3.06E+02',
      'Inclusion criteria': '',
      'Parameters': '2.12E+05',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'University of California, Santa Cruz',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Biederman, Irving',
      'Publication date': '01/04/1987',
      'Year': '1987',
      'Reference': 'Recognition-by-components: A theory of human image understanding',
      'Link': 'https://psycnet.apa.org/record/1987-20898-001',
      'Citations': '7.59E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'NetTalk',
      'Domain': 'Speech',
      'Task': 'Speech synthesis',
      'Organization(s)': 'Princeton University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'TJ Sejnowski, CR Rosenberg',
      'Publication date': '06/06/1987',
      'Year': '1987',
      'Reference': 'Parallel Networks that Learn to Pronounce English Text',
      'Link': 'http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=03A3D3EDF0BAF35405ABCF083411B55E?doi=10.1.1.154.7012&rep=rep1&type=pdf',
      'Citations': '2.56E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.86E+04',
      'Training compute (FLOPs)': '8.1E+10',
      'Training dataset': '',
      'Training dataset size (datapoints)': '2.10E+04',
      'Hidden layers': '1',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'MADALINE II',
      'Domain': 'Other',
      'Task': 'Pattern classification',
      'Organization(s)': 'Stanford University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Rodney Winter, Bernard Widrow',
      'Publication date': '24/07/1988',
      'Year': '1988',
      'Reference': 'MADALINE RULE II: A Training Algorithm for Neural Networks',
      'Link': 'https://ieeexplore.ieee.org/document/23872',
      'Citations': '8.10E+01',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'Roke Manor Research',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Harris & Stephens',
      'Publication date': '01/07/1988',
      'Year': '1988',
      'Reference': 'A Combined Corner and Edge Detector',
      'Link': 'http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.434.4816&rep=rep1&type=pdf',
      'Citations': '1.91E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.50E+03',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Adaptive Broom Balancer',
      'Domain': 'Games',
      'Task': 'Pole balancing',
      'Organization(s)': 'Stanford University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'VV Tolat, B Widrow',
      'Publication date': '24/07/1988',
      'Year': '1988',
      'Reference': 'An Adaptive “Broom Balancer” with Visual Inputs',
      'Link': 'https://ieeexplore.ieee.org/document/23982',
      'Citations': '8.00E+01',
      'Inclusion criteria': '',
      'Parameters': '1.10E+02',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Technische Universität Wien Austria & University of California',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Kurt Hornik & Maxwell Stinchcombe & Halbert White',
      'Publication date': '09/03/1989',
      'Year': '1989',
      'Reference': 'Multilayer feedforward networks are universal approximators',
      'Link': 'https://www.sciencedirect.com/science/article/abs/pii/0893608089900208',
      'Citations': '2.17E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Time-delay neural networks',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Carnegie Mellon University & ATR Interpreting Telephony Research Laboratories & University of Toronto ',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang',
      'Publication date': '03/03/1989',
      'Year': '1989',
      'Reference': 'Phoneme recognition using time-delay neural networks',
      'Link': 'https://ieeexplore.ieee.org/abstract/document/21701',
      'Citations': '3.45E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Q-learning',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'University of London',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Christopher Watkins',
      'Publication date': '01/01/1989',
      'Year': '1989',
      'Reference': 'Learning from delayed rewards',
      'Link': 'http://www.cs.rhul.ac.uk/~chrisw/thesis.html',
      'Citations': '8.03E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Zip CNN',
      'Domain': 'Vision',
      'Task': 'Character recognition',
      'Organization(s)': 'AT&T Bell Laboratories',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Y. LeCun B. Boser J. S. Denker D. Henderson R. E. Howard W. Hubbard L. D. Jackel',
      'Publication date': '01/12/1989',
      'Year': '1989',
      'Reference': 'Backpropagation applied to handwritten zip code recognition',
      'Link': 'https://ieeexplore.ieee.org/document/6795724',
      'Citations': '9.05E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '9.76E+03',
      'Training compute (FLOPs)': '4.3E+10',
      'Training dataset': 'Buffalo zips',
      'Training dataset size (datapoints)': '7.29E+03',
      'Hidden layers': '3',
      'Inference compute (FLOPs)': '1.29E+05',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'ALVINN',
      'Domain': 'Driving',
      'Task': '',
      'Organization(s)': 'Carnegie Mellon University ',
      'Organization Categorization': 'Academia',
      'Author(s)': 'DA Pomerleau',
      'Publication date': '01/12/1989',
      'Year': '1989',
      'Reference': 'ALVINN: an autonomous land vehicle in a neural network',
      'Link': 'https://proceedings.neurips.cc/paper/1988/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html',
      'Citations': '1.58E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '3.99E+03',
      'Training compute (FLOPs)': '8.1E+10',
      'Training dataset': 'Road snapshots',
      'Training dataset size (datapoints)': '1.20E+03',
      'Hidden layers': '1',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Air Force Institute of Technology, OH, USA',
      'Organization Categorization': 'Academia',
      'Author(s)': 'D.W. Ruck & S.K. Rogers & M. Kabrisky & M.E. Oxley & B.W. Suter',
      'Publication date': '01/12/1990',
      'Year': '1990',
      'Reference': 'The multilayer perceptron as an approximation to a Bayes optimal discriminant function',
      'Link': 'https://ieeexplore.ieee.org/abstract/document/80266',
      'Citations': '1.05E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'MADALINE III',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'University of Stanford',
      'Organization Categorization': 'Academia',
      'Author(s)': 'B Widrow, M. A. Lehr',
      'Publication date': '01/09/1990',
      'Year': '1990',
      'Reference': '30 years of adaptive neural networks: perceptron, madaline, and backpropagation',
      'Link': 'https://ieeexplore.ieee.org/document/58323',
      'Citations': '3.01E+03',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'DIABETES',
      'Domain': 'Other',
      'Task': 'Medical diagnosis',
      'Organization(s)': 'Aalborg University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'S. Andreassen, R. Hovorka, J. Benn, K. G. Olesen, and E. R. Carson',
      'Publication date': '24/06/1991',
      'Year': '1991',
      'Reference': 'A Model-based Approach to Insulin Adjustment',
      'Link': 'https://link.springer.com/chapter/10.1007/978-3-642-48650-0_19',
      'Citations': '1.32E+02',
      'Inclusion criteria': '',
      'Parameters': '4.29E+05',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'University of California, San Diego',
      'Organization Categorization': 'Academia',
      'Author(s)': 'J. L. Elman',
      'Publication date': '01/09/1991',
      'Year': '1991',
      'Reference': 'Distributed representations, simple recurrent networks, and grammatical structure',
      'Link': 'https://dl.acm.org/doi/10.1007/BF00114844',
      'Citations': '1.72E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.78E+05',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Northeastern University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'R. J. Williams',
      'Publication date': '01/05/1992',
      'Year': '1992',
      'Reference': 'Simple statistical gradient-following algorithms for connectionist reinforcement learning',
      'Link': 'https://dl.acm.org/doi/10.1007/BF00992696',
      'Citations': '6.53E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'TD-Gammon',
      'Domain': 'Games',
      'Task': 'Backgammon',
      'Organization(s)': 'IBM',
      'Organization Categorization': 'Industry',
      'Author(s)': 'G Tesauro',
      'Publication date': '01/05/1992',
      'Year': '1992',
      'Reference': 'Practical Issues in Temporal Difference Learning',
      'Link': 'https://papers.nips.cc/paper/1991/file/68ce199ec2c5517597ce0a4d89620f55-Paper.pdf',
      'Citations': '1.34E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '2.50E+04',
      'Training compute (FLOPs)': '1.8E+13',
      'Training dataset': '',
      'Training dataset size (datapoints)': '6.30E+06',
      'Hidden layers': '1',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Fuzzy NN',
      'Domain': 'Speech',
      'Task': 'Speech recognition',
      'Organization(s)': 'Indian Statistical Institute',
      'Organization Categorization': 'Academia',
      'Author(s)': 'SK Pal, S Mitra',
      'Publication date': '01/09/1992',
      'Year': '1992',
      'Reference': 'Multilayer perceptron, fuzzy sets, and classification',
      'Link': 'https://ieeexplore.ieee.org/document/159058',
      'Citations': '1.22E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.17E+03',
      'Training compute (FLOPs)': '3.0E+06',
      'Training dataset': '',
      'Training dataset size (datapoints)': '8.71E+02',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'IBM-5',
      'Domain': 'Language',
      'Task': 'Translation',
      'Organization(s)': 'IBM T.J. Watson Research Center',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, Robert L. Mercer',
      'Publication date': '15/06/1993',
      'Year': '1993',
      'Reference': 'The Mathematics of Statistical Machine Translation: Parameter Estimation',
      'Link': 'https://dl.acm.org/doi/10.5555/972470.972474',
      'Citations': '5.75E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.66E+06',
      'Training compute (FLOPs)': '',
      'Training dataset': 'Proceedings of the Canadian parliament',
      'Training dataset size (datapoints)': '5.34E+07',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': 'Part-of-speech tagging',
      'Organization(s)': 'EURECOM',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Bernard Merialdo',
      'Publication date': '01/06/1994',
      'Year': '1994',
      'Reference': 'Tagging English Text with a Probabilistic Model',
      'Link': 'https://dl.acm.org/doi/10.5555/972525.972526',
      'Citations': '7.88E+02',
      'Inclusion criteria': '',
      'Parameters': '2.45E+06',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.00E+06',
      'Hidden layers': 'N/A',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'GroupLens',
      'Domain': 'Recommendation',
      'Task': '',
      'Organization(s)': 'Massachusetts Institute of Technology',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Peter Bergstrom, John Riedl',
      'Publication date': '22/10/1994',
      'Year': '1994',
      'Reference': 'GroupLens : an Open Architecture for Collaborative Filtering of Netnews',
      'Link': 'https://dl.acm.org/doi/10.1145/192844.192905',
      'Citations': '7.73E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.00E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.00E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Support Vector Machines',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'AT&T Bell Laboratories',
      'Organization Categorization': 'Industry',
      'Author(s)': 'C Cortes, V Vapnik',
      'Publication date': '09/1995',
      'Year': '1995',
      'Reference': 'Support-Vector Networks',
      'Link': 'https://link.springer.com/article/10.1007/BF00994018',
      'Citations': '4.90E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.00E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': 'MNIST',
      'Training dataset size (datapoints)': '6.00E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'University of Pennsylvania',
      'Organization Categorization': 'Academia',
      'Author(s)': 'D Yarowsky',
      'Publication date': '26/06/1995',
      'Year': '1995',
      'Reference': 'Unsupervised Word Sense Disambiguation Rivaling Supervised Methods',
      'Link': 'https://dl.acm.org/doi/10.3115/981658.981684',
      'Citations': '3.00E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '4.60E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Xerox',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Eric Saund',
      'Publication date': '01/01/1995',
      'Year': '1995',
      'Reference': 'A Multiple Cause Mixture Model for Unsupervised Learning',
      'Link': 'https://ieeexplore.ieee.org/document/6795568',
      'Citations': '1.76E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': 'Perplexity... sorta',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Random Decision Forests',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'AT&T Bell Laboratories',
      'Organization Categorization': 'Industry',
      'Author(s)': 'TK Ho',
      'Publication date': '14/08/1995',
      'Year': '1995',
      'Reference': 'Random decision forests',
      'Link': 'https://ieeexplore.ieee.org/document/598994',
      'Citations': '4.68E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': 'MNIST',
      'Training dataset size (datapoints)': '6.00E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'System 11',
      'Domain': 'Vision',
      'Task': 'Face recognition',
      'Organization(s)': 'Carnegie Mellon University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'HA Rowley, S Baluja, T Kanade',
      'Publication date': '18/06/1996',
      'Year': '1996',
      'Reference': 'Neural Network-Based Face Detection',
      'Link': 'https://ieeexplore.ieee.org/document/655647',
      'Citations': '6.01E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '6.45E+03',
      'Training compute (FLOPs)': '9.6E+08',
      'Training dataset': '',
      'Training dataset size (datapoints)': '9.05E+03',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '1.29E+04',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'HMM Word Alignment',
      'Domain': 'Language',
      'Task': 'Word alignment',
      'Organization(s)': 'University of Erlangen - Nuremburg',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Stephan Vogel, Hermann Ney, Christoph Tillmann',
      'Publication date': '05/08/1996',
      'Year': '1996',
      'Reference': 'HMM-Based Word Alignment in Statistical Translation',
      'Link': 'https://dl.acm.org/doi/10.3115/993268.993313',
      'Citations': '1.10E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '4.42E+05',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': 'Supervised',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'LSTM',
      'Domain': 'Other',
      'Task': 'Sequence recognition (?)',
      'Organization(s)': 'The Technical University of Munich',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Sepp Hochreiter ; Jurgen Schmidhuber',
      'Publication date': '15/11/1997',
      'Year': '1997',
      'Reference': 'Long short-term memory',
      'Link': 'https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext',
      'Citations': '5.20E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.05E+04',
      'Training compute (FLOPs)': '2.1E+13',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.27E+06',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '4.20E+04',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Cambridge University Engineering & Carnegie Mellon University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'P Clarkson, R Rosenfeld',
      'Publication date': '01/07/1997',
      'Year': '1997',
      'Reference': 'Statistical language modeling using the CMU-Cambridge toolkit',
      'Link': 'https://www.semanticscholar.org/paper/Statistical-language-modeling-using-the-toolkit-Clarkson-Rosenfeld/fdf4aa623e4d5b5edaeb873ed8e8b1cef0b59c87',
      'Citations': '9.54E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': 'Supervised',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'MIT',
      'Organization Categorization': 'Academia',
      'Author(s)': 'E. Osuna, R. Freund, F. Girosi',
      'Publication date': '17/06/1997',
      'Year': '1997',
      'Reference': 'Training Support Vector Machines: An Application to Face Detection',
      'Link': 'https://ieeexplore.ieee.org/document/609310',
      'Citations': '3.85E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '5.00E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'BRNN',
      'Domain': 'Speech',
      'Task': 'Speech recognition',
      'Organization(s)': 'ATR Labs, Japan',
      'Organization Categorization': 'Industry',
      'Author(s)': 'M. Schuster, KK Paliwal',
      'Publication date': '11/1997',
      'Year': '1997',
      'Reference': 'Bidirectional recurrent neural networks',
      'Link': 'https://ieeexplore.ieee.org/document/650093',
      'Citations': '6.09E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.30E+04',
      'Training compute (FLOPs)': '',
      'Training dataset': 'TIMIT',
      'Training dataset size (datapoints)': '7.39E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'UC Davis, Cornell',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Bruno A. Olshausen, David J. Field',
      'Publication date': '12/1997',
      'Year': '1997',
      'Reference': 'Sparse coding with an overcomplete basis set: A strategy employed by V1?',
      'Link': 'https://www.sciencedirect.com/science/article/pii/S0042698997001697',
      'Citations': '4.26E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.00E+01',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Speech',
      'Task': '',
      'Organization(s)': 'Johns Hopkins University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'F Jelinek',
      'Publication date': '01/15/1998',
      'Year': '1998',
      'Reference': 'Statistical Methods for Speech Recognition',
      'Link': 'https://mitpress.mit.edu/books/statistical-methods-speech-recognition',
      'Citations': '3.06E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Face recognition',
      'Organization(s)': 'Carnegie Mellon University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'H Schneiderman, T Kanade',
      'Publication date': '23/06/1998',
      'Year': '1998',
      'Reference': 'Probabilistic modeling of local appearance and spatial relationships for object recognition',
      'Link': 'https://ieeexplore.ieee.org/document/698586',
      'Citations': '6.02E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.20E+05',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Other',
      'Task': 'Recommender system',
      'Organization(s)': 'AT&T Labs and Rutgers University and Bell Communications Research',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'C Basu, H Hirsh, W Cohen',
      'Publication date': '01/07/1998',
      'Year': '1998',
      'Reference': 'Recommendation as Classification : Using Social and Content-based Information in Recommendation',
      'Link': 'https://www.aaai.org/Papers/AAAI/1998/AAAI98-101.pdf',
      'Citations': '1.56E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '4.50E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'RNN for speech',
      'Domain': 'Speech',
      'Task': 'Speech synthesis',
      'Organization(s)': 'National Chiao Tung University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'SH Chen, SH Hwang, YR Wang',
      'Publication date': '15/05/1998',
      'Year': '1998',
      'Reference': 'An RNN-based prosodic information synthesizer for Mandarin text-to-speech',
      'Link': 'https://ieeexplore.ieee.org/abstract/document/668817',
      'Citations': '2.31E+02',
      'Inclusion criteria': '',
      'Parameters': '7.51E+03',
      'Training compute (FLOPs)': '2.3E+11',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.41E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'LeNet-5',
      'Domain': 'Vision',
      'Task': 'Character recognition',
      'Organization(s)': 'AT&T Labs',
      'Organization Categorization': 'Industry - Academia Collaboration (Industry leaning)',
      'Author(s)': 'Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner',
      'Publication date': '01/11/1998',
      'Year': '1998',
      'Reference': 'Gradient-based Learning Applied to Document Recognition',
      'Link': 'http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf',
      'Citations': '3.86E+04',
      'Inclusion criteria': 'Historical relevance',
      'Parameters': '6.00E+04',
      'Training compute (FLOPs)': '2.8E+12',
      'Training dataset': 'MNIST',
      'Training dataset size (datapoints)': '6.00E+04',
      'Hidden layers': '6',
      'Inference compute (FLOPs)': '7.81E+05',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '0.021',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Character recognition',
      'Organization(s)': 'University of California San Diego & Shannon Laboratory, AT&T Labs',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Yoav Freund & Robert E. Schapire',
      'Publication date': '01/12/1999',
      'Year': '1999',
      'Reference': 'Large Margin Classification Using the Perceptron Algorithm',
      'Link': 'https://link.springer.com/article/10.1023/A:1007662407062',
      'Citations': '1.73E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': 'MNIST',
      'Training dataset size (datapoints)': '6.00E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'IBM Model 4',
      'Domain': 'Language',
      'Task': 'Translation',
      'Organization(s)': 'University of Southern California & IBM & University of Pennsylvania',
      'Organization Categorization': 'Industry - Academia Collaboration (Academia leaning)',
      'Author(s)': 'Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky',
      'Publication date': '02/07/1999',
      'Year': '1999',
      'Reference': 'Statistical machine translation',
      'Link': 'http://www-i6.informatik.rwth-aachen.de/publications/download/266/al-onaizan--1999.pdf',
      'Citations': '1.92E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '8.00E+05',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'IDSIA Switzerland',
      'Organization Categorization': 'Academia',
      'Author(s)': 'F. A. Gers, J. Schmidhuber, and F. Cummins',
      'Publication date': '02/01/1999',
      'Year': '1999',
      'Reference': 'Learning to forget: Continual prediction with LSTM',
      'Link': 'https://ieeexplore.ieee.org/document/818041',
      'Citations': '4.52E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '2.76E+02',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '3.00E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Recommendation',
      'Task': '',
      'Organization(s)': 'University of Minnesota',
      'Organization Categorization': 'Academia',
      'Author(s)': 'B Sarwar, G Karypis, J Konstan, J Riedl',
      'Publication date': '14/07/2000',
      'Year': '2000',
      'Reference': 'Application of Dimensionality Reduction in Recommender System -- A Case Study',
      'Link': 'http://robotics.stanford.edu/~ronnyk/WEBKDD2000/papers/sarwar.pdf',
      'Citations': '2.13E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Peephole LSTM',
      'Domain': 'Other',
      'Task': 'Periodic function approximation',
      'Organization(s)': 'IDSIA Switzerland',
      'Organization Categorization': 'Academia',
      'Author(s)': 'F.A. Gers; J. Schmidhuber',
      'Publication date': '26/07/2000',
      'Year': '2000',
      'Reference': 'Recurrent nets that time and count',
      'Link': 'https://ieeexplore.ieee.org/document/861302',
      'Citations': '6.30E+02',
      'Inclusion criteria': '',
      'Parameters': '1.70E+01',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '6.50E+07',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'University of Rochester',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Daniel Gildea, Daniel Jurafsky',
      'Publication date': '09/2000',
      'Year': '2000',
      'Reference': 'Automatic Labeling of Semantic Roles',
      'Link': 'https://dl.acm.org/doi/10.1162/089120102760275983',
      'Citations': '2.33E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': 'FrameNet',
      'Training dataset size (datapoints)': '5.00E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'RWTH Aachen - University of Technology',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Franz Josef Och, Hermann Ney',
      'Publication date': '10/2000',
      'Year': '2000',
      'Reference': 'Improved Statistical Alignment Models',
      'Link': 'https://aclanthology.org/P00-1056/',
      'Citations': '1.32E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Standford University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Jerome H. Friedman',
      'Publication date': '01/10/2001',
      'Year': '2001',
      'Reference': 'Greedy function approximation: A gradient boosting machine',
      'Link': 'https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full',
      'Citations': '1.43E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Immediate trihead',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Brown University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'E Charniak',
      'Publication date': '07/2001',
      'Year': '2001',
      'Reference': 'Immediate-Head Parsing for Language Models',
      'Link': 'https://dl.acm.org/doi/10.3115/1073012.1073029',
      'Citations': '4.22E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Decision tree (classification)',
      'Domain': 'Vision',
      'Task': 'Face recognition',
      'Organization(s)': 'Mitsubishi Electric Research Labs and Compaq CRL',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'P. Viola, M. Jones',
      'Publication date': '8/12/2001',
      'Year': '2001',
      'Reference': 'Rapid object detection using a boosted cascade of simple features',
      'Link': 'https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf',
      'Citations': '2.34E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.20E+08',
      'Training compute (FLOPs)': '6.3E+13',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.45E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '6.70E+07',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': 'Translation',
      'Organization(s)': 'RWTH Aachen and University of Southern California',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Franz Josef Och and Hermann Ney',
      'Publication date': '07/2002',
      'Year': '2002',
      'Reference': 'Discriminative Training and Maximum Entropy Models for Statistical Machine Translation',
      'Link': 'https://aclanthology.org/P02-1038/',
      'Citations': '1.41E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '5.20E+05',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'IBM TJ Watson Research Centre',
      'Organization Categorization': 'Industry',
      'Author(s)': 'K Papineni, S Roukos, T Ward, WJ Zhu',
      'Publication date': '06/07/2002',
      'Year': '2002',
      'Reference': 'Bleu: a method for automatic evaluation of machine translation',
      'Link': 'https://dl.acm.org/doi/10.3115/1073083.1073135',
      'Citations': '1.58E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Korea Advanced Institute of Science and Technology',
      'Organization Categorization': 'Academia',
      'Author(s)': 'YH Cho, JK Kim, SH Kim',
      'Publication date': '01/10/2002',
      'Year': '2002',
      'Reference': 'A personalized recommender system based on web usage mining and decision tree induction',
      'Link': 'https://reader.elsevier.com/reader/sd/pii/S0957417402000520?token=155B6D1937982D7D0271AFD1CFB034DFD7F3D1DE816B66C025EBC9D0A305BA6DA685DD62989DC05246C794CAC74CDAEF&originRegion=us-east-1&originCreation=20220325235441',
      'Citations': '6.56E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'AT&T Labs',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Michael Collins',
      'Publication date': '01/06/2002',
      'Year': '2002',
      'Reference': 'Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms',
      'Link': 'https://dl.acm.org/doi/10.3115/1118693.1118694',
      'Citations': '2.58E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'University of Southern California',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Daniel Marcu and William Wong',
      'Publication date': '06/2002',
      'Year': '2002',
      'Reference': 'A Phrase-Based, Joint Probability Model for Statistical Machine Translation',
      'Link': 'https://dl.acm.org/doi/10.3115/1118693.1118711',
      'Citations': '6.23E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': 'Hansard Corpus',
      'Training dataset size (datapoints)': '1.07E+06',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'IDSIA Switzerland',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Justin Bayer, Daan Wierstra, Julian Togelius, Jürgen Schmidhuber',
      'Publication date': '01/06/2002',
      'Year': '2002',
      'Reference': 'Evolving Neural Networks through Augmenting Topologies ',
      'Link': 'https://direct.mit.edu/evco/article/10/2/99/1123/Evolving-Neural-Networks-through-Augmenting',
      'Citations': '3.37E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Thumbs Up?',
      'Domain': 'Language',
      'Task': 'Sentiment classification',
      'Organization(s)': 'Cornell University and IBM Almaden Research Center',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Bo Pang, Lillian Lee, Shivakumar Vaithyanathan',
      'Publication date': '05/2002',
      'Year': '2002',
      'Reference': 'Thumbs up? Sentiment Classification using Machine Learning Techniques',
      'Link': 'https://arxiv.org/abs/cs/0205070',
      'Citations': '1.07E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': 'IMDb',
      'Training dataset size (datapoints)': '2.05E+03',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Driving',
      'Task': 'Helicopter driving',
      'Organization(s)': 'Stanford and UC Berkeley',
      'Organization Categorization': 'Academia',
      'Author(s)': 'H. Kim, Michael Jordan, Shankar Sastry, Andrew Ng',
      'Publication date': '09/03/2006',
      'Year': '2006',
      'Reference': 'Autonomous helicopter flight via reinforcement learning',
      'Link': 'https://papers.nips.cc/paper/2003/hash/b427426b8acd2c2e53827970f2c2f526-Abstract.html',
      'Citations': '3.85E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'University of Oxford',
      'Organization Categorization': 'Academia',
      'Author(s)': 'R Fergus, P Perona, A Zisserman',
      'Publication date': '18/06/2003',
      'Year': '2003',
      'Reference': 'Object Class Recognition by Unsupervised Scale-Invariant Learning',
      'Link': 'https://ieeexplore.ieee.org/document/1211479',
      'Citations': '2.97E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '4.51E+02',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '3.50E+03',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Recommendation',
      'Task': '',
      'Organization(s)': 'University of Washington',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'G. Linden, B. Smith, and J. York',
      'Publication date': '01/01/2003',
      'Year': '2003',
      'Reference': 'Amazon.com Recommendations: Item-to-Item Collaborative Filtering',
      'Link': 'https://ieeexplore.ieee.org/document/1167344',
      'Citations': '7.26E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'LDA',
      'Domain': 'Language',
      'Task': 'Document classification',
      'Organization(s)': 'University of California, Stanford University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'David M. Blei, Andrew Y. Ng, Michael I. Jordan',
      'Publication date': '02/02/2003',
      'Year': '2003',
      'Reference': 'Latent Dirichlet Allocation',
      'Link': 'https://jmlr.org/papers/volume3/blei03a/blei03a.pdf',
      'Citations': '3.87E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'NPLM',
      'Domain': 'Language',
      'Task': 'Text autocompletion',
      'Organization(s)': 'Université de Montréal',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Jauvin',
      'Publication date': '15/03/2003',
      'Year': '2003',
      'Reference': 'A Neural Probabilistic Language Model',
      'Link': 'https://dl.acm.org/doi/10.5555/944919.944966',
      'Citations': '7.63E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.19E+07',
      'Training compute (FLOPs)': '1.3E+15',
      'Training dataset': 'Brown corpus',
      'Training dataset size (datapoints)': '1.00E+06',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '2.17E+07',
      'Equivalent training time (hours)': '20160',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Phrase-based translation',
      'Domain': 'Language',
      'Task': 'Translation',
      'Organization(s)': 'University of Southern California',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Philipp Koehn, Franz Josef Och, Daniel Marcu',
      'Publication date': '05/2003',
      'Year': '2003',
      'Reference': 'Statistical Phrase-Based Translation',
      'Link': 'https://dl.acm.org/doi/10.3115/1073445.1073462',
      'Citations': '4.27E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '9.18E+06',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '2.00E+07',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'CNN Best Practices',
      'Domain': 'Vision',
      'Task': 'Character recognition',
      'Organization(s)': 'One Microsoft Way',
      'Organization Categorization': 'Industry',
      'Author(s)': 'PY Simard, D Steinkraus, JC Platt',
      'Publication date': '06/08/2003',
      'Year': '2003',
      'Reference': 'Best practices for convolutional neural networks applied to visual document analysis',
      'Link': 'https://ieeexplore.ieee.org/document/1227801',
      'Citations': '3.07E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': 'MNIST',
      'Training dataset size (datapoints)': '5.00E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': 'Word sense disambiguation',
      'Organization(s)': 'University of Sussex',
      'Organization Categorization': 'Academia',
      'Author(s)': 'D McCarthy, R Koeling, J Weeds',
      'Publication date': '07/2004',
      'Year': '2004',
      'Reference': 'Finding Predominant Word Senses in Untagged Text',
      'Link': 'https://aclanthology.org/P04-1036/',
      'Citations': '4.75E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '5.00E+03',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Sandstorm (DARPA Grand Challenge I)',
      'Domain': 'Driving',
      'Task': 'Self-driving car',
      'Organization(s)': 'Carnegie Mellon University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'William Red L. Whittaker',
      'Publication date': '06/2004',
      'Year': '2004',
      'Reference': 'DARPA Grand Challenge Technical Paper',
      'Link': 'https://ieeexplore.ieee.org/document/1336386',
      'Citations': '6.60E+01',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Stanford University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'B. Taskar, C. Guestrin, and D. Koller',
      'Publication date': '03/2004',
      'Year': '2004',
      'Reference': 'Max-margin markov networks',
      'Link': 'https://papers.nips.cc/paper/2003/file/878d5691c824ee2aaf770f7d36c151d6-Paper.pdf',
      'Citations': '1.76E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '6.00E+02',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Soongsil University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'KS Oh, K Jung',
      'Publication date': '01/06/2004',
      'Year': '2004',
      'Reference': 'GPU implementation of neural networks',
      'Link': 'https://www.sciencedirect.com/science/article/pii/S0031320304000524',
      'Citations': '4.71E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'LIRA',
      'Domain': 'Vision',
      'Task': 'Character recognition',
      'Organization(s)': 'Instituto de Ciencias Aplicadas y Technologia',
      'Organization Categorization': 'Academia',
      'Author(s)': 'E Kussul, T Baidyk',
      'Publication date': '30/07/2004',
      'Year': '2004',
      'Reference': 'Improved method of handwritten digit recognition tested on MNIST database',
      'Link': 'https://www.sciencedirect.com/science/article/abs/pii/S0262885604000721',
      'Citations': '1.88E+02',
      'Inclusion criteria': '',
      'Parameters': '1.00E+05',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.00E+04',
      'Hidden layers': '1',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '45',
      'Inference time (ms)': '500',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Hiero',
      'Domain': 'Language',
      'Task': 'Translation',
      'Organization(s)': 'University of Maryland, College Park',
      'Organization Categorization': 'Academia',
      'Author(s)': 'David Chiang',
      'Publication date': '01/06/2005',
      'Year': '2005',
      'Reference': 'A Hierarchical Phrase-Based Model for Statistical Machine Translation',
      'Link': 'https://aclanthology.org/P05-1033/',
      'Citations': '1.49E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.20E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.71E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Stanley /DARPA Grand Challenge 2)',
      'Domain': 'Driving',
      'Task': 'Self-driving car',
      'Organization(s)': 'Stanford University',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'S Thrun, M Montemerlo, H Dahlkamp',
      'Publication date': '01/01/2006',
      'Year': '2006',
      'Reference': 'Stanley: The Robot that Wonthe DARPA Grand Challenge',
      'Link': 'https://www.researchgate.net/publication/220648006_Stanley_The_robot_that_won_the_DARPA_Grand_Challenge',
      'Citations': '2.56E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'University of the Balearic Islands and CMLA',
      'Organization Categorization': 'Academia',
      'Author(s)': 'A Buades, B Coll, JM Morel',
      'Publication date': '20/06/2005',
      'Year': '2005',
      'Reference': 'A non-local algorithm for image denoising',
      'Link': 'http://www.iro.umontreal.ca/~mignotte/IFT6150/Articles/Buades-NonLocal.pdf',
      'Citations': '6.87E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'New York University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'S Chopra, R Hadsell, Y LeCun',
      'Publication date': '20/06/2005',
      'Year': '2005',
      'Reference': 'Learning a similarity metric discriminatively, with application to face verification',
      'Link': 'https://ieeexplore.ieee.org/document/1467314',
      'Citations': '3.05E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.40E+05',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'SACHS',
      'Domain': 'Other',
      'Task': '',
      'Organization(s)': 'MIT and Stanford',
      'Organization Categorization': 'Academia',
      'Author(s)': 'K. Sachs, O. Perez, D. Pe\'er, D. A. Lauffenburger and G. P. Nolan',
      'Publication date': '22/04/2005',
      'Year': '2005',
      'Reference': 'Causal Protein-Signaling Networks Derived from Multiparameter Single-Cell Data.',
      'Link': 'https://science.sciencemag.org/content/308/5721/523.long',
      'Citations': '1.68E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.78E+02',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '5.40E+03',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'Inria Grenoble Rhône-Alpes',
      'Organization Categorization': 'Academia',
      'Author(s)': 'N Dalal, B Triggs',
      'Publication date': '25/06/2005',
      'Year': '2005',
      'Reference': 'Histograms of oriented gradients for human detection',
      'Link': 'https://ieeexplore.ieee.org/document/1467360',
      'Citations': '3.66E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.81E+03',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'BiLSTM for Speech',
      'Domain': 'Speech',
      'Task': 'Speech recognition',
      'Organization(s)': 'IDSIA and TU Munich',
      'Organization Categorization': 'Academia',
      'Author(s)': 'A Graves, J Schmidhuber',
      'Publication date': '01/08/2005',
      'Year': '2005',
      'Reference': 'Framewise phoneme classification with bidirectional LSTM and other neural network architectures',
      'Link': 'https://www.sciencedirect.com/science/article/abs/pii/S0893608005001206',
      'Citations': '3.39E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.52E+05',
      'Training compute (FLOPs)': '2.4E+13',
      'Training dataset': 'TIMIT',
      'Training dataset size (datapoints)': '4.16E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'NYU',
      'Organization Categorization': 'Academia',
      'Author(s)': 'M Ranzato, C Poultney, S Chopra, Y Cun',
      'Publication date': '04/12/2006',
      'Year': '2006',
      'Reference': 'Efficient Learning of Sparse Representations with an Energy-Based Model.',
      'Link': 'https://papers.nips.cc/paper/2006/hash/87f4d79e36d68c3031ccf6c55e9bbd39-Abstract.html',
      'Citations': '1.60E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': 'MNIST',
      'Training dataset size (datapoints)': '6.00E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Stanford University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Rion Snow, Dan Jurafsky, and Andrew Y. Ng',
      'Publication date': '07/2006',
      'Year': '2006',
      'Reference': 'Semantic Taxonomy Induction from Heterogenous Evidence',
      'Link': 'https://www.aclweb.org/anthology/P06-1101/',
      'Citations': '5.71E+02',
      'Inclusion criteria': '',
      'Parameters': '1.00E+02',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '8.51E+05',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'FAST',
      'Domain': 'Vision',
      'Task': 'Corner detection',
      'Organization(s)': 'University of Cambridge',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Edward Rosten and Tom Drummond',
      'Publication date': '07/05/2006',
      'Year': '2006',
      'Reference': 'Machine Learning for High-Speed Corner Detection',
      'Link': 'https://link.springer.com/chapter/10.1007/11744023_34',
      'Citations': '5.42E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'University of Illinois, INRIA, Ecole Normale',
      'Organization Categorization': 'Academia',
      'Author(s)': 'S Lazebnik, C Schmid, J Ponce',
      'Publication date': '17/06/2006',
      'Year': '2006',
      'Reference': 'Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories',
      'Link': 'https://inc.ucsd.edu/mplab/users/marni/Igert/Lazebnik_06.pdf',
      'Citations': '9.81E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'CRAN, CENPARMI',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Fabian Lauer, Ching Y Suen, Gerard Bloch',
      'Publication date': '02/02/2006',
      'Year': '2006',
      'Reference': 'A trainable feature extractor for handwritten digit recognition',
      'Link': 'https://hal.archives-ouvertes.fr/hal-00018426/en',
      'Citations': '3.47E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'DrLIM',
      'Domain': 'Vision',
      'Task': 'Image embedding',
      'Organization(s)': 'New York University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'R. Hadsell; S. Chopra; Y. LeCun',
      'Publication date': '17/06/2006',
      'Year': '2006',
      'Reference': 'Dimensionality Reduction by Learning an Invariant Mapping',
      'Link': 'https://ieeexplore.ieee.org/document/1640964',
      'Citations': '2.69E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '3.71E+04',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '2.17E+05',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'CTC-Trained LSTM',
      'Domain': 'Speech',
      'Task': 'Speech recognition',
      'Organization(s)': 'IDSIA, TUM',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Alex Graves, Santiago Fernández, Faustino Gómez, Jürgen Schmidhuber',
      'Publication date': '25/06/2006',
      'Year': '2006',
      'Reference': 'Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks',
      'Link': 'https://www.cs.toronto.edu/~graves/icml_2006.pdf',
      'Citations': '3.29E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.15E+05',
      'Training compute (FLOPs)': '',
      'Training dataset': 'TIMIT',
      'Training dataset size (datapoints)': '4.16E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'DImensionality Reduction',
      'Domain': 'Vision',
      'Task': 'Face recognition',
      'Organization(s)': 'University of Toronto',
      'Organization Categorization': 'Academia',
      'Author(s)': 'GE Hinton, RR Salakhutdinov',
      'Publication date': '18/07/2006',
      'Year': '2006',
      'Reference': 'Reducing the dimensionality of data with neural networks.',
      'Link': 'https://www.cs.toronto.edu/~hinton/science.pdf',
      'Citations': '1.57E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '3.80E+06',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '7.00E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Deep Belief Nets',
      'Domain': 'Vision',
      'Task': 'Character recognition',
      'Organization(s)': 'University of Toronto, NUS',
      'Organization Categorization': 'Academia',
      'Author(s)': 'GE Hinton, S Osindero, YW Teh',
      'Publication date': '18/07/2006',
      'Year': '2006',
      'Reference': 'A fast learning algorithm for deep belief nets',
      'Link': 'https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf',
      'Citations': '1.61E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.60E+06',
      'Training compute (FLOPs)': '',
      'Training dataset': 'MNIST',
      'Training dataset size (datapoints)': '6.00E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Machine Vision group, Finland',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Timo Ahonen, Abdenour Hadid, and Matti Pietikainen',
      'Publication date': '12/2006',
      'Year': '2006',
      'Reference': 'Face Description with Local Binary Patterns: Application to Face Recognition',
      'Link': 'http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.456.1094&rep=rep1&type=pdf',
      'Citations': '3.11E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'University of Bern, IDSIA, TU Munich',
      'Organization Categorization': 'Academia',
      'Author(s)': 'M Liwicki, A Graves, S Fernàndez',
      'Publication date': '23/09/2007',
      'Year': '2007',
      'Reference': 'A Novel Approach to On-Line Handwriting Recognition Based on Bidirectional Long Short-Term Memory Networks',
      'Link': 'https://people.idsia.ch//~juergen/icdar_2007.pdf',
      'Citations': '2.53E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'University of Montreal',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Y Bengio, P Lamblin, D Popovici',
      'Publication date': '04/12/2006',
      'Year': '2006',
      'Reference': 'Greedy layer-wise training of deep networks',
      'Link': 'https://dl.acm.org/doi/10.5555/2976456.2976476',
      'Citations': '5.61E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'NEC Laboratories, Google Zurich',
      'Organization Categorization': 'Industry',
      'Author(s)': 'L Bottou, O Bousquet',
      'Publication date': '03/12/2007',
      'Year': '2007',
      'Reference': 'The Tradeoffs of Large Scale Learning',
      'Link': 'https://dl.acm.org/doi/10.5555/2981562.2981583',
      'Citations': '1.70E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Boss (DARPA Urban Challenge)',
      'Domain': 'Driving',
      'Task': 'Self-driving car',
      'Organization(s)': 'Carnegie Mellon University',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Chris Urmson, Joshua Anhalt, Drew Bagnell,Christopher Baker, Robert Bittner,M. N. Clark, John Dolan, Dave Duggins,Tugrul Galatali, Chris Geyer,Michele Gittleman, Sam Harbaugh,Martial Hebert, Thomas M. Howard,Sascha Kolski, Alonzo Kelly,Maxim Likhachev, Matt McNaughton,Nick Miller, Kevin Peterson, Brian Pilnick,Raj Rajkumar, Paul Rybski, Bryan Salesky,Young-Woo Seo, Sanjiv Singh, Jarrod Snider,Anthony Stentz, William “Red” Whittaker,Ziv Wolkowicki, and Jason Ziglar',
      'Publication date': '23/07/2008',
      'Year': '2008',
      'Reference': 'Autonomous Driving in UrbanEnvironments: Boss and theUrban Challenge',
      'Link': 'https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.20255',
      'Citations': '1.84E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Recommendation',
      'Task': '',
      'Organization(s)': 'AT&T Labs',
      'Organization Categorization': 'Industry',
      'Author(s)': 'RM Bell, Y Koren',
      'Publication date': '28/10/2007',
      'Year': '2007',
      'Reference': 'Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights',
      'Link': 'http://brettb.net/project/papers/2007%20Scalable%20collaborative%20filtering%20with%20jointly%20derived%20neighborhood%20interpolation%20weights.pdf',
      'Citations': '6.87E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'BellKor 2007',
      'Domain': 'Recommendation',
      'Task': 'Movie ratings',
      'Organization(s)': 'AT&T Labs',
      'Organization Categorization': 'Industry',
      'Author(s)': 'RM Bell, Y Koren, C Volinsky',
      'Publication date': '21/09/2009',
      'Year': '2009',
      'Reference': 'The BellKor solution to the Netflix Prize',
      'Link': 'https://www.semanticscholar.org/paper/The-BellKor-solution-to-the-Netflix-Prize-Bell-Koren/f4ebb542c752a0dc423f94fd121e2edb8f6275ba',
      'Citations': '2.14E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': 'Netflix Prize',
      'Training dataset size (datapoints)': '1.00E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'University of Edinburgh',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch',
      'Publication date': '06/2007',
      'Year': '2007',
      'Reference': 'Moses: Open Source Toolkit for Statistical Machine Translation',
      'Link': 'https://aclanthology.org/P07-2045.pdf',
      'Citations': '6.10E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'λ-WASP',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'UT Austin',
      'Organization Categorization': 'Academia',
      'Author(s)': 'YW Wong, R Mooney',
      'Publication date': '06/2007',
      'Year': '2007',
      'Reference': 'Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus',
      'Link': 'https://www.aclweb.org/anthology/P07-1121/',
      'Citations': '3.50E+02',
      'Inclusion criteria': '',
      'Parameters': '4.90E+05',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '7.92E+02',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'University of Montreal',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Hugo Larechelle, Dumithru Erhan, Aaron C Courville, James Bergsta, Yoshua Bengio',
      'Publication date': '01/06/2007',
      'Year': '2007',
      'Reference': 'An empirical evaluation of deep architectures on problems with many factors of variation',
      'Link': 'https://dl.acm.org/doi/10.1145/1273496.1273556',
      'Citations': '1.12E+03',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Restricted Bolzmann machines',
      'Domain': 'Recommendation',
      'Task': '',
      'Organization(s)': 'University of Toronto',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Russ Salukhutdinov, Andriy Mnih, GE Hinton',
      'Publication date': '20/06/2007',
      'Year': '2007',
      'Reference': 'Restricted Boltzmann machines for collaborative filtering',
      'Link': 'https://dl.acm.org/doi/abs/10.1145/1273496.1273596?casa_token=cfdkH2x12MwAAAAA:sEUzfllIGyPcOfzgUoDPHlpC1ukfCAo8ewocBXWBswIIF9eS5HdFo30nOtfmIV8gm-XpBpQJJ5zYVO8',
      'Citations': '2.14E+03',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': 'Netflix Prize',
      'Training dataset size (datapoints)': '1.00E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Recommendation',
      'Task': '',
      'Organization(s)': 'Warsaw University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'A Paterek',
      'Publication date': '12/08/2007',
      'Year': '2007',
      'Reference': 'Improving regularized singular value decomposition for collaborative filtering',
      'Link': 'https://www.semanticscholar.org/paper/Improving-regularized-singular-value-decomposition-Paterek/f732d0f69fe4e84a95c32706b28b9e4ef1753c61',
      'Citations': '1.12E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': 'Netflix Prize',
      'Training dataset size (datapoints)': '1.00E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'BLSTM',
      'Domain': 'Vision',
      'Task': 'Character recognition',
      'Organization(s)': 'University of Bern, IDSIA, TU Munich',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Alex Graves, Marcus Liwicki, Horst Bunke, Jürgen Schmidhuber, Santiago Fernández',
      'Publication date': '12/2007',
      'Year': '2007',
      'Reference': 'Unconstrained online handwriting recognition with recurrent neural networks',
      'Link': 'https://proceedings.neurips.cc/paper/2007/hash/4b0250793549726d5c1ea3906726ebfe-Abstract.html',
      'Citations': '3.11E+02',
      'Inclusion criteria': '',
      'Parameters': '1.01E+05',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Denoising Autoencoders',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'University of Montreal',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Pascal Vincent, Hugo Larechelle, Yoshua Bengio, Pierre- Antoine Manzagol',
      'Publication date': '05/07/2008',
      'Year': '2008',
      'Reference': 'Extracting and Composing Robust Features with Denoising Autoencoders',
      'Link': 'https://dl.acm.org/doi/10.1145/1390156.1390294',
      'Citations': '6.30E+03',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Google, NUANCE Communications, UIUC, IDIAP',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Jason Weston, Frederick, Ratle, Hossein Mobahi, Ronan Collobert',
      'Publication date': '05/07/2008',
      'Year': '2008',
      'Reference': 'Deep Learning via Semi-Supervised Embedding',
      'Link': 'https://dl.acm.org/doi/10.1145/1390156.1390303',
      'Citations': '1.09E+03',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'University of Chicago & Toyota Technological Institute, Chicago & University of California, Irvine',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Pedro Felzenszwalb, David McAllester, Deva Ramanan',
      'Publication date': '23/06/2008',
      'Year': '2008',
      'Reference': 'A discriminatively trained, multiscale, deformable part model',
      'Link': 'https://ieeexplore.ieee.org/abstract/document/4587597',
      'Citations': '3.09E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'NEC Laboratories',
      'Organization Categorization': 'Industry',
      'Author(s)': 'R Collobert, J Weston',
      'Publication date': '05/07/2008',
      'Year': '2008',
      'Reference': 'A unified architecture for natural language processing: Deep neural networks with multitask learning',
      'Link': 'https://dl.acm.org/doi/10.1145/1390156.1390177',
      'Citations': '5.76E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '6.31E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Stacked Semisuperviser Autoencoders',
      'Domain': 'Language',
      'Task': 'Document representation',
      'Organization(s)': 'New York University and Microsoft Research Cambridge',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'MA Ranzato, M Szummer',
      'Publication date': '15/07/2008',
      'Year': '2008',
      'Reference': 'Semisupervised learning of compact document representations with deep networks',
      'Link': 'https://dl.acm.org/doi/10.1145/1390156.1390256',
      'Citations': '2.27E+02',
      'Inclusion criteria': '',
      'Parameters': '3.00E+06',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '6.61E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'Univeristy of Lubeck, Germany',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Kai Labusch, Erhadt Barth, Thomas Martinetz',
      'Publication date': '19/11/2008',
      'Year': '2008',
      'Reference': 'Simple method for high-performance digit recognition based on sparse coding',
      'Link': 'https://pubmed.ncbi.nlm.nih.gov/19000969/',
      'Citations': '1.24E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'BigChaos 2008',
      'Domain': 'Recommendation',
      'Task': 'Movie ratings',
      'Organization(s)': 'AT&T Labs',
      'Organization Categorization': 'Industry',
      'Author(s)': 'A Töscher, M Jahrer',
      'Publication date': '25/11/2008',
      'Year': '2008',
      'Reference': 'The BigChaos Solution to the Netflix Prize 2008',
      'Link': 'https://www.researchgate.net/publication/228419683_The_bigchaos_solution_to_the_netflix_prize_2008',
      'Citations': '3.50E+01',
      'Inclusion criteria': 'Historical relevance',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': 'Netflix Prize',
      'Training dataset size (datapoints)': '1.00E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Semantic Hashing',
      'Domain': 'Other',
      'Task': '',
      'Organization(s)': 'University of Toronto',
      'Organization Categorization': 'Academia',
      'Author(s)': 'R Salakhutdinov, G Hinton',
      'Publication date': '10/12/2008',
      'Year': '2008',
      'Reference': 'Semantic Hashing',
      'Link': 'https://www.cs.cmu.edu/~rsalakhu/papers/sdarticle.pdf',
      'Citations': '1.49E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '2.60E+06',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '3.11E+05',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'University of Edinburgh, University of Pittsburgh',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Theresa Wilson, Janyce Wiebe, Paul Hoffmann.',
      'Publication date': '09/2009',
      'Year': '2009',
      'Reference': 'Recognizing Contextual Polarity: An Exploration of Features for Phrase-Level Sentiment Analysis',
      'Link': 'https://aclanthology.org/J09-3003.pdf',
      'Citations': '7.87E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.11E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Recommendation',
      'Task': '',
      'Organization(s)': 'AT&T Labs, Yahoo Research',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Yehuda Koren, Robert Bell, and Chris Volinsky',
      'Publication date': '07/08/2009',
      'Year': '2009',
      'Reference': 'Matrix factorization techniques for recommender systems',
      'Link': 'https://ieeexplore.ieee.org/document/5197422',
      'Citations': '8.91E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': 'Netflix Prize',
      'Training dataset size (datapoints)': '1.00E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': '3D reconstruction',
      'Task': '',
      'Organization(s)': 'University of Washington, Cornell, Microsoft Research',
      'Organization Categorization': 'Industry - Academia Collaboration (Academia leaning)',
      'Author(s)': 'Sameer Agarwal, Noah Snavely, Ian Simon, Steven M. Seitz and Richard Szeliski',
      'Publication date': '29/09/2009',
      'Year': '2009',
      'Reference': 'Building Rome in a Day',
      'Link': 'https://grail.cs.washington.edu/rome/',
      'Citations': '2.20E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Stanford',
      'Organization Categorization': 'Academia',
      'Author(s)': 'H Lee, R Grosse, R Ranganath, AY Ng',
      'Publication date': '14/06/2009',
      'Year': '2009',
      'Reference': 'Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations',
      'Link': 'https://dl.acm.org/doi/10.1145/1553374.1553453',
      'Citations': '2.96E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'University of Montreal, Microsoft Research',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Y Bengio',
      'Publication date': '15/11/2009',
      'Year': '2009',
      'Reference': 'Learning deep architectures for AI',
      'Link': 'https://www.nowpublishers.com/article/Details/MAL-006',
      'Citations': '9.78E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Deep Boltzmann Machines',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'University of Toronto',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Ruslan Salakhutdinov, Geoffrey Hinton',
      'Publication date': '16/04/2009',
      'Year': '2009',
      'Reference': 'Deep Boltzmann Machines',
      'Link': 'https://www.sciencedirect.com/topics/computer-science/deep-boltzmann-machine',
      'Citations': '2.67E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'GPU DBNs',
      'Domain': 'Other',
      'Task': '',
      'Organization(s)': 'Stanford',
      'Organization Categorization': 'Academia',
      'Author(s)': 'R Raina, A Madhavan, AY Ng',
      'Publication date': '15/06/2009',
      'Year': '2009',
      'Reference': 'Large-scale Deep Unsupervised Learning using Graphics Processors',
      'Link': 'http://www.machinelearning.org/archive/icml2009/papers/218.pdf',
      'Citations': '7.89E+02',
      'Inclusion criteria': '',
      'Parameters': '1.00E+08',
      'Training compute (FLOPs)': '1.0E+15',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.00E+06',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'BellKor 2008',
      'Domain': 'Recommendation',
      'Task': 'Movie ratings',
      'Organization(s)': 'AT&T Labs',
      'Organization Categorization': 'Industry',
      'Author(s)': 'RM Bell, Y Koren, C Volinsky',
      'Publication date': '1/8/2009',
      'Year': '2009',
      'Reference': 'The BellKor 2008 Solution to the Netflix Prize',
      'Link': 'https://www.researchgate.net/publication/228766792_The_BellKor_2008_solution_to_the_Netflix_Prize',
      'Citations': '1.58E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': 'Netflix Prize',
      'Training dataset size (datapoints)': '1.00E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'RL mapping instructions',
      'Domain': 'Reading',
      'Task': 'Instruction interpretation',
      'Organization(s)': 'MIT',
      'Organization Categorization': 'Academia',
      'Author(s)': 'SRK Branavan, H Chen, LS Zettlemoyer, R Barzilay',
      'Publication date': '08/2009',
      'Year': '2009',
      'Reference': 'Reinforcement Learning for Mapping Instructions to Actions',
      'Link': 'https://aclanthology.org/P09-1010/',
      'Citations': '2.53E+02',
      'Inclusion criteria': '',
      'Parameters': '1.33E+05',
      'Training compute (FLOPs)': '',
      'Training dataset': 'Windows Help and Support',
      'Training dataset size (datapoints)': '1.33E+03',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'RL mapping instructions',
      'Domain': 'Reading',
      'Task': 'Instruction interpretation',
      'Organization(s)': 'MIT',
      'Organization Categorization': 'Academia',
      'Author(s)': 'SRK Branavan, H Chen, LS Zettlemoyer, R Barzilay',
      'Publication date': '08/2009',
      'Year': '2009',
      'Reference': 'Reinforcement Learning for Mapping Instructions to Actions',
      'Link': 'https://aclanthology.org/P09-1010/',
      'Citations': '2.53E+02',
      'Inclusion criteria': '',
      'Parameters': '8.09E+04',
      'Training compute (FLOPs)': '',
      'Training dataset': 'Windows Help and Support',
      'Training dataset size (datapoints)': '2.93E+02',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Recommendation',
      'Task': 'Movie ratings',
      'Organization(s)': 'AT&T Labs',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Y Koren',
      'Publication date': '08/2009',
      'Year': '2009',
      'Reference': 'The BellKor Solution to the Netflix Grand Prize',
      'Link': 'https://www2.seas.gwu.edu/~simhaweb/champalg/cf/papers/KorenBellKor2009.pdf',
      'Citations': '5.07E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': 'Netflix Prize',
      'Training dataset size (datapoints)': '1.00E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Recommendation',
      'Task': 'Movie ratings',
      'Organization(s)': 'AT&T Labs',
      'Organization Categorization': 'Industry',
      'Author(s)': 'A Töscher, M Jahrer, RM Bell',
      'Publication date': '08/2009',
      'Year': '2009',
      'Reference': 'The BigChaos Solution to the Netflix Grand Prize',
      'Link': 'https://www.asc.ohio-state.edu/statistics/statgen/joul_aut2009/BigChaos.pdf',
      'Citations': '2.11E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': 'Netflix Prize',
      'Training dataset size (datapoints)': '1.00E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Recommendation',
      'Task': 'Movie ratings',
      'Organization(s)': 'Pragmatic Theory Inc.',
      'Organization Categorization': 'Industry',
      'Author(s)': 'M Piotte, M Chabbert',
      'Publication date': '08/2009',
      'Year': '2009',
      'Reference': 'The Pragmatic Theory solution to the Netflix Grand Prize',
      'Link': 'https://www.asc.ohio-state.edu/statistics/statgen/joul_aut2009/PragmaticTheory.pdf',
      'Citations': '1.11E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': 'Netflix Prize',
      'Training dataset size (datapoints)': '1.00E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'University of Colorado & New Mexico State University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Thomas K Landauer, Peter W. Foltz & Darrell Laham',
      'Publication date': '11/11/2009',
      'Year': '2009',
      'Reference': 'An Introduction to Latent Semantic Analysis',
      'Link': 'https://www.tandfonline.com/doi/abs/10.1080/01638539809545028',
      'Citations': '6.42E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'University of Toronto',
      'Organization Categorization': 'Academia',
      'Author(s)': 'GE Hinton',
      'Publication date': '02/08/2010',
      'Year': '2010',
      'Reference': 'A practical guide to training restricted boltzmann machines',
      'Link': 'https://link.springer.com/chapter/10.1007/978-3-642-35289-8_32',
      'Citations': '3.34E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'INRIA, Ecole, NYU',
      'Organization Categorization': 'Academia',
      'Author(s)': 'YL Boureau, F Bach, Y LeCun',
      'Publication date': '13/06/2010',
      'Year': '2010',
      'Reference': 'Learning mid-level features for recognition',
      'Link': 'https://ieeexplore.ieee.org/document/5539963',
      'Citations': '1.31E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'University of Montreal, University of Toronto',
      'Organization Categorization': 'Academia',
      'Author(s)': 'P Vincent, H Larochelle, I Lajoie, Y Bengio',
      'Publication date': '03/01/2010',
      'Year': '2010',
      'Reference': 'Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion',
      'Link': 'https://www.jmlr.org/papers/v11/vincent10a.html',
      'Citations': '6.23E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'Xerox Research Centre Europe (XRCE)',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Florent PerronninJorge SánchezThomas Mensink',
      'Publication date': '05/09/2010',
      'Year': '2010',
      'Reference': 'Improving the Fisher Kernel for Large-Scale Image Classification',
      'Link': 'https://link.springer.com/chapter/10.1007/978-3-642-15561-1_11',
      'Citations': '3.06E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '6-layer MLP (MNIST)',
      'Domain': 'Vision',
      'Task': 'Character recognition',
      'Organization(s)': 'IDSIA ; University of Lugano & SUPSI',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Dan Claudiu Ciresan, Ueli Meier, Luca Maria Gambardella, Juergen Schmidhuber',
      'Publication date': '01/03/2010',
      'Year': '2010',
      'Reference': 'Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition',
      'Link': 'https://arxiv.org/abs/1003.0358',
      'Citations': '1.26E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.21E+07',
      'Training compute (FLOPs)': '1.3E+14',
      'Training dataset': 'MNIST',
      'Training dataset size (datapoints)': '6.00E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Feedforward NN',
      'Domain': 'Vision',
      'Task': 'Digit recognition',
      'Organization(s)': 'University of Montreal',
      'Organization Categorization': 'Academia',
      'Author(s)': 'X Glorot, Y Bengio',
      'Publication date': '13/05/2010',
      'Year': '2010',
      'Reference': 'Understanding the difficulty of training deep feedforward neural networks',
      'Link': 'https://proceedings.mlr.press/v9/glorot10a.html',
      'Citations': '1.33E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '7.08E+06',
      'Training compute (FLOPs)': '3.5E+14',
      'Training dataset': 'MNIST',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '1.40E+07',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Word Representations',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'University of Montreal, University of Illinois at Urbana- Champaigne',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Joseph Turian, Lev-Arie Ratinov, Yoshua Bengio',
      'Publication date': '01/06/2010',
      'Year': '2010',
      'Reference': 'Word Representations: A Simple and General Method for Semi-Supervised Learning',
      'Link': 'https://aclanthology.org/P10-1040.pdf',
      'Citations': '2.51E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '3.70E+07',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Deconvolutional Network',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'NYU',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Matthew D. Zeiler, Dilip Krishnan, Graham W. Taylor and Rob Fergus',
      'Publication date': '13/06/2010',
      'Year': '2010',
      'Reference': 'Deconvolutional Networks',
      'Link': 'https://ieeexplore.ieee.org/document/5539957',
      'Citations': '1.52E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'ReLU (NORB)',
      'Domain': 'Vision',
      'Task': 'Object recognition',
      'Organization(s)': 'University of Toronto',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Nair, V., Hinton, G. E.',
      'Publication date': '15/06/2010',
      'Year': '2010',
      'Reference': 'Rectified linear units improve restricted boltzmann machines',
      'Link': 'https://dl.acm.org/doi/10.5555/3104322.3104425',
      'Citations': '1.40E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.62E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '2.92E+05',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'ReLU (LFW)',
      'Domain': 'Vision',
      'Task': 'Face recognition',
      'Organization(s)': 'University of Toronto',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Nair, V., Hinton, G. E.',
      'Publication date': '15/06/2010',
      'Year': '2010',
      'Reference': 'Rectified linear units improve restricted boltzmann machines',
      'Link': 'https://dl.acm.org/doi/10.5555/3104322.3104425',
      'Citations': '1.40E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'RNN 500/10 + RT09 LM (NIST RT05)',
      'Domain': 'Speech',
      'Task': 'Transcription',
      'Organization(s)': 'Brno University of Technology, Johns Hopkins University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'T. Mikolov, M. Karafiat, L. Burget, J. Cernock ´ y, and S. Khudanpur',
      'Publication date': '26/09/2010',
      'Year': '2010',
      'Reference': 'Recurrent neural network based language model.',
      'Link': 'https://www.researchgate.net/publication/221489926_Recurrent_neural_network_based_language_model',
      'Citations': '5.67E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '5.27E+06',
      'Training compute (FLOPs)': '3.4E+15',
      'Training dataset': 'NIST RT05',
      'Training dataset size (datapoints)': '5.40E+06',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '1.05E+07',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'KN5 LM + RNN 400/10 (WSJ)',
      'Domain': 'Speech',
      'Task': 'Transcription',
      'Organization(s)': 'Brno University of Technology, Johns Hopkins University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'T. Mikolov, M. Karafiat, L. Burget, J. Cernock ´ y, and S. Khudanpur',
      'Publication date': '26/09/2010',
      'Year': '2010',
      'Reference': 'Recurrent neural network based language model.',
      'Link': 'https://www.researchgate.net/publication/221489926_Recurrent_neural_network_based_language_model',
      'Citations': '5.67E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '8.00E+07',
      'Training compute (FLOPs)': '6.1E+16',
      'Training dataset': 'WSJ',
      'Training dataset size (datapoints)': '6.40E+06',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '1.60E+08',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Recommendation',
      'Task': '',
      'Organization(s)': 'Google Inc',
      'Organization Categorization': 'Industry',
      'Author(s)': 'J Davidson, B Liebald, J Liu, P Nandy',
      'Publication date': '26/09/2010',
      'Year': '2010',
      'Reference': 'The YouTube Video Recommendation System',
      'Link': 'https://dl.acm.org/doi/10.1145/1864708.1864770',
      'Citations': '1.07E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.00E+10',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'University of Michigan, Stanford University',
      'Organization Categorization': '',
      'Author(s)': 'A Coates, A Ng, H Lee',
      'Publication date': '11/04/2011',
      'Year': '2011',
      'Reference': 'An analysis of single-layer networks in unsupervised feature learning',
      'Link': 'http://proceedings.mlr.press/v15/coates11a.html',
      'Citations': '2.66E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'University of Montreal',
      'Organization Categorization': 'Academia',
      'Author(s)': 'X Glorot, A Bordes, Y Bengio',
      'Publication date': '13/04/2011',
      'Year': '2011',
      'Reference': 'Deep sparse rectifier neural networks',
      'Link': 'http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf',
      'Citations': '7.22E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Harvard',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'JB Michel, YK Shen, AP Aiden, A Veres, MK Gray',
      'Publication date': '16/12/2010',
      'Year': '2010',
      'Reference': 'Quantitative Analysis of Culture Using Millions of Digitized Books',
      'Link': 'https://science.sciencemag.org/content/331/6014/176',
      'Citations': '2.27E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': 'Part-of-speech tagging',
      'Organization(s)': 'Carnegie Mellon University, Google Research',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Dipanjan Das, Slav Petrov',
      'Publication date': '06/2011',
      'Year': '2011',
      'Reference': 'Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections',
      'Link': 'https://aclanthology.org/P11-1061/',
      'Citations': '3.16E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Domain Adaptation',
      'Domain': 'Vision',
      'Task': 'Object Recognition',
      'Organization(s)': 'University of Maryland, College Park',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Raghuraman Gopalan, Ruonan Li, Rama Chellappa',
      'Publication date': '06/11/2011',
      'Year': '2011',
      'Reference': 'Domain Adaptation for Object Recognition: An Unsupervised Approach',
      'Link': 'http://ftp.idiap.ch/pub/courses/EE-700/material/05-12-2012/2011_ICCV_DomainAdaptation.pdf',
      'Citations': '1.06E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.53E+04',
      'Training compute (FLOPs)': '',
      'Training dataset': 'Dataset introduced in \'Adapting Visual Category Models to New Domains\'',
      'Training dataset size (datapoints)': '4.65E+03',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': 'Supervised',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Brno University of Technology, Johns Hopkins University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'T. Mikolov, S. Kombrink, L. Burget, J. Cernock ˇ y, and S. Khudanpur',
      'Publication date': '22/05/2011',
      'Year': '2011',
      'Reference': 'Extensions of recurrent neural network language model',
      'Link': 'https://ieeexplore.ieee.org/document/5947611',
      'Citations': '1.24E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': 'Penn Tree Bank',
      'Training dataset size (datapoints)': '6.98E+05',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Stanford',
      'Organization Categorization': 'Academia',
      'Author(s)': 'R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning',
      'Publication date': '07/2011',
      'Year': '2011',
      'Reference': 'Semi-supervised recursive autoencoders for predicting sentiment distributions',
      'Link': 'https://aclanthology.org/D11-1014/',
      'Citations': '1.48E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Univeristy of California Berkley, Technion- Israel Institute of Technology, Google',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'J Duchi, E Hazan, Y Singer',
      'Publication date': '03/10/2011',
      'Year': '2011',
      'Reference': 'Adaptive Subgradient Methods for Online Learning and Stochastic Optimization',
      'Link': 'https://dl.acm.org/doi/10.5555/1953048.2021068',
      'Citations': '8.81E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'NLP from scratch',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'NEC Laboratories, Princeton',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Ronan Collobert, J. Weston, L. Bottou, Michael Karlen, K. Kavukcuoglu, P. Kuksa',
      'Publication date': '11/08/2011',
      'Year': '2011',
      'Reference': 'Natural Language Processing (Almost) from Scratch',
      'Link': 'https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf',
      'Citations': '7.64E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '5.00E+06',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '8.52E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Games',
      'Task': '',
      'Organization(s)': 'Collège de France',
      'Organization Categorization': 'Academia',
      'Author(s)': 'G Synnaeve, P Bessiere',
      'Publication date': '31/08/2011',
      'Year': '2011',
      'Reference': 'A Bayesian Model for RTS Units Control applied to StarCraft',
      'Link': 'https://ieeexplore.ieee.org/document/6032006',
      'Citations': '8.60E+01',
      'Inclusion criteria': '',
      'Parameters': '1.31E+04',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'University of Wisconsin Madison',
      'Organization Categorization': 'Academia',
      'Author(s)': 'F Niu, B Recht, C Ré, SJ Wright',
      'Publication date': '11/11/2011',
      'Year': '2011',
      'Reference': 'HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent',
      'Link': 'https://arxiv.org/abs/1106.5730',
      'Citations': '2.12E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Google',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden, Jon Orwant, Will Brockman and Slav Petrov',
      'Publication date': '07/2012',
      'Year': '2012',
      'Reference': 'Syntactic Annotations for the Google Books NGram Corpus',
      'Link': 'https://aclanthology.org/P12-3029/',
      'Citations': '4.89E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Karlsruhe Institute of Technology, Toyota Technological Institute at Chicago',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'A Geiger, P Lenz, R Urtasun',
      'Publication date': '16/06/2012',
      'Year': '2012',
      'Reference': 'Are we ready for autonomous driving? The KITTI vision benchmark suite',
      'Link': 'http://www.cvlibs.net/publications/Geiger2012CVPR_slides.pdf',
      'Citations': '7.14E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'MV-RNN',
      'Domain': 'Language',
      'Task': 'Text classification',
      'Organization(s)': 'Stanford',
      'Organization Categorization': 'Academia',
      'Author(s)': 'R. Socher, B. Huval, C. D. Manning, and A. Y. Ng',
      'Publication date': '12/07/2012',
      'Year': '2012',
      'Reference': 'Semantic compositionality through recursive matrix-vector spaces',
      'Link': 'https://www.aclweb.org/anthology/D12-1110/',
      'Citations': '1.46E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '3.51E+06',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'MCDNN (MNIST)',
      'Domain': 'Vision',
      'Task': 'Character recognition',
      'Organization(s)': 'IDSIA',
      'Organization Categorization': 'Academia',
      'Author(s)': 'D Ciregan, U Meier, J Schmidhuber',
      'Publication date': '13/02/2012',
      'Year': '2012',
      'Reference': 'Multi-column Deep Neural Networks for Image Classification',
      'Link': 'https://arxiv.org/abs/1202.2745v1',
      'Citations': '4.83E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.99E+06',
      'Training compute (FLOPs)': '3.7E+15',
      'Training dataset': 'MNIST',
      'Training dataset size (datapoints)': '6.00E+04',
      'Hidden layers': '3',
      'Inference compute (FLOPs)': '2.59E+07',
      'Equivalent training time (hours)': '490',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '0.021',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Dropout (MNIST)',
      'Domain': 'Vision',
      'Task': 'Character recognition',
      'Organization(s)': 'University of Toronto',
      'Organization Categorization': 'Academia',
      'Author(s)': 'GE Hinton, N Srivastava, A Krizhevsky',
      'Publication date': '03/06/2012',
      'Year': '2012',
      'Reference': 'Improving neural networks by preventing co-adaptation of feature detectors',
      'Link': 'https://arxiv.org/abs/1207.0580',
      'Citations': '6.68E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '5.59E+06',
      'Training compute (FLOPs)': '6.0E+15',
      'Training dataset': 'MNIST',
      'Training dataset size (datapoints)': '6.00E+04',
      'Hidden layers': '2',
      'Inference compute (FLOPs)': '1.12E+07',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '0.021',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Dropout (TIMIT)',
      'Domain': 'Speech',
      'Task': 'Speech recognition',
      'Organization(s)': 'University of Toronto',
      'Organization Categorization': 'Academia',
      'Author(s)': 'GE Hinton, N Srivastava, A Krizhevsky',
      'Publication date': '03/06/2012',
      'Year': '2012',
      'Reference': 'Improving neural networks by preventing co-adaptation of feature detectors',
      'Link': 'https://arxiv.org/abs/1207.0580',
      'Citations': '6.68E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '4.88E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': 'TIMIT',
      'Training dataset size (datapoints)': '4.16E+04',
      'Hidden layers': '4',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Dropout (CIFAR)',
      'Domain': 'Vision',
      'Task': 'Character recognition',
      'Organization(s)': 'University of Toronto',
      'Organization Categorization': 'Academia',
      'Author(s)': 'GE Hinton, N Srivastava, A Krizhevsky',
      'Publication date': '03/06/2012',
      'Year': '2012',
      'Reference': 'Improving neural networks by preventing co-adaptation of feature detectors',
      'Link': 'https://arxiv.org/abs/1207.0580',
      'Citations': '6.68E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Dropout (ImageNet)',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'University of Toronto',
      'Organization Categorization': 'Academia',
      'Author(s)': 'GE Hinton, N Srivastava, A Krizhevsky',
      'Publication date': '03/06/2012',
      'Year': '2012',
      'Reference': 'Improving neural networks by preventing co-adaptation of feature detectors',
      'Link': 'https://arxiv.org/abs/1207.0580',
      'Citations': '6.68E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': 'ImageNet',
      'Training dataset size (datapoints)': '1.00E+06',
      'Hidden layers': '7',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'AlexNet',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'University of Toronto',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton',
      'Publication date': '30/09/2012',
      'Year': '2012',
      'Reference': 'ImageNet Classification with Deep Convolutional Neural Networks',
      'Link': 'https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html',
      'Citations': '8.51E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '6.00E+07',
      'Training compute (FLOPs)': '4.7E+17',
      'Training dataset': 'ImageNet',
      'Training dataset size (datapoints)': '1.20E+06',
      'Hidden layers': '8',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'University of Toronto, University of Sherbrooke, Harvard University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'J Snoek, H Larochelle, RP Adams',
      'Publication date': '02/12/2012',
      'Year': '2012',
      'Reference': 'Practical Bayesian optimization of machine learning algorithms',
      'Link': 'https://arxiv.org/abs/1206.2944',
      'Citations': '5.67E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Object detection',
      'Organization(s)': 'Univeristy of Trento, University of Amsterdam',
      'Organization Categorization': 'Academia',
      'Author(s)': 'JRR Uijlings, KEA Van De Sande, T Gevers',
      'Publication date': '02/04/2013',
      'Year': '2013',
      'Reference': 'Selective search for object recognition',
      'Link': 'https://link.springer.com/article/10.1007/s11263-013-0620-5',
      'Citations': '5.59E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'University of Trento, University of Amsterdam',
      'Organization Categorization': 'Academia',
      'Author(s)': 'JRR Uijlings, KEA Van De Sande, T Gevers',
      'Publication date': '02/04/2013',
      'Year': '2013',
      'Reference': 'Selective search for object recognition',
      'Link': 'https://link.springer.com/article/10.1007/s11263-013-0620-5',
      'Citations': '5.59E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Stanford',
      'Organization Categorization': 'Academia',
      'Author(s)': 'R Socher, D Chen, CD Manning, A Ng',
      'Publication date': '12/2013',
      'Year': '2013',
      'Reference': 'Reasoning With Neural Tensor Networks for Knowledge Base Completion',
      'Link': 'https://papers.nips.cc/paper/2013/hash/b337e84de8752b27eda3a12363109e80-Abstract.html',
      'Citations': '1.66E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'DQN',
      'Domain': 'Games',
      'Task': 'Atari',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'V Mnih, K Kavukcuoglu, D Silver, A Graves',
      'Publication date': '01/01/2013',
      'Year': '2013',
      'Reference': 'Playing Atari with Deep Reinforcement Learning',
      'Link': 'https://arxiv.org/abs/1312.5602',
      'Citations': '6.68E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '8.36E+05',
      'Training compute (FLOPs)': '2.3E+15',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'Stanford',
      'Organization Categorization': 'Academia',
      'Author(s)': 'R Socher, M Ganjoo, H Sridhar, O Bastani',
      'Publication date': '16/01/2013',
      'Year': '2013',
      'Reference': 'Zero-Shot Learning Through Cross-Modal Transfer',
      'Link': 'https://arxiv.org/abs/1301.3666',
      'Citations': '1.21E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Maxout Networks ',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'University of Montreal',
      'Organization Categorization': 'Academia',
      'Author(s)': ' Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, Yoshua Bengio',
      'Publication date': '18/02/2013',
      'Year': '2013',
      'Reference': 'Maxout Networks ',
      'Link': 'https://arxiv.org/abs/1302.4389',
      'Citations': '2.58E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'PreTrans-3L-250H',
      'Domain': 'Speech',
      'Task': 'Speech recognition',
      'Organization(s)': 'Univeristy of Toronto',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Alex Graves, Abdel-rahman Mohamed, Geoffrey Hinton',
      'Publication date': '22/03/2013',
      'Year': '2013',
      'Reference': 'Speech Recognition with Deep Recurrent Neural Networks',
      'Link': 'https://arxiv.org/abs/1303.5778',
      'Citations': '7.79E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '4.30E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Microsoft Research',
      'Organization Categorization': 'Industry',
      'Author(s)': 'T Mikolov, W Yih, G Zweig',
      'Publication date': '09/06/2013',
      'Year': '2013',
      'Reference': 'Linguistic Regularities in Continuous Space Word Representations',
      'Link': 'https://www.aclweb.org/anthology/N13-1090/',
      'Citations': '3.63E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Mitosis',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'IDSIA',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Dan C. Cireşan, Alessandro Giusti, Luca M. Gambardella, Jürgen Schmidhuber',
      'Publication date': '22/09/2013',
      'Year': '2013',
      'Reference': 'Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks',
      'Link': 'https://link.springer.com/chapter/10.1007/978-3-642-40763-5_51',
      'Citations': '1.46E+03',
      'Inclusion criteria': 'ICPR 2012 mitosis detection competition winner',
      'Parameters': '3.72E+04',
      'Training compute (FLOPs)': '1.4E+17',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.00E+06',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Word2Vec (large)',
      'Domain': 'Language',
      'Task': 'Semantic embedding',
      'Organization(s)': 'Google',
      'Organization Categorization': 'Industry',
      'Author(s)': 'T Mikolov, I Sutskever, K Chen, GS Corrado',
      'Publication date': '16/10/2013',
      'Year': '2013',
      'Reference': 'Distributed Representations of Words and Phrases and their Compositionality',
      'Link': 'https://arxiv.org/abs/1310.4546',
      'Citations': '2.87E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '6.92E+09',
      'Training compute (FLOPs)': '3.9E+16',
      'Training dataset': '',
      'Training dataset size (datapoints)': '6.92E+05',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': 'Predict nearby words',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Word2Vec (small)',
      'Domain': 'Language',
      'Task': 'Semantic embedding',
      'Organization(s)': 'Google',
      'Organization Categorization': 'Industry',
      'Author(s)': 'T Mikolov, I Sutskever, K Chen, GS Corrado',
      'Publication date': '16/10/2013',
      'Year': '2013',
      'Reference': 'Distributed Representations of Words and Phrases and their Compositionality',
      'Link': 'https://arxiv.org/abs/1310.4546',
      'Citations': '2.87E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '2.08E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '6.92E+05',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'R-CNN (T-net)',
      'Domain': 'Vision',
      'Task': 'Object detection',
      'Organization(s)': 'UC Berkeley',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik',
      'Publication date': '11/11/2013',
      'Year': '2013',
      'Reference': 'Rich feature hierarchies for accurate object detection and semantic segmentation',
      'Link': 'https://arxiv.org/abs/1311.2524',
      'Citations': '1.91E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '6.90E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Visualizing CNNs',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'NYU',
      'Organization Categorization': 'Academia',
      'Author(s)': 'MD Zeiler, R Fergus',
      'Publication date': '12/11/2013',
      'Year': '2013',
      'Reference': 'Visualizing and Understanding Convolutional Networks',
      'Link': 'https://arxiv.org/abs/1311.2901',
      'Citations': '1.30E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '5.3E+17',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': 'Predict nearby words',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'TransE',
      'Domain': 'Other',
      'Task': 'Entity embedding',
      'Organization(s)': 'CNRS, Google',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Antoine Bordes, Nicolas Usunier, Alberto Garcia- Duran, Jason Weston, and Oksana Yakhnenko',
      'Publication date': '05/12/2013',
      'Year': '2013',
      'Reference': 'Translating Embeddings for Modeling Multi- relational Data',
      'Link': 'https://papers.nips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html',
      'Citations': '4.00E+03',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '1.3E+18',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.70E+07',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'DBLSTM',
      'Domain': 'Speech',
      'Task': 'Speech recognition',
      'Organization(s)': 'Univeristy of Toronto',
      'Organization Categorization': 'Academia',
      'Author(s)': 'A Graves, N Jaitly, A Mohamed',
      'Publication date': '08/12/2013',
      'Year': '2013',
      'Reference': 'Hybrid speech recognition with Deep Bidirectional LSTM',
      'Link': 'https://ieeexplore.ieee.org/document/6707742',
      'Citations': '1.46E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '2.99E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Network in Network',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'National University of Singapore',
      'Organization Categorization': 'Academia',
      'Author(s)': 'M Lin, Q Chen, S Yan',
      'Publication date': '16/12/2013',
      'Year': '2013',
      'Reference': 'Network In Network',
      'Link': 'https://arxiv.org/abs/1312.4400',
      'Citations': '5.50E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Image generation',
      'Domain': 'Vision',
      'Task': 'Image clustering',
      'Organization(s)': 'Univeristy of Amsterdam',
      'Organization Categorization': 'Academia',
      'Author(s)': 'DP Kingma, M Welling',
      'Publication date': '20/12/2013',
      'Year': '2013',
      'Reference': 'Auto-Encoding Variational Bayes',
      'Link': 'https://arxiv.org/abs/1312.6114',
      'Citations': '1.56E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '4.8E+14',
      'Training dataset': 'MNIST',
      'Training dataset size (datapoints)': '6.00E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'New York University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann LeCun',
      'Publication date': '21/12/2013',
      'Year': '2013',
      'Reference': 'OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks',
      'Link': 'https://arxiv.org/abs/1312.6229',
      'Citations': '5.15E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'NTM',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Google DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Alex Graves, Greg Wayne, Ivo Danihelka',
      'Publication date': '10/12/2014',
      'Year': '2014',
      'Reference': 'Neural Turing Machines',
      'Link': 'https://arxiv.org/abs/1410.5401',
      'Citations': '1.93E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'University of Toronto',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Nitish Shrivasta, Geoffrey Hinton, Alex Krizhevsky',
      'Publication date': '01/06/2014',
      'Year': '2014',
      'Reference': 'Dropout: A Simple Way to Prevent Neural Networks from Overfitting',
      'Link': 'https://jmlr.org/papers/v15/srivastava14a.html',
      'Citations': '3.19E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'SmooCT',
      'Domain': 'Games',
      'Task': '',
      'Organization(s)': 'University College London',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Johannes Heinrich, David Silver',
      'Publication date': '01/07/2014',
      'Year': '2014',
      'Reference': 'Self-Play Monte-Carlo Tree Search in Computer Poker',
      'Link': 'https://www.semanticscholar.org/paper/Self-play-Monte-Carlo-tree-search-in-computer-poker-Heinrich-Silver/7b687599b4425aa959036071030e1212a3b359c7',
      'Citations': '1.60E+01',
      'Inclusion criteria': 'SOTA improvement',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Multiresolution CNN',
      'Domain': 'Video',
      'Task': 'Video classification',
      'Organization(s)': 'Stanford, Google',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'A Karpathy, G Toderici, S Shetty, T Leung',
      'Publication date': '23/06/2014',
      'Year': '2014',
      'Reference': 'Large-Scale Video Classification with Convolutional Neural Networks',
      'Link': 'https://ieeexplore.ieee.org/document/6909619',
      'Citations': '5.90E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.26E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'GloVe (6B)',
      'Domain': 'Language',
      'Task': 'Semantic embedding',
      'Organization(s)': 'Stanford',
      'Organization Categorization': 'Academia',
      'Author(s)': 'J Pennington, R Socher, CD Manning',
      'Publication date': '01/01/2014',
      'Year': '2014',
      'Reference': 'GloVe: Global Vectors for Word Representation',
      'Link': 'https://nlp.stanford.edu/projects/glove/',
      'Citations': '2.25E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.20E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': 'Gigaword5 + Wikipedia2014',
      'Training dataset size (datapoints)': '6.00E+09',
      'Hidden layers': '',
      'Inference compute (FLOPs)': 'N/A',
      'Equivalent training time (hours)': '12.1875',
      'Inference time (ms)': '0',
      'Training dataset size (GB)': '35',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'GloVe (32B)',
      'Domain': 'Language',
      'Task': 'Semantic embedding',
      'Organization(s)': 'Stanford',
      'Organization Categorization': 'Academia',
      'Author(s)': 'J Pennington, R Socher, CD Manning',
      'Publication date': '01/01/2014',
      'Year': '2014',
      'Reference': 'GloVe: Global Vectors for Word Representation',
      'Link': 'https://nlp.stanford.edu/projects/glove/',
      'Citations': '2.25E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.20E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': 'Common Crawl',
      'Training dataset size (datapoints)': '4.20E+10',
      'Hidden layers': '',
      'Inference compute (FLOPs)': 'N/A',
      'Equivalent training time (hours)': '12.1875',
      'Inference time (ms)': '0',
      'Training dataset size (GB)': '35',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'DBNs',
      'Domain': 'Language',
      'Task': 'Text classification',
      'Organization(s)': 'Microsoft, University of Toronto',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'R Sarikaya, GE Hinton, A Deoras',
      'Publication date': '11/02/2014',
      'Year': '2014',
      'Reference': 'Application of Deep Belief Networks for Natural Language Understanding',
      'Link': 'https://ieeexplore.ieee.org/document/6737243',
      'Citations': '4.45E+02',
      'Inclusion criteria': '',
      'Parameters': '1.02E+06',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.78E+05',
      'Hidden layers': '3',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'HyperNEAT',
      'Domain': 'Games',
      'Task': 'Atari Games',
      'Organization(s)': 'University of Texas',
      'Organization Categorization': 'Academia',
      'Author(s)': 'M Hausknecht, J Lehman',
      'Publication date': '05/03/2014',
      'Year': '2014',
      'Reference': 'A Neuroevolution Approach to General Atari Game Playing',
      'Link': 'https://ieeexplore.ieee.org/abstract/document/6756960',
      'Citations': '1.95E+02',
      'Inclusion criteria': '',
      'Parameters': '2.40E+05',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'GRUs',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'University of Montreal, Jacobs University, University du Maine',
      'Organization Categorization': 'Academia',
      'Author(s)': 'K Cho, B Van Merriënboer, C Gulcehre',
      'Publication date': '03/06/2014',
      'Year': '2014',
      'Reference': 'Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation',
      'Link': 'https://arxiv.org/abs/1406.1078',
      'Citations': '1.50E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Video',
      'Task': 'Video classification',
      'Organization(s)': 'University of Oxford',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Karen Simonyan, Andrew Zisserman',
      'Publication date': '09/06/2014',
      'Year': '2014',
      'Reference': 'Two-Stream Convolutional Networks for Action Recognition in Videos',
      'Link': 'https://arxiv.org/abs/1406.2199',
      'Citations': '6.22E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'GANs',
      'Domain': 'Drawing',
      'Task': 'Image generation',
      'Organization(s)': 'Universite de Montréal',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio',
      'Publication date': '10/06/2014',
      'Year': '2014',
      'Reference': 'Generative Adversarial Networks',
      'Link': 'https://arxiv.org/abs/1406.2661',
      'Citations': '3.69E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '5.2E+17',
      'Training dataset': 'CIFAR-10',
      'Training dataset size (datapoints)': '6.00E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'SPPNet',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Microsoft, Xi’an Jiaotong University, University of Science and Technology of China',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': '',
      'Publication date': '18/06/2014',
      'Year': '2014',
      'Reference': 'Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition',
      'Link': 'https://arxiv.org/abs/1406.4729',
      'Citations': '7.41E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '6.1E+18',
      'Training dataset': 'Imagenet-1k',
      'Training dataset size (datapoints)': '1.28E+06',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Face verification',
      'Organization(s)': 'Tel Aviv University, Facebook',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Y Taigman, M Yang, MA Ranzato',
      'Publication date': '23/06/2014',
      'Year': '2014',
      'Reference': 'DeepFace: Closing the Gap to Human-Level Performance in Face Verification',
      'Link': 'https://ieeexplore.ieee.org/document/6909616',
      'Citations': '5.74E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Facebook',
      'Organization Categorization': 'Industry',
      'Author(s)': 'X He, J Pan, O Jin, T Xu, B Liu, T Xu, Y Shi',
      'Publication date': '24/08/2014',
      'Year': '2014',
      'Reference': 'Practical Lessons from Predicting Clicks on Ads at Facebook',
      'Link': 'https://dl.acm.org/doi/10.1145/2648584.2648589',
      'Citations': '5.86E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'RNNsearch-50*',
      'Domain': 'Language',
      'Task': 'Translation',
      'Organization(s)': 'Universite de Montréal, Jacobs University Bremen',
      'Organization Categorization': 'Academia',
      'Author(s)': 'D Bahdanau, K Cho, Y Bengio',
      'Publication date': '01/09/2014',
      'Year': '2014',
      'Reference': 'Neural Machine Translation by Jointly Learning to Align and Translate',
      'Link': 'https://arxiv.org/abs/1409.0473',
      'Citations': '1.92E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '1.6E+18',
      'Training dataset': 'WMT\'14 + selection',
      'Training dataset size (datapoints)': '3.84E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'VGG16',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'University of Oxford',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Karen Simonyan; Andrew Zisserman',
      'Publication date': '04/09/2014',
      'Year': '2014',
      'Reference': 'Very Deep Convolutional Networks for Large-Scale Image Recognition',
      'Link': 'https://arxiv.org/abs/1409.1556',
      'Citations': '6.13E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.38E+08',
      'Training compute (FLOPs)': '8.5E+18',
      'Training dataset': 'ILSVRC-2012',
      'Training dataset size (datapoints)': '1.30E+06',
      'Hidden layers': '16',
      'Inference compute (FLOPs)': '1.53E+10',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'VGG19',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'University of Oxford',
      'Organization Categorization': 'Academia',
      'Author(s)': 'K Simonyan, A Zisserman',
      'Publication date': '04/09/2014',
      'Year': '2014',
      'Reference': 'Very Deep Convolutional Networks for Large-Scale Image Recognition',
      'Link': 'https://arxiv.org/abs/1409.1556',
      'Citations': '6.13E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.44E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': 'ILSVRC-2012',
      'Training dataset size (datapoints)': '1.30E+06',
      'Hidden layers': '19',
      'Inference compute (FLOPs)': '1.96E+10',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Seq2Seq',
      'Domain': 'Language',
      'Task': 'Translation',
      'Organization(s)': 'Google',
      'Organization Categorization': 'Industry',
      'Author(s)': 'I Sutskever, O Vinyals, QV Le',
      'Publication date': '10/09/2014',
      'Year': '2014',
      'Reference': 'Sequence to Sequence Learning with Neural Networks',
      'Link': 'https://arxiv.org/abs/1409.3215',
      'Citations': '1.57E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '3.84E+08',
      'Training compute (FLOPs)': '7.3E+18',
      'Training dataset': 'WMT\'14 dataset',
      'Training dataset size (datapoints)': '3.84E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'LRCN',
      'Domain': 'Vision',
      'Task': 'Video description',
      'Organization(s)': 'UT Austin, UMass Lowell, UC Berkeley',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Jeff Donahue, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, Sergio Guadarrama, Kate Saenko, Trevor Darrell',
      'Publication date': '07/11/2014',
      'Year': '2014',
      'Reference': 'Long-term Recurrent Convolutional Networks for Visual Recognition and Description',
      'Link': 'https://arxiv.org/abs/1411.4389',
      'Citations': '5.22E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.43E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': 'TaCoS',
      'Training dataset size (datapoints)': '4.00E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': 'Reinforcement learning',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Image segmentation',
      'Organization(s)': 'University of California, Berkeley',
      'Organization Categorization': 'Academia',
      'Author(s)': 'J Long, E Shelhamer, T Darrell',
      'Publication date': '14/11/2014',
      'Year': '2014',
      'Reference': 'Fully Convolutional Networks for Semantic Segmentation',
      'Link': 'https://arxiv.org/abs/1411.4038',
      'Citations': '2.47E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'The Chinese University of Hong Kong',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Z Liu, P Luo, X Wang, X Tang',
      'Publication date': '28/11/2014',
      'Year': '2014',
      'Reference': 'Deep Learning Face Attributes in the Wild',
      'Link': 'https://arxiv.org/abs/1411.7766',
      'Citations': '4.01E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'ADAM (CIFAR-10)',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'University of Amsterdam, OpenAI, University of Toronto',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'DP Kingma, J Ba',
      'Publication date': '22/12/2014',
      'Year': '2014',
      'Reference': 'Adam: A Method for Stochastic Optimization',
      'Link': 'https://arxiv.org/abs/1412.6980',
      'Citations': '8.11E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '6.0E+16',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'DeepLab',
      'Domain': 'Vision',
      'Task': 'Image segmentation',
      'Organization(s)': 'Google, University of California Los Angeles',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille',
      'Publication date': '22/12/2014',
      'Year': '2014',
      'Reference': 'Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs',
      'Link': 'https://arxiv.org/abs/1412.7062',
      'Citations': '3.70E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'MSRA (C, PReLU)',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Microsoft research',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun',
      'Publication date': '09/01/2015',
      'Year': '2015',
      'Reference': 'Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition',
      'Link': 'https://arxiv.org/abs/1406.4729',
      'Citations': '1.41E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '8.70E+07',
      'Training compute (FLOPs)': '2.4E+19',
      'Training dataset': 'Imagenet-1k',
      'Training dataset size (datapoints)': '1.28E+06',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Image segmentation',
      'Organization(s)': 'University of Oxford, Stanford University, Baidu',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, Philip H. S. Torr',
      'Publication date': '11/02/2015',
      'Year': '2015',
      'Reference': 'Conditional Random Fields as Recurrent Neural Networks',
      'Link': 'https://arxiv.org/abs/1502.03240',
      'Citations': '2.66E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'DQN-2015',
      'Domain': 'Games',
      'Task': 'Atari Games',
      'Organization(s)': 'Google',
      'Organization Categorization': 'Industry',
      'Author(s)': 'V Mnih, K Kavukcuoglu, D Silver, AA Rusu, J Veness',
      'Publication date': '25/02/2015',
      'Year': '2015',
      'Reference': 'Human-level control through deep reinforcement learning',
      'Link': 'https://www.nature.com/articles/nature14236',
      'Citations': '1.57E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.62E+06',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Constituency-Tree LSTM',
      'Domain': 'Language',
      'Task': 'Semantic embedding',
      'Organization(s)': 'Stanford, MetaMind Inc',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'KS Tai, R Socher, CD Manning',
      'Publication date': '28/02/2015',
      'Year': '2015',
      'Reference': 'Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks',
      'Link': 'https://arxiv.org/abs/1503.00075',
      'Citations': '2.62E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '2.05E+05',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Fast R-CNN',
      'Domain': 'Vision',
      'Task': 'Object detection',
      'Organization(s)': 'Microsoft Research',
      'Organization Categorization': 'Industry',
      'Author(s)': 'R Girshick',
      'Publication date': '30/04/2015',
      'Year': '2015',
      'Reference': 'Fast R-CNN',
      'Link': 'https://arxiv.org/abs/1504.08083',
      'Citations': '1.58E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Video',
      'Organization(s)': 'University of Maryland, University of Texas, Google Inc.',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat Monga, George Toderici',
      'Publication date': '01/05/2015',
      'Year': '2015',
      'Reference': 'Beyond Short Snippets: Deep Networks for Video Classification',
      'Link': 'https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Ng_Beyond_Short_Snippets_2015_CVPR_paper.html',
      'Citations': '2.26E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'DSN',
      'Domain': 'VIsion',
      'Task': 'Image classification',
      'Organization(s)': 'University of California, Microsoft Research',
      'Organization Categorization': 'Industry - Academia Collaboration (Academia leaning)',
      'Author(s)': 'Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, Zhuowen Tu',
      'Publication date': '09/05/2015',
      'Year': '2015',
      'Reference': 'Deeply-Supervised Nets',
      'Link': 'http://proceedings.mlr.press/v38/lee15a.html',
      'Citations': '1.94E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Action recognition',
      'Organization(s)': 'Chinese University of Hong Kong, Chinese Academy of Sciences',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Limin Wang, Yu Qiao, Xiaoou Tang',
      'Publication date': '01/06/2015',
      'Year': '2015',
      'Reference': 'Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors',
      'Link': 'https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Wang_Action_Recognition_With_2015_CVPR_paper.html',
      'Citations': '3.47E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Faster R-CNN',
      'Domain': 'Vision',
      'Task': 'Object detection',
      'Organization(s)': 'Microsoft Research',
      'Organization Categorization': 'Industry',
      'Author(s)': 'S Ren, K He, R Girshick, J Sun',
      'Publication date': '04/06/2015',
      'Year': '2015',
      'Reference': 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks',
      'Link': 'https://arxiv.org/abs/1506.01497',
      'Citations': '2.30E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'GoogLeNet / InceptionV1',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Google, University of Michigan, University of North Carolina',
      'Organization Categorization': 'Industry - Academia Collaboration (Industry leaning)',
      'Author(s)': 'Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich',
      'Publication date': '07/06/2015',
      'Year': '2015',
      'Reference': 'Going deeper with convolutions',
      'Link': 'https://arxiv.org/abs/1409.4842',
      'Citations': '3.28E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '6.80E+06',
      'Training compute (FLOPs)': '1.6E+18',
      'Training dataset': 'ILSVRC 2014',
      'Training dataset size (datapoints)': '1.20E+06',
      'Hidden layers': '22',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'YOLO',
      'Domain': 'Vision',
      'Task': 'Object detection',
      'Organization(s)': 'University of Washington',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'J Redmon, S Divvala, R Girshick',
      'Publication date': '08/06/2015',
      'Year': '2015',
      'Reference': 'You Only Look Once: Unified, Real-Time Object Detection',
      'Link': 'https://arxiv.org/abs/1506.02640',
      'Citations': '1.75E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '6.45E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'BatchNorm',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Google',
      'Organization Categorization': 'Industry',
      'Author(s)': 'S Ioffe, C Szegedy',
      'Publication date': '15/06/2015',
      'Year': '2015',
      'Reference': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift',
      'Link': 'https://arxiv.org/abs/1502.03167',
      'Citations': '2.92E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Google, Carnegie Mellon University',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals',
      'Publication date': '20/08/2015',
      'Year': '2015',
      'Reference': 'Listen, attend and spell: A neural network for large vocabulary conversational speech recognition',
      'Link': 'https://ieeexplore.ieee.org/document/7472621',
      'Citations': '1.62E+03',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'BPE',
      'Domain': 'Language',
      'Task': 'Translation',
      'Organization(s)': 'University of Edinburgh',
      'Organization Categorization': 'Academia',
      'Author(s)': 'R Sennrich, B Haddow, A Birch',
      'Publication date': '31/08/2015',
      'Year': '2015',
      'Reference': 'Neural Machine Translation of Rare Words with Subword Units',
      'Link': 'https://arxiv.org/abs/1508.07909',
      'Citations': '4.06E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': 'WMT\'15',
      'Training dataset size (datapoints)': '3.75E+07',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Google DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'TP Lillicrap, JJ Hunt, A Pritzel, N Heess, T Erez',
      'Publication date': '09/09/2015',
      'Year': '2015',
      'Reference': 'Continuous control with deep reinforcement learning',
      'Link': 'https://arxiv.org/abs/1509.02971',
      'Citations': '6.35E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'AlphaGo Fan',
      'Domain': 'Games',
      'Task': 'Go',
      'Organization(s)': 'Google DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'D Silver, A Huang, CJ Maddison, A Guez, L Sifre',
      'Publication date': '01/10/2015',
      'Year': '2015',
      'Reference': 'Mastering the game of Go with deep neural networks and tree search',
      'Link': 'https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ',
      'Citations': '5.18E+03',
      'Inclusion criteria': 'SOTA improvement',
      'Parameters': '8.21E+06',
      'Training compute (FLOPs)': '3.8E+20',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Google DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Z Wang, T Schaul, M Hessel',
      'Publication date': '20/11/2015',
      'Year': '2015',
      'Reference': 'Dueling Network Architectures for Deep Reinforcement Learning',
      'Link': 'https://arxiv.org/abs/1511.06581',
      'Citations': '2.00E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Image segmentation',
      'Organization(s)': 'Princeton University, Intel Labs',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Fisher Yu, Vladlen Koltun',
      'Publication date': '23/11/2015',
      'Year': '2015',
      'Reference': 'Multi-Scale Context Aggregation by Dilated Convolutions',
      'Link': 'https://arxiv.org/abs/1511.07122',
      'Citations': '5.84E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Recommendation',
      'Task': '',
      'Organization(s)': 'Netflix',
      'Organization Categorization': 'Industry',
      'Author(s)': 'CA Gomez-Uribe, N Hunt',
      'Publication date': '12/2015',
      'Year': '2015',
      'Reference': 'The Netflix Recommender System: Algorithms, Business Value, and Innovation',
      'Link': 'https://dl.acm.org/doi/pdf/10.1145/2843948',
      'Citations': '1.09E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Inception v3',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Google, University College London',
      'Organization Categorization': 'Industry - Academia Collaboration (Industry leaning)',
      'Author(s)': 'Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna',
      'Publication date': '02/12/2015',
      'Year': '2015',
      'Reference': 'Rethinking the inception architecture for computer vision.',
      'Link': 'https://arxiv.org/abs/1512.00567',
      'Citations': '1.47E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '2.36E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': 'ILSVRC 2012',
      'Training dataset size (datapoints)': '1.20E+06',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '1.15E+11',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': 'Score',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'DeepSpeech2',
      'Domain': 'Speech',
      'Task': 'Speech recognition',
      'Organization(s)': 'Baidu Research- Silicon Valley AI Lab',
      'Organization Categorization': 'Industry',
      'Author(s)': 'D Amodei, S Ananthanarayanan',
      'Publication date': '08/12/2015',
      'Year': '2015',
      'Reference': 'Deep Speech 2: End-to-End Speech Recognition in English and Mandarin',
      'Link': 'https://arxiv.org/abs/1512.02595',
      'Citations': '2.21E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '3.80E+07',
      'Training compute (FLOPs)': '2.6E+19',
      'Training dataset': '',
      'Training dataset size (datapoints)': '9.80E+09',
      'Hidden layers': '11',
      'Inference compute (FLOPs)': '1.80E+09',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'ResNet-152 (ImageNet)',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Microsoft',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun',
      'Publication date': '10/12/2015',
      'Year': '2015',
      'Reference': 'Deep Residual Learning for Image Recognition',
      'Link': 'https://arxiv.org/abs/1512.03385',
      'Citations': '8.58E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '6.00E+07',
      'Training compute (FLOPs)': '1.2E+19',
      'Training dataset': 'ILSVRC 2012',
      'Training dataset size (datapoints)': '1.20E+06',
      'Hidden layers': '152',
      'Inference compute (FLOPs)': '2.26E+10',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '138',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'ResNet-110 (CIFAR-10)',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Microsoft',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun',
      'Publication date': '10/12/2015',
      'Year': '2015',
      'Reference': 'Deep Residual Learning for Image Recognition',
      'Link': 'https://arxiv.org/abs/1512.03385',
      'Citations': '8.58E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.70E+06',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '110',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'BPL',
      'Domain': 'Drawing',
      'Task': 'Image generation',
      'Organization(s)': 'NYU, University of Toronto, MIT',
      'Organization Categorization': 'Academia',
      'Author(s)': 'BM Lake, R Salakhutdinov, JB Tenenbaum',
      'Publication date': '11/12/2015',
      'Year': '2015',
      'Reference': 'Human-level concept learning through probabilistic program induction',
      'Link': 'https://science.sciencemag.org/content/350/6266/1332/',
      'Citations': '2.01E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Advantage Learning',
      'Domain': 'Games',
      'Task': 'Atari Games',
      'Organization(s)': 'Google DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'MG Bellemare, G Ostrovski, A Guez',
      'Publication date': '15/12/2015',
      'Year': '2015',
      'Reference': 'Increasing the Action Gap: New Operators for Reinforcement Learning',
      'Link': 'http://arxiv.org/abs/1512.04860v1',
      'Citations': '1.04E+02',
      'Inclusion criteria': 'SOTA improvement',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Image super-resolution',
      'Organization(s)': 'Seoul National University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee',
      'Publication date': '11/11/2016',
      'Year': '2016',
      'Reference': 'Deeply-Recursive Convolutional Network for Image Super-Resolution',
      'Link': 'https://arxiv.org/abs/1511.04491',
      'Citations': '1.97E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'AlphaGo Lee',
      'Domain': 'Games',
      'Task': 'Go',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'D Silver, A Huang, CJ Maddison, A Guez, L Sifre',
      'Publication date': '27/01/2016',
      'Year': '2016',
      'Reference': 'Mastering the game of Go with deep neural networks and tree search',
      'Link': 'https://www.nature.com/articles/nature16961',
      'Citations': '1.08E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '1.9E+21',
      'Training dataset': '',
      'Training dataset size (datapoints)': '2.94E+07',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Pose estimation',
      'Organization(s)': 'The Robotics Institute, Carnegie Mellon University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Shih-En Wei, Varun Ramakrishna, Takeo Kanade, Yaser Sheikh',
      'Publication date': '30/01/2016',
      'Year': '2016',
      'Reference': 'Convolutional Pose Machines',
      'Link': 'https://arxiv.org/abs/1602.00134',
      'Citations': '2.42E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'A3C FF hs',
      'Domain': 'Games',
      'Task': 'Atari Games',
      'Organization(s)': 'Google, University of Montreal',
      'Organization Categorization': 'Industry - Academia Collaboration (Industry leaning)',
      'Author(s)': 'V Mnih, AP Badia, M Mirza, A Graves',
      'Publication date': '04/02/2016',
      'Year': '2016',
      'Reference': 'Asynchronous Methods for Deep Reinforcement Learning',
      'Link': 'http://arxiv.org/abs/1602.01783v2',
      'Citations': '5.28E+03',
      'Inclusion criteria': 'SOTA improvement',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Inceptionv4',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Google',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi',
      'Publication date': '23/02/2016',
      'Year': '2016',
      'Reference': 'Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning',
      'Link': 'https://arxiv.org/abs/1602.07261',
      'Citations': '8.21E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '4.30E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '2.46E+10',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Inception-ResNet-V2',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Google',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi',
      'Publication date': '23/02/2016',
      'Year': '2016',
      'Reference': 'Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning',
      'Link': 'https://arxiv.org/abs/1602.07261',
      'Citations': '8.21E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '5.60E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '2.64E+09',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'SqueezeNet',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'DeepScale, UC Berkeley, Stanford',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer',
      'Publication date': '24/02/2016',
      'Year': '2016',
      'Reference': 'SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size',
      'Link': 'https://arxiv.org/abs/1602.07360',
      'Citations': '4.40E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '0.00E+00',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Image super-resolution',
      'Organization(s)': 'Nanjing University, University of Adelaide',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Xiao-Jiao Mao, Chunhua Shen, Yu-Bin Yang',
      'Publication date': '30/03/2016',
      'Year': '2016',
      'Reference': 'Image Restoration Using Very Deep Convolutional Encoder-Decoder Networks with Symmetric Skip Connections',
      'Link': 'https://arxiv.org/abs/1603.09056v2',
      'Citations': '1.18E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Video',
      'Organization(s)': 'Graz University of Technology, University of Oxford',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Christoph Feichtenhofer, Axel Pinz, Andrew Zisserman',
      'Publication date': '01/06/2016',
      'Year': '2016',
      'Reference': 'Convolutional Two-Stream Network Fusion for Video Action Recognition',
      'Link': 'https://openaccess.thecvf.com/content_cvpr_2016/html/Feichtenhofer_Convolutional_Two-Stream_Network_CVPR_2016_paper.html',
      'Citations': '2.28E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'DMN',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Salesforce',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, Richard Socher',
      'Publication date': '20/06/2016',
      'Year': '2016',
      'Reference': 'Ask Me Anything: Dynamic Memory Networks for Natural Language Processing',
      'Link': 'https://arxiv.org/abs/1506.07285',
      'Citations': '1.19E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'R-FCN',
      'Domain': 'Vision',
      'Task': 'Object detection',
      'Organization(s)': 'Microsoft research, Tsinghua university',
      'Organization Categorization': 'Industry - Academia Collaboration (Industry leaning)',
      'Author(s)': 'Jifeng Dai, Y. Li, Kaiming He, and Jian Sun',
      'Publication date': '21/06/2016',
      'Year': '2016',
      'Reference': 'R-fcn: Object detection via region-based fully convolutional networks.',
      'Link': 'https://arxiv.org/abs/1605.06409',
      'Citations': '4.49E+03',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '6.1E+16',
      'Training dataset': 'PASCAL VOC (2007 and 2012 vesrions) + MS COCO',
      'Training dataset size (datapoints)': '9.44E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '12.06567222',
      'Inference time (ms)': '170',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Recommendation',
      'Task': '',
      'Organization(s)': 'Google',
      'Organization Categorization': 'Industry',
      'Author(s)': 'HT Cheng, L Koc, J Harmsen, T Shaked',
      'Publication date': '24/06/2016',
      'Year': '2016',
      'Reference': 'Wide & Deep Learning for Recommender Systems',
      'Link': 'https://arxiv.org/abs/1606.07792',
      'Citations': '1.61E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Facebook AI research',
      'Organization Categorization': 'Industry',
      'Author(s)': 'A Joulin, E Grave, P Bojanowski, T Mikolov',
      'Publication date': '06/07/2016',
      'Year': '2016',
      'Reference': 'Bag of Tricks for Efficient Text Classification',
      'Link': 'https://arxiv.org/abs/1607.01759',
      'Citations': '3.09E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Facebook AI research',
      'Organization Categorization': 'Industry',
      'Author(s)': 'P Bojanowski, E Grave, A Joulin',
      'Publication date': '15/07/2016',
      'Year': '2016',
      'Reference': 'Enriching Word Vectors with Subword Information',
      'Link': 'https://arxiv.org/abs/1607.04606',
      'Citations': '6.35E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Part-of-sentence tagging model',
      'Domain': 'Language',
      'Task': 'POS tagging',
      'Organization(s)': 'University of Toronto',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton',
      'Publication date': '21/07/2016',
      'Year': '2016',
      'Reference': 'Layer Normalization.',
      'Link': 'https://arxiv.org/abs/1607.06450',
      'Citations': '4.13E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '1.5E+17',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '12',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Named Entity Recognition model',
      'Domain': 'Language',
      'Task': 'Named Entity Recognition model',
      'Organization(s)': 'University of Toronto',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton',
      'Publication date': '21/07/2016',
      'Year': '2016',
      'Reference': 'Layer Normalization.',
      'Link': 'https://arxiv.org/abs/1607.06450',
      'Citations': '4.13E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '9.7E+16',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '8',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'DenseNet-264',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Tsinghua University, Cornell, Facebook AI research',
      'Organization Categorization': 'Industry - Academia Collaboration (Academia leaning)',
      'Author(s)': 'G Huang, Z Liu, L Van Der Maaten',
      'Publication date': '25/08/2016',
      'Year': '2016',
      'Reference': 'Densely Connected Convolutional Networks',
      'Link': 'https://arxiv.org/abs/1608.06993',
      'Citations': '1.78E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '3.40E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Face detection',
      'Organization(s)': 'Chinese Academy of Sciences, Chinese University of Hong Kong',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, Yu Qiao',
      'Publication date': '26/08/2016',
      'Year': '2016',
      'Reference': 'Joint Face Detection and Alignment using Multitask cascaded convolutional networks',
      'Link': 'https://arxiv.org/abs/1604.02878',
      'Citations': '3.40E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Speech',
      'Task': '',
      'Organization(s)': 'Google DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'A Oord, S Dieleman, H Zen, K Simonyan',
      'Publication date': '12/09/2016',
      'Year': '2016',
      'Reference': 'WaveNet: A Generative Model for Raw Audio',
      'Link': 'https://arxiv.org/abs/1609.03499',
      'Citations': '3.12E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Recommendation',
      'Task': '',
      'Organization(s)': 'Google',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Paul Covington, Jay Adams, and Emre Sargin',
      'Publication date': '15/09/2016',
      'Year': '2016',
      'Reference': 'Deep Neural Networks for YouTube Recommendations',
      'Link': 'https://research.google/pubs/pub45530/',
      'Citations': '1.55E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Stacked hourglass network',
      'Domain': 'Vision',
      'Task': 'Pose estimation',
      'Organization(s)': 'University of Michigan',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Alejandro Newell, Kaiyu Yang, Jia Deng',
      'Publication date': '17/09/2016',
      'Year': '2016',
      'Reference': 'Stacked Hourglass Networks for Human Pose Estimation',
      'Link': 'https://link.springer.com/chapter/10.1007/978-3-319-46484-8_29',
      'Citations': '3.60E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Microsoft',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun',
      'Publication date': '17/09/2016',
      'Year': '2016',
      'Reference': 'Identity Mappings in Deep Residual Networks',
      'Link': 'https://link.springer.com/chapter/10.1007/978-3-319-46493-0_38',
      'Citations': '6.89E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Action recognition',
      'Organization(s)': 'ETH Zurich, The Chinese University of Hong Kong, Shenzhen Institute of Advanced Technology',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, Luc Van Gool',
      'Publication date': '17/09/2016',
      'Year': '2016',
      'Reference': 'Temporal Segment Networks: Towards Good Practices for Deep Action Recognition',
      'Link': 'https://link.springer.com/chapter/10.1007/978-3-319-46484-8_2',
      'Citations': '2.62E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'MS-CNN',
      'Domain': 'Vision',
      'Task': 'Object detection',
      'Organization(s)': 'SVCL UC San Diego, IBM',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Zhaowei Cai, Quanfu Fan, Rogerio S. Feris, Nuno Vasconcelos',
      'Publication date': '17/09/2016',
      'Year': '2016',
      'Reference': 'A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection',
      'Link': 'https://link.springer.com/chapter/10.1007/978-3-319-46493-0_22',
      'Citations': '1.32E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Wide Residual Network',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Université Paris-Est',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Sergey Zagoruyko, Nikos Komodakis',
      'Publication date': '19/09/2016',
      'Year': '2016',
      'Reference': 'Wide Residual Networks',
      'Link': 'https://arxiv.org/abs/1605.07146',
      'Citations': '4.52E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'GNMT',
      'Domain': 'Language',
      'Task': 'Translation',
      'Organization(s)': 'Google',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean',
      'Publication date': '26/09/2016',
      'Year': '2016',
      'Reference': 'Google\'s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation',
      'Link': 'https://research.google/pubs/pub45610/',
      'Citations': '4.50E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '2.78E+08',
      'Training compute (FLOPs)': '6.9E+21',
      'Training dataset': '',
      'Training dataset size (datapoints)': '3.60E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Xception',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Google',
      'Organization Categorization': 'Industry',
      'Author(s)': 'François Chollet',
      'Publication date': '07/10/2016',
      'Year': '2016',
      'Reference': 'Xception: Deep Learning with Depthwise Separable Convolutions',
      'Link': 'https://arxiv.org/abs/1610.02357',
      'Citations': '5.84E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '2.29E+07',
      'Training compute (FLOPs)': '4.4E+19',
      'Training dataset': 'JFT',
      'Training dataset size (datapoints)': '3.50E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '1.68E+10',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Google DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwińska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, Adrià Puigdomènech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu & Demis Hassabis',
      'Publication date': '12/10/2016',
      'Year': '2016',
      'Reference': 'Hybrid computing using a neural network with dynamic external memory',
      'Link': 'https://www.nature.com/articles/nature20101',
      'Citations': '1.24E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'NASv3 (CIFAR-10)',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'Google Brain',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Barret Zoph, Quoc V. Le',
      'Publication date': '05/11/2016',
      'Year': '2016',
      'Reference': 'Neural Architecture Search with Reinforcement Learning',
      'Link': 'https://arxiv.org/abs/1611.01578',
      'Citations': '2.97E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '3.74E+07',
      'Training compute (FLOPs)': '2.2E+21',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '39',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'ResNeXt-50',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'UC San Diego, Facebook',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He',
      'Publication date': '16/11/2016',
      'Year': '2016',
      'Reference': 'Aggregated Residual Transformations for Deep Neural Networks',
      'Link': 'https://arxiv.org/abs/1611.05431',
      'Citations': '4.80E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '2.50E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '8.40E+09',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'PolyNet',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'The Chinese University of Hong Kong',
      'Organization Categorization': 'Academia',
      'Author(s)': 'X Zhang, Z Li, C Change Loy',
      'Publication date': '17/11/2016',
      'Year': '2016',
      'Reference': 'PolyNet: A Pursuit of Structural Diversity in Very Deep Networks',
      'Link': 'https://arxiv.org/abs/1611.05725',
      'Citations': '1.88E+02',
      'Inclusion criteria': '',
      'Parameters': '9.20E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'RefineNet',
      'Domain': 'Vision',
      'Task': 'Object detection',
      'Organization(s)': 'University of Adelaide, Australian Centre for Robotic Vision',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Guosheng Lin, Anton Milan, Chunhua Shen, Ian Reid',
      'Publication date': '20/11/2016',
      'Year': '2016',
      'Reference': 'RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation',
      'Link': 'https://arxiv.org/abs/1611.06612v3',
      'Citations': '2.06E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'UC Berkeley',
      'Organization Categorization': 'Academia',
      'Author(s)': 'P Isola, JY Zhu, T Zhou',
      'Publication date': '21/11/2016',
      'Year': '2016',
      'Reference': 'Image-to-Image Translation with Conditional Adversarial Networks',
      'Link': 'https://arxiv.org/abs/1611.07004',
      'Citations': '9.86E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'J Kirkpatrick, R Pascanu',
      'Publication date': '02/12/2016',
      'Year': '2016',
      'Reference': 'Overcoming catastrophic forgetting in neural networks',
      'Link': 'https://arxiv.org/abs/1612.00796',
      'Citations': '2.16E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'PointNet',
      'Domain': 'Other',
      'Task': '3d segmentation',
      'Organization(s)': 'Stanford',
      'Organization Categorization': 'Academia',
      'Author(s)': 'CR Qi, H Su, K Mo, LJ Guibas',
      'Publication date': '02/12/2016',
      'Year': '2016',
      'Reference': 'PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation',
      'Link': 'https://arxiv.org/abs/1612.00593',
      'Citations': '5.04E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'OpenAI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen',
      'Publication date': '05/12/2016',
      'Year': '2016',
      'Reference': 'Improved Techniques for Training GANs',
      'Link': 'https://dl.acm.org/doi/10.5555/3157096.3157346',
      'Citations': '6.06E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'UT Austin, Google Inc, UC Berkeley',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'V Gulshan, L Peng, M Coram, MC Stumpe, D Wu',
      'Publication date': '13/12/2016',
      'Year': '2016',
      'Reference': 'Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs',
      'Link': 'https://jamanetwork.com/journals/jama/article-abstract/2588763',
      'Citations': '3.54E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'YOLOv2',
      'Domain': 'Vision',
      'Task': 'Object detection',
      'Organization(s)': 'University of Washington, Allen Institute for AI',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Joseph Redmon, Ali Farhadi',
      'Publication date': '25/12/2016',
      'Year': '2016',
      'Reference': 'YOLO9000: Better, Faster, Stronger',
      'Link': 'https://arxiv.org/abs/1612.08242',
      'Citations': '9.37E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '5.10E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Image super-resolution',
      'Organization(s)': 'Twitter',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi',
      'Publication date': '25/05/2017',
      'Year': '2017',
      'Reference': 'Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network',
      'Link': 'https://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html',
      'Citations': '7.18E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Libratus',
      'Domain': 'Games',
      'Task': 'Poker',
      'Organization(s)': 'Carnegie Mellon University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'N Brown, T Sandholm, S Machine',
      'Publication date': '01/01/2017',
      'Year': '2017',
      'Reference': 'Libratus: The Superhuman AI for No-Limit Poker',
      'Link': 'https://www.cs.cmu.edu/~noamb/papers/17-IJCAI-Libratus.pdf',
      'Citations': '6.40E+01',
      'Inclusion criteria': 'SOTA improvement',
      'Parameters': '',
      'Training compute (FLOPs)': '1.1E+21',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '3000000',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'AlphaGo Master',
      'Domain': 'Games',
      'Task': 'Go',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'D Silver, J Schrittwieser, K Simonyan, I Antonoglou',
      'Publication date': '01/01/2017',
      'Year': '2017',
      'Reference': 'Mastering the game of Go without human knowledge',
      'Link': 'https://www.researchgate.net/publication/320473480_Mastering_the_game_of_Go_without_human_knowledge',
      'Citations': '5.81E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '1.5E+23',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'DeepStack',
      'Domain': 'Games',
      'Task': 'Poker',
      'Organization(s)': 'University of Alberta, Charles University, Czech Technical University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Matej Moravčík, Martin Schmid, Neil Burch, Viliam Lisý, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, Michael Bowling',
      'Publication date': '06/01/2017',
      'Year': '2017',
      'Reference': 'DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker',
      'Link': 'https://arxiv.org/abs/1701.01724',
      'Citations': '6.18E+02',
      'Inclusion criteria': '',
      'Parameters': '2.50E+06',
      'Training compute (FLOPs)': '1.5E+14',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.00E+07',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'MoE',
      'Domain': 'Language',
      'Task': 'Language modelling / Machine translation',
      'Organization(s)': 'Google Brain, Jagiellonian University, Cracow',
      'Organization Categorization': 'Industry - Academia Collaboration (Industry leaning)',
      'Author(s)': 'N Shazeer, A Mirhoseini, K Maziarz, A Davis',
      'Publication date': '23/01/2017',
      'Year': '2017',
      'Reference': 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer',
      'Link': 'https://arxiv.org/abs/1701.06538',
      'Citations': '6.87E+02',
      'Inclusion criteria': '',
      'Parameters': '8.70E+09',
      'Training compute (FLOPs)': '9.4E+19',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.00E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': 'Sparse',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'VIsion',
      'Task': 'Image super-resolution',
      'Organization(s)': 'Harbin Institute of Technology, Hong Kong Polytechnic University, ULSee Inc., Xi’an Jiaotong University',
      'Organization Categorization': 'Industry - Academia Collaboration (Academia leaning)',
      'Author(s)': 'Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, Lei Zhang',
      'Publication date': '01/02/2017',
      'Year': '2017',
      'Reference': 'Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising',
      'Link': 'https://ieeexplore.ieee.org/abstract/document/7839189',
      'Citations': '4.00E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'University of Toronto, Twitter',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': ' Jake Snell, Kevin Swersky, Richard S. Zemel',
      'Publication date': '15/03/2017',
      'Year': '2017',
      'Reference': 'Prototypical Networks for Few-shot Learning',
      'Link': 'https://arxiv.org/abs/1703.05175',
      'Citations': '3.57E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Mask R-CNN',
      'Domain': 'Vision',
      'Task': 'Image segmentation',
      'Organization(s)': 'Facebook AI Research',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick',
      'Publication date': '30/03/2017',
      'Year': '2017',
      'Reference': 'Mask R-CNN',
      'Link': 'https://arxiv.org/abs/1703.06870',
      'Citations': '1.50E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': 'COCO',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '352',
      'Inference time (ms)': '195',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Image generation',
      'Organization(s)': 'Montreal Institute for learning Algorithms, Courant Institute of Mathematical Sciences',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville',
      'Publication date': '31/03/2017',
      'Year': '2017',
      'Reference': 'Improved Training of Wasserstein GANs',
      'Link': 'https://arxiv.org/abs/1704.00028',
      'Citations': '6.04E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'MobileNet',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'Google Inc.',
      'Organization Categorization': 'Industry',
      'Author(s)': 'AG Howard, M Zhu, B Chen, D Kalenichenko',
      'Publication date': '17/04/2017',
      'Year': '2017',
      'Reference': 'MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications',
      'Link': 'https://arxiv.org/abs/1704.04861',
      'Citations': '9.19E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '4.20E+06',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '1.14E+09',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Image segmentation',
      'Organization(s)': 'Google Inc., University King College, Johns Hopkins University',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille',
      'Publication date': '27/04/2017',
      'Year': '2017',
      'Reference': 'DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs',
      'Link': 'https://ieeexplore.ieee.org/abstract/document/7913730',
      'Citations': '1.01E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Action recognition',
      'Organization(s)': 'DeepMind, University of Oxford',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Joao Carreira, Andrew Zisserman',
      'Publication date': '01/06/2017',
      'Year': '2017',
      'Reference': 'Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset',
      'Link': 'https://arxiv.org/abs/1705.07750',
      'Citations': '3.98E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'PointNet++',
      'Domain': '',
      'Task': '3D segmentation',
      'Organization(s)': 'Stanford University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Charles R. Qi, Li Yi, Hao Su, Leonidas J. Guibas',
      'Publication date': '07/06/2017',
      'Year': '2017',
      'Reference': 'PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space',
      'Link': 'https://arxiv.org/abs/1706.02413',
      'Citations': '4.02E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Image super-resolution',
      'Organization(s)': 'Seoul National University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee',
      'Publication date': '10/06/2017',
      'Year': '2017',
      'Reference': 'Enhanced Deep Residual Networks for Single Image Super-Resolution',
      'Link': 'https://arxiv.org/abs/1707.02921',
      'Citations': '3.07E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Transformer',
      'Domain': 'Language',
      'Task': 'Translation',
      'Organization(s)': 'Google Brain ; Google Research',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',
      'Publication date': '12/06/2017',
      'Year': '2017',
      'Reference': 'Attention Is All You Need',
      'Link': 'https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf',
      'Citations': '2.52E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '2.13E+08',
      'Training compute (FLOPs)': '7.4E+18',
      'Training dataset': '',
      'Training dataset size (datapoints)': '3.60E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '5.40E+10',
      'Equivalent training time (hours)': '672',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'HRA',
      'Domain': 'Games',
      'Task': 'Ms Pacman',
      'Organization(s)': 'Microsoft Maluuba',
      'Organization Categorization': 'Industry - Academia Collaboration (Industry leaning)',
      'Author(s)': 'H Van Seijen, M Fatemi, J Romoff, R Laroche',
      'Publication date': '13/06/2017',
      'Year': '2017',
      'Reference': 'Hybrid Reward Architecture for Reinforcement Learning',
      'Link': 'https://arxiv.org/abs/1706.04208',
      'Citations': '1.68E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'DeepLabV3',
      'Domain': 'Vision',
      'Task': 'Semantic segmentation',
      'Organization(s)': 'Google Inc',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam',
      'Publication date': '17/06/2017',
      'Year': '2017',
      'Reference': 'Rethinking Atrous Convolution for Semantic Image Segmentation',
      'Link': 'https://arxiv.org/abs/1706.05587',
      'Citations': '3.90E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Image generation',
      'Organization(s)': 'Johannes Kepler University Linz',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Günter Klambauer, Sepp Hochreiter',
      'Publication date': '26/06/2017',
      'Year': '2017',
      'Reference': 'GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium',
      'Link': 'https://arxiv.org/abs/1706.08500v1',
      'Citations': '4.17E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'NoisyNet-Dueling',
      'Domain': 'Games',
      'Task': 'Atari Games',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'M Fortunato, MG Azar, B Piot, J Menick',
      'Publication date': '30/06/2017',
      'Year': '2017',
      'Reference': 'Noisy Networks for Exploration',
      'Link': 'https://arxiv.org/abs/1706.10295v3',
      'Citations': '4.80E+02',
      'Inclusion criteria': 'SOTA improvement',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'ShuffleNet v1',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'Megvii Inc',
      'Organization Categorization': 'Industry',
      'Author(s)': 'X Zhang, X Zhou, M Lin, J Sun',
      'Publication date': '03/07/2017',
      'Year': '2017',
      'Reference': 'ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices',
      'Link': 'https://arxiv.org/abs/1707.01083',
      'Citations': '2.78E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '2.43E+06',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '1.40E+08',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'NASNet-A',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Google Brain',
      'Organization Categorization': 'Industry',
      'Author(s)': 'B Zoph, V Vasudevan, J Shlens',
      'Publication date': '21/07/2017',
      'Year': '2017',
      'Reference': 'Learning Transferable Architectures for Scalable Image Recognition',
      'Link': 'https://arxiv.org/abs/1707.07012',
      'Citations': '3.10E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '8.90E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Image segmentation',
      'Organization(s)': 'Chinese University of Hong Kong',
      'Organization Categorization': 'Industry - Academia Collaboration (Academia leaning)',
      'Author(s)': 'Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia',
      'Publication date': '21/07/2017',
      'Year': '2017',
      'Reference': 'Pyramid Scene Parsing Network',
      'Link': 'https://ieeexplore.ieee.org/document/8100143',
      'Citations': '6.07E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'JFT',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'Google Research, CMU',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'ChenSun,AbhinavShrivastava,SaurabhSingh,andAbhinavGupta',
      'Publication date': '04/08/2017',
      'Year': '2017',
      'Reference': 'Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.',
      'Link': 'https://arxiv.org/abs/1707.02968',
      'Citations': '1.14E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '4.8E+20',
      'Training dataset': 'JFT-300M',
      'Training dataset size (datapoints)': '3.00E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'RetinaNet-R50',
      'Domain': 'Vision',
      'Task': 'Object detection',
      'Organization(s)': 'Facebook AI research',
      'Organization Categorization': 'Industry',
      'Author(s)': 'TY Lin, P Goyal, R Girshick, K He',
      'Publication date': '07/08/2017',
      'Year': '2017',
      'Reference': 'Focal loss for dense object detection',
      'Link': 'https://arxiv.org/abs/1708.02002',
      'Citations': '8.42E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '3.40E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '9.70E+10',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'RetinaNet-R50',
      'Domain': 'Vision',
      'Task': 'Object detection',
      'Organization(s)': 'Facebook AI research',
      'Organization Categorization': 'Industry',
      'Author(s)': 'TY Lin, P Goyal, R Girshick, K He',
      'Publication date': '07/08/2017',
      'Year': '2017',
      'Reference': 'Focal loss for dense object detection',
      'Link': 'https://arxiv.org/abs/1708.02002',
      'Citations': '8.42E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '5.30E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '1.27E+11',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'OpenAI TI7 DOTA 1v1',
      'Domain': 'Games',
      'Task': 'DOTA',
      'Organization(s)': 'OpenAI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'A Radford, K Narasimhan, T Salimans, I Sutskever',
      'Publication date': '11/08/2017',
      'Year': '2017',
      'Reference': 'Dota 2 ',
      'Link': 'https://openai.com/five/',
      'Citations': 'NA',
      'Inclusion criteria': '',
      'Parameters': '1.50E+08',
      'Training compute (FLOPs)': '6.0E+20',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'University of Guelph, Canadian Institute for Advanced Research and Vector Institute',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': ' Terrance DeVries, Graham W. Taylor',
      'Publication date': '15/08/2017',
      'Year': '2017',
      'Reference': 'Improved Regularization of Convolutional Neural Networks with Cutout',
      'Link': 'https://arxiv.org/abs/1708.04552',
      'Citations': '1.45E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '3.50E+07',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'NeuMF (Pinterest)',
      'Domain': 'Recommendation',
      'Task': 'Collaborative filtering',
      'Organization(s)': 'NUS, Columbia, Shandong University, Texas A&M',
      'Organization Categorization': 'Academia',
      'Author(s)': 'X He, L Liao, H Zhang, L Nie, X Hu',
      'Publication date': '16/08/2017',
      'Year': '2017',
      'Reference': 'Neural Collaborative Filtering',
      'Link': 'https://arxiv.org/abs/1708.05031',
      'Citations': '2.43E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'SENet (ImageNet)',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Chinese Academy of Sciences ; University of Oxford',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu',
      'Publication date': '05/09/2017',
      'Year': '2017',
      'Reference': 'Squeeze-and-Excitation Networks',
      'Link': 'https://arxiv.org/abs/1709.01507',
      'Citations': '7.94E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '2.81E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': 'ImageNet',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '3.87E+09',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'AlphaGo Zero',
      'Domain': 'Games',
      'Task': 'Go',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'D Silver, J Schrittwieser, K Simonyan, I Antonoglou',
      'Publication date': '19/10/2017',
      'Year': '2017',
      'Reference': 'Mastering the game of Go without human knowledge',
      'Link': 'https://www.researchgate.net/publication/320473480_Mastering_the_game_of_Go_without_human_knowledge',
      'Citations': '5.81E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '4.64E+07',
      'Training compute (FLOPs)': '3.4E+23',
      'Training dataset': '',
      'Training dataset size (datapoints)': '5.80E+09',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Person re-identification',
      'Organization(s)': 'University of Technology Sydney',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Zhedong Zheng, Liang Zheng, Yi Yang',
      'Publication date': '22/10/2017',
      'Year': '2017',
      'Reference': 'Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro',
      'Link': 'https://arxiv.org/abs/1701.07717',
      'Citations': '1.40E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'CapsNet (MNIST)',
      'Domain': 'Vision',
      'Task': 'Character recognition',
      'Organization(s)': 'Google Brain',
      'Organization Categorization': 'Industry',
      'Author(s)': 'S Sabour, N Frosst, GE Hinton',
      'Publication date': '26/10/2017',
      'Year': '2017',
      'Reference': 'Dynamic Routing Between Capsules',
      'Link': 'https://arxiv.org/abs/1710.09829',
      'Citations': '2.74E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '8.20E+06',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'CapsNet (MultiMNIST)',
      'Domain': 'Vision',
      'Task': 'Character recognition',
      'Organization(s)': 'Google Brain',
      'Organization Categorization': 'Industry',
      'Author(s)': 'S Sabour, N Frosst, GE Hinton',
      'Publication date': '26/10/2017',
      'Year': '2017',
      'Reference': 'Dynamic Routing Between Capsules',
      'Link': 'https://arxiv.org/abs/1710.09829',
      'Citations': '2.74E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.14E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Image generation',
      'Organization(s)': 'Nvidia',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen',
      'Publication date': '27/10/2017',
      'Year': '2017',
      'Reference': 'Progressive Growing of GANs for Improved Quality, Stability, and Variation',
      'Link': 'https://arxiv.org/abs/1710.10196',
      'Citations': '3.91E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Person re-identification',
      'Organization(s)': 'Visual Computing Institute, Aachen University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Alexander Hermans, Lucas Beyer, Bastian Leibe',
      'Publication date': '21/11/2017',
      'Year': '2017',
      'Reference': 'In Defense of the Triplet Loss for Person Re-Identification',
      'Link': 'https://arxiv.org/abs/1703.07737',
      'Citations': '1.99E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'PNAS-net',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Johns Hopkins University, Stanford, Google AI',
      'Organization Categorization': 'Industry - Academia Collaboration (Industry leaning)',
      'Author(s)': 'C Liu, B Zoph, M Neumann, J Shlens',
      'Publication date': '02/12/2017',
      'Year': '2017',
      'Reference': 'Progressive Neural Architecture Search',
      'Link': 'https://arxiv.org/abs/1712.00559',
      'Citations': '1.15E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '8.60E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'PNASNet-5',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Johns Hopkins University, Stanford, Google AI',
      'Organization Categorization': 'Industry - Academia Collaboration (Industry leaning)',
      'Author(s)': 'C Liu, B Zoph, M Neumann, J Shlens',
      'Publication date': '2/12/2017',
      'Year': '2017',
      'Reference': 'Progressive Neural Architecture Search',
      'Link': 'https://arxiv.org/abs/1712.00559',
      'Citations': '1.34E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '6.6E+19',
      'Training dataset': 'Imagenet-1k',
      'Training dataset size (datapoints)': '1.28E+06',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'AlphaZero',
      'Domain': 'Games',
      'Task': '',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'D Silver, T Hubert, J Schrittwieser, I Antonoglou',
      'Publication date': '05/12/2017',
      'Year': '2017',
      'Reference': 'Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm',
      'Link': 'https://arxiv.org/abs/1712.01815',
      'Citations': '1.08E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '3.7E+22',
      'Training dataset': '',
      'Training dataset size (datapoints)': '7.00E+05',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': 'Score',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Refined Part Pooling',
      'Domain': 'Vision',
      'Task': 'Person retrieval',
      'Organization(s)': 'Tsinghua University, University of Technology Sydney, University of Texas at San Antonio',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, Shengjin Wang',
      'Publication date': '09/01/2018',
      'Year': '2018',
      'Reference': 'Beyond Part Models: Person Retrieval with Refined Part Pooling (and a Strong Convolutional Baseline)',
      'Link': 'https://arxiv.org/abs/1711.09349',
      'Citations': '1.24E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'ULM-FiT',
      'Domain': 'Language',
      'Task': 'Text classification',
      'Organization(s)': 'University of San Francisco, Insight Centre NUI Galway',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'J Howard, S Ruder',
      'Publication date': '18/01/2018',
      'Year': '2018',
      'Reference': 'Universal Language Model Fine-tuning for Text Classification',
      'Link': 'https://arxiv.org/abs/1801.06146',
      'Citations': '1.94E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'ELMo',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'AI2',
      'Organization Categorization': 'Industry',
      'Author(s)': 'ME Peters, M Neumann, M Iyyer, M Gardner',
      'Publication date': '01/02/2018',
      'Year': '2018',
      'Reference': 'Deep contextualized word representations',
      'Link': 'https://arxiv.org/abs/1802.05365',
      'Citations': '7.48E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '9.40E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '2.60E+10',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'IMPALA',
      'Domain': 'Games',
      'Task': 'Atari',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu',
      'Publication date': '05/02/2018',
      'Year': '2018',
      'Reference': 'IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures',
      'Link': 'https://arxiv.org/abs/1802.01561',
      'Citations': '6.75E+02',
      'Inclusion criteria': '',
      'Parameters': '1.60E+06',
      'Training compute (FLOPs)': '1.7E+20',
      'Training dataset': '',
      'Training dataset size (datapoints)': '2.40E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'AmoebaNet-A',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Google Brain',
      'Organization Categorization': 'Industry',
      'Author(s)': 'E Real, A Aggarwal, Y Huang, QV Le',
      'Publication date': '05/02/2018',
      'Year': '2018',
      'Reference': 'Regularized Evolution for Image Classifier Architecture Search',
      'Link': 'https://arxiv.org/abs/1802.01548',
      'Citations': '1.43E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '8.70E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'AmoebaNet-A',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Google Brain',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Esteban Real, Alok Aggarwal, Yanping Huang, Quoc V Le',
      'Publication date': '05/02/2018',
      'Year': '2018',
      'Reference': 'Regularized Evolution for Image Classifier Architecture Search',
      'Link': 'https://arxiv.org/abs/1802.01548',
      'Citations': '1.71E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '3.9E+20',
      'Training dataset': 'Imagenet-1k',
      'Training dataset size (datapoints)': '1.28E+06',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'DeepLabV3+',
      'Domain': 'Vision',
      'Task': 'Semantic segmentation',
      'Organization(s)': 'Google Inc.',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam',
      'Publication date': '07/02/2018',
      'Year': '2018',
      'Reference': 'Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation',
      'Link': 'https://arxiv.org/abs/1802.02611v3',
      'Citations': '5.37E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Image generation',
      'Organization(s)': 'Preferred Networks Inc, Ritsumeikan University, National Institute of Informatics',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida',
      'Publication date': '16/02/2018',
      'Year': '2018',
      'Reference': 'Spectral Normalization for Generative Adversarial Networks',
      'Link': 'https://arxiv.org/abs/1802.05957',
      'Citations': '2.74E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Image super-resolution',
      'Organization(s)': 'Northeastern University, University of Rochester',
      'Organization Categorization': 'Academia',
      'Author(s)': ' Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu',
      'Publication date': '24/02/2018',
      'Year': '2018',
      'Reference': 'Residual Dense Network for Image Super-Resolution',
      'Link': 'https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Residual_Dense_Network_CVPR_2018_paper.html',
      'Citations': '1.78E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Chinese - English translation',
      'Domain': 'Language',
      'Task': 'Translation',
      'Organization(s)': 'Microsoft',
      'Organization Categorization': 'Industry',
      'Author(s)': 'H Hassan, A Aue, C Chen, V Chowdhary',
      'Publication date': '01/03/2018',
      'Year': '2018',
      'Reference': 'Achieving Human Parity on Automatic Chinese to English News Translation',
      'Link': 'https://www.microsoft.com/en-us/research/publication/achieving-human-parity-on-automatic-chinese-to-english-news-translation/',
      'Citations': '3.64E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Rotation',
      'Domain': 'Drawing',
      'Task': 'Image completion',
      'Organization(s)': 'École des Ponts ParisTech',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Spyros Gidaris, Praveer Singh, Nikos Komodakis',
      'Publication date': '21/03/2018',
      'Year': '2018',
      'Reference': 'Unsupervised Representation Learning by Predicting Image Rotations',
      'Link': 'https://arxiv.org/abs/1803.07728',
      'Citations': '1.16E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '8.60E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'YOLOv3',
      'Domain': 'Vision',
      'Task': 'Object detection',
      'Organization(s)': 'University of Washington',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Joseph Redmon, Ali Farhadi',
      'Publication date': '08/04/2018',
      'Year': '2018',
      'Reference': 'YOLOv3: An Incremental Improvement',
      'Link': 'https://arxiv.org/abs/1804.02767',
      'Citations': '7.71E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.06E+08',
      'Training compute (FLOPs)': '5.1E+19',
      'Training dataset': 'ImageNet',
      'Training dataset size (datapoints)': '1.28E+06',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '7.10E+10',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'ResNeXt-101 32x48d',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Facebook',
      'Organization Categorization': 'Industry',
      'Author(s)': 'D Mahajan, R Girshick',
      'Publication date': '02/05/2018',
      'Year': '2018',
      'Reference': 'Exploring the Limits of Weakly Supervised Pretraining',
      'Link': 'https://arxiv.org/abs/1805.00932',
      'Citations': '6.19E+02',
      'Inclusion criteria': '',
      'Parameters': '8.29E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '3.12E+10',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'GPT',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'OpenAI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'A Radford, K Narasimhan, T Salimans, I Sutskever',
      'Publication date': '01/06/2018',
      'Year': '2018',
      'Reference': 'Improving Language Understanding by Generative Pre-Training',
      'Link': 'https://openai.com/blog/language-unsupervised/',
      'Citations': '2.26E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.17E+08',
      'Training compute (FLOPs)': '1.8E+19',
      'Training dataset': 'BooksCorpus',
      'Training dataset size (datapoints)': '1.00E+09',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '3.00E+10',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'MobileNetV2',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'Google Inc.',
      'Organization Categorization': 'Industry',
      'Author(s)': 'M Sandler, A Howard, M Zhu',
      'Publication date': '18/06/2018',
      'Year': '2018',
      'Reference': 'MobileNetV2: Inverted Residuals and Linear Bottlenecks',
      'Link': 'https://ieeexplore.ieee.org/document/8578572',
      'Citations': '5.71E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '3.40E+06',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '6.00E+08',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'ShuffleNet v2',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'Tsinghua University, Megvii Inc',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'N Ma, X Zhang, HT Zheng',
      'Publication date': '30/06/2018',
      'Year': '2018',
      'Reference': 'ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design',
      'Link': 'https://arxiv.org/abs/1807.11164',
      'Citations': '1.41E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '2.28E+06',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '3.00E+08',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Population-based DRL',
      'Domain': 'Games',
      'Task': 'Capture the flag',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, Thore Graepel',
      'Publication date': '03/07/2018',
      'Year': '2018',
      'Reference': 'Human-level performance in first-person multiplayer games with population-based deep reinforcement learning',
      'Link': 'https://arxiv.org/abs/1807.01281',
      'Citations': '4.34E+02',
      'Inclusion criteria': '',
      'Parameters': '1.22E+08',
      'Training compute (FLOPs)': '3.5E+19',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '6.00E+10',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Image super-resolution',
      'Organization(s)': 'Northeastern University',
      'Organization Categorization': 'Academia',
      'Author(s)': ' Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, Yun Fu',
      'Publication date': '08/07/2018',
      'Year': '2018',
      'Reference': 'Image Super-Resolution Using Very Deep Residual Channel Attention Networks',
      'Link': 'https://openaccess.thecvf.com/content_ECCV_2018/html/Yulun_Zhang_Image_Super-Resolution_Using_ECCV_2018_paper.html',
      'Citations': '1.80E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'ESRGAN',
      'Domain': 'Vision',
      'Task': 'Image super-resolution',
      'Organization(s)': 'Chinese University of Hong Kong, Chinese Academy of Sciences, Nanyang Technological University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen Change Loy, Yu Qiao, Xiaoou Tang',
      'Publication date': '01/09/2018',
      'Year': '2018',
      'Reference': 'ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks',
      'Link': 'https://arxiv.org/abs/1809.00219',
      'Citations': '1.50E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'BigGAN-deep 512x512',
      'Domain': 'Drawing',
      'Task': 'Image generation',
      'Organization(s)': 'Heriot-Watt University, DeepMind',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'A Brock, J Donahue, K Simonyan',
      'Publication date': '28/09/2018',
      'Year': '2018',
      'Reference': 'Large Scale GAN Training for High Fidelity Natural Image Synthesis',
      'Link': 'https://arxiv.org/abs/1809.11096',
      'Citations': '1.98E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.13E+08',
      'Training compute (FLOPs)': '3.0E+21',
      'Training dataset': 'JFT-300M',
      'Training dataset size (datapoints)': '2.92E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'BERT-Large',
      'Domain': 'Language',
      'Task': 'Next sentence prediction',
      'Organization(s)': 'Google AI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'J Devlin, MW Chang, K Lee, K Toutanova',
      'Publication date': '11/10/2018',
      'Year': '2018',
      'Reference': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',
      'Link': 'https://arxiv.org/abs/1810.04805',
      'Citations': '2.38E+04',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '3.40E+08',
      'Training compute (FLOPs)': '2.9E+20',
      'Training dataset': '',
      'Training dataset size (datapoints)': '3.30E+09',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '7.90E+10',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'MetaMimic',
      'Domain': 'Games',
      'Task': '',
      'Organization(s)': 'Google',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Tom Le Paine, Sergio Gomez',
      'Publication date': '11/10/2018',
      'Year': '2018',
      'Reference': 'One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL',
      'Link': 'https://arxiv.org/abs/1810.05017',
      'Citations': '1.30E+01',
      'Inclusion criteria': '',
      'Parameters': '2.20E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'GPipe (Amoeba)',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Google',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Y Huang, Y Cheng, A Bapna, O Firat',
      'Publication date': '16/11/2018',
      'Year': '2018',
      'Reference': 'GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism',
      'Link': 'https://arxiv.org/abs/1811.06965',
      'Citations': '4.86E+02',
      'Inclusion criteria': '',
      'Parameters': '5.57E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': 'ImageNet',
      'Training dataset size (datapoints)': '1.28E+06',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'GPipe (Transformer)',
      'Domain': 'Language',
      'Task': 'Translation',
      'Organization(s)': 'Google',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Y Huang, Y Cheng, A Bapna, O Firat',
      'Publication date': '16/11/2018',
      'Year': '2018',
      'Reference': 'GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism',
      'Link': 'https://arxiv.org/abs/1811.06965',
      'Citations': '4.86E+02',
      'Inclusion criteria': '',
      'Parameters': '6.00E+09',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Semantic segmentation',
      'Organization(s)': 'Chinese Academy of Sciences',
      'Organization Categorization': 'Industry - Academia Collaboration (Academia leaning)',
      'Author(s)': 'Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, Hanqing Lu',
      'Publication date': '21/04/2019',
      'Year': '2019',
      'Reference': 'Dual Attention Network for Scene Segmentation',
      'Link': 'https://openaccess.thecvf.com/content_CVPR_2019/html/Fu_Dual_Attention_Network_for_Scene_Segmentation_CVPR_2019_paper.html',
      'Citations': '1.99E+03',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '2.0E+10',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Transformer ELMo',
      'Domain': 'Language',
      'Task': 'Univeristy of Washington, Allen Instititute for Artificial Intelligence',
      'Organization(s)': 'AI2',
      'Organization Categorization': 'Industry - Academia Collaboration (Industry leaning)',
      'Author(s)': 'ME Peters, M Neumann, L Zettlemoyer',
      'Publication date': '01/01/2019',
      'Year': '2019',
      'Reference': 'Dissecting Contextual Word Embeddings: Architecture and Representation',
      'Link': 'https://www.semanticscholar.org/paper/Dissecting-Contextual-Word-Embeddings%3A-Architecture-Peters-Neumann/ac11062f1f368d97f4c826c317bf50dcc13fdb59',
      'Citations': '1.96E+02',
      'Inclusion criteria': '',
      'Parameters': '4.65E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Decoupled weight decay regularization',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'University of Freiburg',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Ilya Loshchilov and Frank Hutter',
      'Publication date': '04/01/2019',
      'Year': '2019',
      'Reference': 'Decoupled weight decay regularization.',
      'Link': 'https://arxiv.org/abs/1711.05101',
      'Citations': '2.06E+03',
      'Inclusion criteria': '',
      'Parameters': '3.65E+07',
      'Training compute (FLOPs)': '2.5E+18',
      'Training dataset': 'CIFAR-10',
      'Training dataset size (datapoints)': '5.00E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '1.73E+09',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'MT-DNN',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Microsoft',
      'Organization Categorization': 'Industry',
      'Author(s)': 'X Liu, P He, W Chen, J Gao',
      'Publication date': '31/01/2019',
      'Year': '2019',
      'Reference': 'Multi-Task Deep Neural Networks for Natural Language Understanding',
      'Link': 'https://arxiv.org/abs/1901.11504',
      'Citations': '5.53E+02',
      'Inclusion criteria': '',
      'Parameters': '3.30E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': 'Encoder-decoder',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Hanabi 4 player',
      'Domain': 'Games',
      'Task': 'Hanabi',
      'Organization(s)': 'DeepMind, University of Oxford, Google Brain, Carnegie Mellon University, ',
      'Organization Categorization': 'Industry - Academia Collaboration (Industry leaning)',
      'Author(s)': '',
      'Publication date': '01/02/2019',
      'Year': '2019',
      'Reference': 'The Hanabi Challenge: A New Frontier for AI Research',
      'Link': 'https://arxiv.org/abs/1902.00506',
      'Citations': '1.15E+02',
      'Inclusion criteria': '',
      'Parameters': '7.64E+05',
      'Training compute (FLOPs)': '9.2E+16',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'GPT-2',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'OpenAI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'A Radford, J Wu, R Child, D Luan, D Amodei',
      'Publication date': '14/02/2019',
      'Year': '2019',
      'Reference': 'Language Models are Unsupervised Multitask Learners',
      'Link': 'https://openai.com/blog/better-language-models/',
      'Citations': '1.70E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.50E+09',
      'Training compute (FLOPs)': '1.5E+21',
      'Training dataset': '',
      'Training dataset size (datapoints)': '3.00E+09',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '3.40E+12',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '40',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '50000',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'ProxylessNAS',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'MIT',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Han Cai, Ligeng Zhu, and Song Han',
      'Publication date': '23/02/2019',
      'Year': '2019',
      'Reference': 'ProxylessNAS: Direct neural architecture search on target task and hardware',
      'Link': 'https://arxiv.org/abs/1812.00332',
      'Citations': '9.96E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '3.7E+19',
      'Training dataset': 'ImageNet',
      'Training dataset size (datapoints)': '1.28E+06',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '2.63E+11',
      'Equivalent training time (hours)': '200',
      'Inference time (ms)': '5.1',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Cross-lingual alignment',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Tel Aviv University, MIT',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Tal Schuster, Ori Ram, Regina Barzilay, and Amir Globerson.',
      'Publication date': '04/04/2019',
      'Year': '2019',
      'Reference': 'Cross-lingual alignment of contextual word embeddings, with applications to zero- shot dependency parsing.',
      'Link': 'https://arxiv.org/abs/1902.09492',
      'Citations': '1.29E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '2.6E+18',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '3.66E+12',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': 'Speech recognition',
      'Organization(s)': 'Google Brain',
      'Organization Categorization': 'Industry',
      'Author(s)': ' Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, Quoc V. Le',
      'Publication date': '18/04/2019',
      'Year': '2019',
      'Reference': 'SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition',
      'Link': 'https://arxiv.org/abs/1904.08779',
      'Citations': '1.41E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': 'Encoder-decoder',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'ResNet-50 Billion-scale',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'OpenAI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'A Radford, J Wu, R Child, D Luan, D Amodei',
      'Publication date': '02/05/2019',
      'Year': '2019',
      'Reference': 'Language Models are Unsupervised Multitask Learners',
      'Link': 'https://paperswithcode.com/paper/language-models-are-unsupervised-multitask',
      'Citations': '1.70E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '2.60E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'ResNeXt-101 Billion-scale',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Facebook AI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'IZ Yalniz, H Jégou, K Chen, M Paluri',
      'Publication date': '02/05/2019',
      'Year': '2019',
      'Reference': 'Billion-scale semi-supervised learning for image classification',
      'Link': 'https://arxiv.org/abs/1905.00546',
      'Citations': '1.56E+02',
      'Inclusion criteria': '',
      'Parameters': '1.93E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'CPC v2',
      'Domain': 'Drawing',
      'Task': 'Image completion',
      'Organization(s)': 'Deepmind, Berkeley',
      'Organization Categorization': 'Industry - Academia Collaboration (Industry leaning)',
      'Author(s)': '',
      'Publication date': '22/05/2019',
      'Year': '2019',
      'Reference': 'Data-Efficient Image Recognition with Contrastive Predictive Coding',
      'Link': 'https://arxiv.org/abs/1905.09272',
      'Citations': '4.91E+02',
      'Inclusion criteria': '',
      'Parameters': '3.03E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'EfficientNet-L2',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Google',
      'Organization Categorization': 'Industry',
      'Author(s)': 'M Tan, Q Le',
      'Publication date': '28/05/2019',
      'Year': '2019',
      'Reference': 'EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks',
      'Link': 'https://arxiv.org/abs/1905.11946',
      'Citations': '3.19E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '4.80E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '3.90E+08',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': 'Score',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'MnasNet-A1 + SSDLite',
      'Domain': 'Vision',
      'Task': 'Performing image classification and object detection on mobile devices',
      'Organization(s)': 'Google ',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V. Le',
      'Publication date': '29/05/2019',
      'Year': '2019',
      'Reference': 'MnasNet: Platform-Aware Neural Architecture Search for Mobile',
      'Link': 'https://arxiv.org/abs/1807.11626',
      'Citations': '1.43E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '4.90E+06',
      'Training compute (FLOPs)': '1.5E+21',
      'Training dataset': 'MS COCO',
      'Training dataset size (datapoints)': '1.18E+05',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'MnasNet-A3',
      'Domain': 'Vision',
      'Task': 'Performing image classification and object detection on mobile devices',
      'Organization(s)': 'Google ',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V. Le',
      'Publication date': '29/05/2019',
      'Year': '2019',
      'Reference': 'MnasNet: Platform-Aware Neural Architecture Search for Mobile',
      'Link': 'https://arxiv.org/abs/1807.11626',
      'Citations': '1.43E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '5.20E+06',
      'Training compute (FLOPs)': '1.5E+21',
      'Training dataset': 'ImageNet',
      'Training dataset size (datapoints)': '1.28E+06',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Grover-Mega',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'University of Washington',
      'Organization Categorization': 'Industry - Academia Collaboration (Academia leaning)',
      'Author(s)': 'R Zellers, A Holtzman, H Rashkin, Y Bisk',
      'Publication date': '29/05/2019',
      'Year': '2019',
      'Reference': 'Defending Against Neural Fake News',
      'Link': 'https://arxiv.org/abs/1905.12616',
      'Citations': '2.72E+02',
      'Inclusion criteria': '',
      'Parameters': '1.50E+09',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'DLRM-2020',
      'Domain': 'Recommendation',
      'Task': '',
      'Organization(s)': 'Facebook AI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'M Naumov, D Mudigere, HJM Shi, J Huang',
      'Publication date': '31/05/2019',
      'Year': '2019',
      'Reference': 'Deep Learning Recommendation Model for Personalization and Recommendation Systems',
      'Link': 'https://arxiv.org/abs/1906.00091',
      'Citations': '1.40E+02',
      'Inclusion criteria': '',
      'Parameters': '1.00E+11',
      'Training compute (FLOPs)': '4.0E+18',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'FTW',
      'Domain': 'Games',
      'Task': 'Capture the flag',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'M Jaderberg, WM Czarnecki, I Dunning, L Marris',
      'Publication date': '31/05/2019',
      'Year': '2019',
      'Reference': 'Human-level performance in 3D multiplayer games with population-based reinforcement learning',
      'Link': 'https://deepmind.com/research/publications/capture-the-flag',
      'Citations': '4.25E+02',
      'Inclusion criteria': '',
      'Parameters': '1.26E+08',
      'Training compute (FLOPs)': '7.3E+21',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '1.21E+12',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'XLM',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Facebook',
      'Organization Categorization': 'Industry',
      'Author(s)': 'G Lample, A Conneau',
      'Publication date': '01/06/2019',
      'Year': '2019',
      'Reference': 'Cross-lingual Language Model Pretraining',
      'Link': 'https://arxiv.org/abs/1901.07291',
      'Citations': '6.78E+02',
      'Inclusion criteria': '',
      'Parameters': '6.65E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'XLNet',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Carnegie Mellon University, Google AI Brain ',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Z Yang, Z Dai, Y Yang, J Carbonell',
      'Publication date': '01/06/2019',
      'Year': '2019',
      'Reference': 'XLNet: Generalized Autoregressive Pretraining for Language Understanding',
      'Link': 'https://arxiv.org/abs/1906.08237',
      'Citations': '3.06E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '3.40E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'AMDIM',
      'Domain': 'Drawing',
      'Task': 'Image completion',
      'Organization(s)': 'Microsoft Research',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Philip Bachman, R Devon Hjelm, William Buchwalter',
      'Publication date': '03/06/2019',
      'Year': '2019',
      'Reference': 'Learning Representations by Maximizing Mutual Information Across Views',
      'Link': 'https://arxiv.org/abs/1906.00910',
      'Citations': '4.86E+02',
      'Inclusion criteria': '',
      'Parameters': '6.26E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'FixRes ResNeXt-101 WSL',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Facebook AI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'H Touvron, A Vedaldi, M Douze, H Jégou',
      'Publication date': '14/06/2019',
      'Year': '2019',
      'Reference': 'Fixing the train-test resolution discrepancy',
      'Link': 'https://arxiv.org/abs/1906.06423',
      'Citations': '1.65E+02',
      'Inclusion criteria': '',
      'Parameters': '8.29E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '9.40E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '1.20E+07',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'RoBERTa',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Facebook',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen',
      'Publication date': '01/07/2019',
      'Year': '2019',
      'Reference': 'RoBERTa: A Robustly Optimized BERT Pretraining Approach',
      'Link': 'https://arxiv.org/abs/1907.11692',
      'Citations': '1.51E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '3.55E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'BigBiGAN',
      'Domain': 'Drawing',
      'Task': 'Image completion',
      'Organization(s)': 'Google',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Spyros Gidaris, Praveer Singh, Nikos Komodakis',
      'Publication date': '04/07/2019',
      'Year': '2019',
      'Reference': 'Large Scale Adversarial Representation Learning',
      'Link': 'https://arxiv.org/abs/1907.02544',
      'Citations': '2.52E+02',
      'Inclusion criteria': '',
      'Parameters': '8.60E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'ObjectNet',
      'Domain': 'Vision',
      'Task': 'Object recognition',
      'Organization(s)': 'MIT',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfre- und, Josh Tenenbaum, and Boris Katz',
      'Publication date': '06/09/2019',
      'Year': '2019',
      'Reference': 'Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models',
      'Link': 'https://papers.nips.cc/paper/2019/file/97af07a14cacba681feacf3012730892-Paper.pdf',
      'Citations': '2.39E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '3.80E+07',
      'Training compute (FLOPs)': '1.9E+19',
      'Training dataset': 'Internal data',
      'Training dataset size (datapoints)': '5.00E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '108',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Hide and Seek',
      'Domain': 'Games',
      'Task': 'Hide and Seek',
      'Organization(s)': 'OpenAI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'B Baker, I Kanitscheider, T Markov, Y Wu',
      'Publication date': '17/09/2019',
      'Year': '2019',
      'Reference': 'Emergent Tool Use From Multi-Agent Autocurricula',
      'Link': 'https://openai.com/blog/emergent-tool-use/',
      'Citations': '2.24E+02',
      'Inclusion criteria': '',
      'Parameters': '1.60E+06',
      'Training compute (FLOPs)': '3.0E+17',
      'Training dataset': '',
      'Training dataset size (datapoints)': '3.17E+10',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Megatron-LM',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'NVIDIA',
      'Organization Categorization': 'Industry',
      'Author(s)': 'M Shoeybi, M Patwary, R Puri, P LeGresley',
      'Publication date': '17/09/2019',
      'Year': '2019',
      'Reference': 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism',
      'Link': 'https://arxiv.org/abs/1909.08053',
      'Citations': '2.46E+02',
      'Inclusion criteria': '',
      'Parameters': '8.30E+09',
      'Training compute (FLOPs)': '9.1E+21',
      'Training dataset': '',
      'Training dataset size (datapoints)': '3.48E+10',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '1.80E+13',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Megatron-BERT',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'NVIDIA',
      'Organization Categorization': 'Industry',
      'Author(s)': 'M Shoeybi, M Patwary, R Puri, P LeGresley',
      'Publication date': '17/09/2019',
      'Year': '2019',
      'Reference': 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism',
      'Link': 'https://arxiv.org/abs/1909.08053',
      'Citations': '2.46E+02',
      'Inclusion criteria': '',
      'Parameters': '3.90E+09',
      'Training compute (FLOPs)': '5.7E+22',
      'Training dataset': '',
      'Training dataset size (datapoints)': '3.48E+10',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'ALBERT',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Google, TTIC',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Z Lan, M Chen, S Goodman, K Gimpel',
      'Publication date': '26/09/2019',
      'Year': '2019',
      'Reference': 'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations',
      'Link': 'https://arxiv.org/abs/1909.11942',
      'Citations': '1.66E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '0.00E+00',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '3.30E+09',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '2.25E+10',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'AlphaX-1',
      'Domain': 'Vision',
      'Task': 'Neural architecture search for computer vision',
      'Organization(s)': 'Brown and Facebook AI Research',
      'Organization Categorization': 'Industry - Academia Collaboration (Academia leaning)',
      'Author(s)': 'Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, Rodrigo Fonseca1',
      'Publication date': '02/10/2019',
      'Year': '2019',
      'Reference': 'AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search',
      'Link': 'https://arxiv.org/abs/1903.11059',
      'Citations': '5.00E+01',
      'Inclusion criteria': '',
      'Parameters': '5.79E+08',
      'Training compute (FLOPs)': '7.6E+18',
      'Training dataset': 'ImageNet',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'DistilBERT',
      'Domain': 'Language',
      'Task': 'Text autocompletion',
      'Organization(s)': 'HuggingFace',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf',
      'Publication date': '02/10/2019',
      'Year': '2019',
      'Reference': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter',
      'Link': 'https://arxiv.org/abs/1910.01108',
      'Citations': '8.95E+02',
      'Inclusion criteria': '',
      'Parameters': '6.60E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': 'Perplexity of next token',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Rubik\'s cube',
      'Domain': 'Robotics',
      'Task': '',
      'Organization(s)': 'Open AI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, Lei Zhang',
      'Publication date': '15/10/2019',
      'Year': '2019',
      'Reference': 'Solving Rubik’s Cube with a Robot Hand',
      'Link': 'https://arxiv.org/abs/1910.07113',
      'Citations': '2.27E+02',
      'Inclusion criteria': '',
      'Parameters': '2.78E+07',
      'Training compute (FLOPs)': '8.5E+20',
      'Training dataset': '',
      'Training dataset size (datapoints)': '6.24E+07',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'T5-3B',
      'Domain': 'Language',
      'Task': 'Text autocompletion',
      'Organization(s)': 'Google',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu',
      'Publication date': '23/10/2019',
      'Year': '2019',
      'Reference': 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer',
      'Link': 'https://arxiv.org/abs/1910.10683',
      'Citations': '1.54E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '3.00E+09',
      'Training compute (FLOPs)': '1.0E+22',
      'Training dataset': 'Colossal Clean Crawled Corpus (C4)',
      'Training dataset size (datapoints)': '1.50E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'T5-11B',
      'Domain': 'Language',
      'Task': 'Text autocompletion',
      'Organization(s)': 'Google',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu',
      'Publication date': '23/10/2019',
      'Year': '2019',
      'Reference': 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer',
      'Link': 'https://arxiv.org/abs/1910.10683',
      'Citations': '1.54E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.10E+10',
      'Training compute (FLOPs)': '4.1E+22',
      'Training dataset': 'Colossal Clean Crawled Corpus (C4)',
      'Training dataset size (datapoints)': '1.50E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'BART-large',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Facebook AI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer',
      'Publication date': '29/10/2019',
      'Year': '2019',
      'Reference': 'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension',
      'Link': 'https://arxiv.org/abs/1910.13461',
      'Citations': '1.01E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '4.06E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'AlphaStar',
      'Domain': 'Games',
      'Task': 'StarCraft',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Oriol Vinyals,Igor Babuschkin,Wojciech M. Czarnecki,Michaël Mathieu,Andrew Dudzik,Junyoung Chung,David H. Choi,Richard Powell,Timo Ewalds,Petko Georgiev,Junhyuk Oh,Dan Horgan,Manuel Kroiss,Ivo Danihelka,Aja Huang,Laurent Sifre,Trevor Cai,John P. Agapiou,Max Jaderberg,Alexander S. Vezhnevets,Rémi Leblond,Tobias Pohlen,Valentin Dalibard,David Budden,Yury Sulsky,James Molloy,Tom L. Paine,Caglar Gulcehre,Ziyu Wang,Tobias Pfaff,Yuhuai Wu,Roman Ring,Dani Yogatama,Dario Wünsch,Katrina McKinney,Oliver Smith,Tom Schaul,Timothy Lillicrap,Koray Kavukcuoglu,Demis Hassabis,Chris Apps,David Silver',
      'Publication date': '30/10/2019',
      'Year': '2019',
      'Reference': 'Grandmaster level in StarCraft II using multi-agent reinforcement learning',
      'Link': 'https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning',
      'Citations': '1.04E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.39E+08',
      'Training compute (FLOPs)': '2.0E+23',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Noisy Student (L2)',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Carnegie Mellon University, Google',
      'Organization Categorization': 'Industry - Academia Collaboration (Industry leaning)',
      'Author(s)': 'Q Xie, MT Luong, E Hovy',
      'Publication date': '11/11/2019',
      'Year': '2019',
      'Reference': 'Self-training with Noisy Student improves ImageNet classification',
      'Link': 'https://paperswithcode.com/paper/self-training-with-noisy-student-improves/review/',
      'Citations': '5.76E+02',
      'Inclusion criteria': '',
      'Parameters': '4.80E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '1.04E+12',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'MoCo',
      'Domain': 'Drawing',
      'Task': 'Image completion',
      'Organization(s)': 'Facebook AI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xe, Ross Girshick',
      'Publication date': '13/11/2019',
      'Year': '2019',
      'Reference': 'Momentum Contrast for Unsupervised Visual Representation Learning',
      'Link': 'https://arxiv.org/abs/1911.05722',
      'Citations': '1.72E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '3.75E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'MuZero',
      'Domain': 'Games',
      'Task': 'Atari Games',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'J Schrittwieser, I Antonoglou, T Hubert, K Simonyan',
      'Publication date': '19/11/2019',
      'Year': '2019',
      'Reference': 'Mastering Atari Go Chess and Shogi by Planning with a Learned Model',
      'Link': 'https://arxiv.org/abs/1911.08265v2',
      'Citations': '4.12E+02',
      'Inclusion criteria': 'SOTA improvement',
      'Parameters': '3.69E+07',
      'Training compute (FLOPs)': '4.8E+19',
      'Training dataset': '',
      'Training dataset size (datapoints)': '2.00E+10',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'StarGAN v2',
      'Domain': 'Drawing',
      'Task': '',
      'Organization(s)': 'NAVER AI Lab, Yonsei University, Swiss Federal Institute of Technology ',
      'Organization Categorization': 'Industry - Academia Collaboration (Industry leaning)',
      'Author(s)': 'Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha',
      'Publication date': '4/12/2019',
      'Year': '2019',
      'Reference': 'StarGAN v2: Diverse Image Synthesis for Multiple Domains',
      'Link': 'https://arxiv.org/abs/1912.01865',
      'Citations': '2.33E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'OpenAI Five Rerun',
      'Domain': 'Games',
      'Task': 'Dota 2',
      'Organization(s)': 'OpenAI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung,Przemysław “Psyho" Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Józefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique Pondé de Oliveira Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, Susan Zhang',
      'Publication date': '13/12/2019',
      'Year': '2019',
      'Reference': 'Dota 2 with Large Scale Deep Reinforcement Learning',
      'Link': 'https://cdn.openai.com/dota-2.pdf',
      'Citations': '3.49E+02',
      'Inclusion criteria': 'SOTA improvement',
      'Parameters': '1.59E+08',
      'Training compute (FLOPs)': '1.3E+22',
      'Training dataset': '',
      'Training dataset size (datapoints)': '5.31E+10',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'OpenAI Five',
      'Domain': 'Games',
      'Task': 'Dota 2',
      'Organization(s)': 'OpenAI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'J Raiman, S Zhang, F Wolski',
      'Publication date': '13/12/2019',
      'Year': '2019',
      'Reference': 'Dota 2 with Large Scale Deep Reinforcement Learning',
      'Link': 'https://arxiv.org/abs/1912.06680',
      'Citations': '4.54E+02',
      'Inclusion criteria': 'SOTA improvement',
      'Parameters': '1.59E+08',
      'Training compute (FLOPs)': '6.7E+22',
      'Training dataset': '',
      'Training dataset size (datapoints)': '4.54E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Big Transfer (BiT-L)',
      'Domain': 'Vision',
      'Task': 'Image classification',
      'Organization(s)': 'Google Research',
      'Organization Categorization': 'Industry',
      'Author(s)': 'A Kolesnikov, L Beyer, X Zhai, J Puigcerver, J Yung',
      'Publication date': '24/12/2019',
      'Year': '2019',
      'Reference': 'Large scale learning of general visual representations for transfer',
      'Link': 'https://arxiv.org/abs/1912.11370',
      'Citations': '5.10E+01',
      'Inclusion criteria': '',
      'Parameters': '9.28E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'DLRM-2021',
      'Domain': 'Recommendation',
      'Task': '',
      'Organization(s)': 'Facebook AI ',
      'Organization Categorization': 'Industry',
      'Author(s)': 'D Mudigere, Y Hao, J Huang, A Tulloch',
      'Publication date': '01/07/2020',
      'Year': '2020',
      'Reference': 'High- performance, Distributed Training of Large scale Deep Learning Recommendation Models',
      'Link': 'https://www.arxiv-vanity.com/papers/2104.05158/',
      'Citations': '2.00E+00',
      'Inclusion criteria': '',
      'Parameters': '1.00E+12',
      'Training compute (FLOPs)': '3.0E+20',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '1.50E+06',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'AlphaFold',
      'Domain': 'Other',
      'Task': 'Protein folding prediction',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Andrew W. Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Žídek, Alexander W. R. Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David T. Jones, David Silver, Koray Kavukcuoglu & Demis Hassabis',
      'Publication date': '15/01/2020',
      'Year': '2020',
      'Reference': 'Improved protein structure prediction using potentials from deep learning',
      'Link': 'https://www.nature.com/articles/s41586-019-1923-7',
      'Citations': '8.40E+02',
      'Inclusion criteria': '',
      'Parameters': '6.90E+07',
      'Training compute (FLOPs)': '1.0E+20',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': 'Score',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'ERNIE-GEN (large)',
      'Domain': 'Language',
      'Task': 'Language Generation',
      'Organization(s)': 'Baidu',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang',
      'Publication date': '06/08/2020',
      'Year': '2020',
      'Reference': 'ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation',
      'Link': 'https://arxiv.org/abs/2001.11314',
      'Citations': '3.70E+01',
      'Inclusion criteria': '',
      'Parameters': '3.40E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '16GB',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Meena',
      'Domain': 'Language',
      'Task': 'Text autocompletion',
      'Organization(s)': 'Google AI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang',
      'Publication date': '28/01/2020',
      'Year': '2020',
      'Reference': 'Towards a Human-like Open-Domain Chatbot',
      'Link': 'https://arxiv.org/abs/2001.09977',
      'Citations': '2.57E+02',
      'Inclusion criteria': '',
      'Parameters': '2.60E+09',
      'Training compute (FLOPs)': '1.1E+23',
      'Training dataset': '',
      'Training dataset size (datapoints)': '4.00E+10',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Theseus 6/768',
      'Domain': 'Language',
      'Task': 'Text autocompletion',
      'Organization(s)': 'UC San Diego, Beihang University, Microsoft',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou',
      'Publication date': '07/02/2020',
      'Year': '2020',
      'Reference': 'BERT-of-Theseus: Compressing BERT by Progressive Module Replacing',
      'Link': 'https://arxiv.org/abs/2002.02925',
      'Citations': '6.70E+01',
      'Inclusion criteria': '',
      'Parameters': '6.60E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '1.13E+10',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'ALBERT-xxlarge',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Google research, Toyota Technological Institute at Chicago',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut',
      'Publication date': '09/02/2020',
      'Year': '2020',
      'Reference': 'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.',
      'Link': 'https://arxiv.org/abs/1909.11942',
      'Citations': '2.18E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '2.35E+08',
      'Training compute (FLOPs)': '2.5E+21',
      'Training dataset': '',
      'Training dataset size (datapoints)': '3.30E+09',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '2.50E+12',
      'Equivalent training time (hours)': '17408',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Turing NLG',
      'Domain': 'Language',
      'Task': 'Text autocompletion',
      'Organization(s)': 'Microsoft',
      'Organization Categorization': 'Industry',
      'Author(s)': 'C Rosset',
      'Publication date': '13/02/2020',
      'Year': '2020',
      'Reference': 'Turing-NLG: A 17-billion-parameter language model by Microsoft',
      'Link': 'https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/',
      'Citations': '3.40E+01',
      'Inclusion criteria': '',
      'Parameters': '1.70E+10',
      'Training compute (FLOPs)': '1.6E+22',
      'Training dataset': '',
      'Training dataset size (datapoints)': '3.48E+10',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '3.60E+13',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': 'Next token prediction',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'SimCLR',
      'Domain': 'Drawing',
      'Task': 'Image completion',
      'Organization(s)': 'Google Research, Brain Team',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton',
      'Publication date': '13/02/2020',
      'Year': '2020',
      'Reference': 'A Simple Framework for Contrastive Learning of Visual Representations',
      'Link': 'https://arxiv.org/abs/2002.05709',
      'Citations': '2.16E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '3.75E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'ProGen',
      'Domain': 'Other',
      'Task': 'Protein generation',
      'Organization(s)': 'Salesforce research, Stanford',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'A Madani, B McCann, N Naik, NS Keskar',
      'Publication date': '13/03/2020',
      'Year': '2020',
      'Reference': 'ProGen: Language Modeling for Protein Generation',
      'Link': 'https://www.biorxiv.org/content/10.1101/2020.03.07.982272v2',
      'Citations': '4.60E+01',
      'Inclusion criteria': '',
      'Parameters': '1.20E+09',
      'Training compute (FLOPs)': '2.7E+20',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'ELECTRA',
      'Domain': 'Language',
      'Task': 'Text autocompletion',
      'Organization(s)': 'Stanford, Google Brain',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning',
      'Publication date': '23/03/2020',
      'Year': '2020',
      'Reference': 'Electra: pre-training text encoders as discriminators rather than generators',
      'Link': 'https://arxiv.org/abs/2003.10555v1',
      'Citations': '8.42E+02',
      'Inclusion criteria': '',
      'Parameters': '3.35E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '7.90E+10',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'MetNet',
      'Domain': 'Other',
      'Task': 'Weather prediction',
      'Organization(s)': 'Google',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Casper Kaae Sønderby, Lasse Espeholt, Jonathan Heek, Mostafa Dehghani, Avital Oliver, Tim Salimans, Shreya Agrawal, Jason Hickey, Nal Kalchbrenner',
      'Publication date': '24/03/2020',
      'Year': '2020',
      'Reference': 'MetNet: A Neural Weather Model for Precipitation Forecasting',
      'Link': 'https://arxiv.org/abs/2003.12140',
      'Citations': '4.30E+01',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Agent57',
      'Domain': 'Games',
      'Task': 'Atari',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'AP Badia, B Piot, S Kapturowski',
      'Publication date': '30/03/2020',
      'Year': '2020',
      'Reference': 'Agent57: Outperforming the Atari Human Benchmark',
      'Link': 'https://arxiv.org/abs/2003.13350',
      'Citations': '1.32E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Vision',
      'Task': 'Person re-identification',
      'Organization(s)': 'Xiamen University, Australian National University, Carnegie Mellon University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, Yi Yang ',
      'Publication date': '03/04/2020',
      'Year': '2020',
      'Reference': ' Random Erasing Data Augmentation ',
      'Link': 'https://arxiv.org/abs/1708.04896',
      'Citations': '1.42E+03',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'MobileBERT',
      'Domain': 'Language',
      'Task': 'Text autocompletion',
      'Organization(s)': 'Carnegie Mellon University, Google Brain',
      'Organization Categorization': 'Industry - Academia Collaboration (Industry leaning)',
      'Author(s)': 'Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, Denny Zhou',
      'Publication date': '06/04/2020',
      'Year': '2020',
      'Reference': 'MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices',
      'Link': 'https://arxiv.org/abs/2004.02984',
      'Citations': '1.80E+02',
      'Inclusion criteria': '',
      'Parameters': '2.53E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '5.36E+09',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'CURL',
      'Domain': 'Games',
      'Task': 'Atari Games',
      'Organization(s)': 'UC Berkeley',
      'Organization Categorization': 'Academia',
      'Author(s)': 'A Srinivas, M Laskin, P Abbeel',
      'Publication date': '08/04/2020',
      'Year': '2020',
      'Reference': 'CURL: Contrastive Unsupervised Representations for Reinforcement Learning',
      'Link': 'https://arxiv.org/abs/2004.04136v4',
      'Citations': '1.22E+02',
      'Inclusion criteria': 'SOTA improvement',
      'Parameters': '9.07E+05',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Go-explore',
      'Domain': 'Games',
      'Task': 'Atari',
      'Organization(s)': 'Uber AI, OpenAI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'A Ecoffet, J Huizinga, J Lehman, KO Stanley, J Clune',
      'Publication date': '27/04/2020',
      'Year': '2020',
      'Reference': 'First return, then explore',
      'Link': 'https://arxiv.org/abs/2004.12919',
      'Citations': '4.00E+01',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'GPT-3 175B',
      'Domain': 'Language',
      'Task': 'Text autocompletion',
      'Organization(s)': 'OpenAI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei',
      'Publication date': '28/04/2020',
      'Year': '2020',
      'Reference': 'Language models are Few- Shot Learners',
      'Link': 'https://arxiv.org/abs/2005.14165',
      'Citations': '1.53E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '1.75E+11',
      'Training compute (FLOPs)': '3.1E+23',
      'Training dataset': 'CommonCrawl; WebText2; Books1; Books2; Wikipedia',
      'Training dataset size (datapoints)': '3.74E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '7.40E+14',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '45TB',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Once for All',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'MIT-IBM Watson AI Lab',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han',
      'Publication date': '29/04/2020',
      'Year': '2020',
      'Reference': 'Once for all: Train one network and specialize it for efficient deployment.',
      'Link': 'https://arxiv.org/abs/1908.09791',
      'Citations': '3.71E+02',
      'Inclusion criteria': '',
      'Parameters': '7.70E+06',
      'Training compute (FLOPs)': '1.8E+21',
      'Training dataset': 'Imagenet',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'SqueezeBERT',
      'Domain': 'Language',
      'Task': 'Text autocompletion',
      'Organization(s)': 'Berkeley',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer',
      'Publication date': '10/06/2020',
      'Year': '2020',
      'Reference': 'SqueezeBERT: What can computer vision teach NLP about efficient neural networks?',
      'Link': 'https://arxiv.org/abs/2006.11316',
      'Citations': '3.00E+01',
      'Inclusion criteria': '',
      'Parameters': '5.11E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '7.42E+09',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'iGPT-L',
      'Domain': 'Drawing',
      'Task': 'Image completion',
      'Organization(s)': 'Open AI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever',
      'Publication date': '17/06/2020',
      'Year': '2020',
      'Reference': 'Generative Pretraining from Pixels',
      'Link': 'https://openai.com/blog/image-gpt/',
      'Citations': '1.82E+02',
      'Inclusion criteria': '',
      'Parameters': '1.36E+09',
      'Training compute (FLOPs)': '8.9E+21',
      'Training dataset': 'ILSVRC 2012',
      'Training dataset size (datapoints)': '9.60E+06',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'iGPT-XL',
      'Domain': 'Drawing',
      'Task': 'Image completion',
      'Organization(s)': 'Open AI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever',
      'Publication date': '17/06/2020',
      'Year': '2020',
      'Reference': 'Generative Pretraining from Pixels',
      'Link': 'https://openai.com/blog/image-gpt/',
      'Citations': '1.82E+02',
      'Inclusion criteria': '',
      'Parameters': '6.80E+09',
      'Training compute (FLOPs)': '3.3E+22',
      'Training dataset': 'ILSVRC 2012',
      'Training dataset size (datapoints)': '9.60E+06',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'GShard (600B)',
      'Domain': 'Language',
      'Task': 'Translation',
      'Organization(s)': 'Google Brain',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen',
      'Publication date': '30/06/2020',
      'Year': '2020',
      'Reference': 'GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding',
      'Link': 'https://arxiv.org/abs/2006.16668',
      'Citations': '9.10E+01',
      'Inclusion criteria': '',
      'Parameters': '6.00E+11',
      'Training compute (FLOPs)': '1.3E+22',
      'Training dataset': '',
      'Training dataset size (datapoints)': '2.60E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'GShard (dense)',
      'Domain': 'Language',
      'Task': 'Translation',
      'Organization(s)': 'Google Brain',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen',
      'Publication date': '30/06/2020',
      'Year': '2020',
      'Reference': 'GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding',
      'Link': 'https://arxiv.org/abs/2006.16668',
      'Citations': '9.10E+01',
      'Inclusion criteria': '',
      'Parameters': '2.30E+09',
      'Training compute (FLOPs)': '2.6E+22',
      'Training dataset': '',
      'Training dataset size (datapoints)': '2.60E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Hopfield Networks (2020)',
      'Domain': 'Other',
      'Task': '',
      'Organization(s)': 'Johannes Kepler University Linz,Institute of Advanced Research in Artificial Intelligence,University of Oslo',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlović, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, Günter Klambauer, Johannes Brandstetter, Sepp Hochreiter',
      'Publication date': '16/07/2020',
      'Year': '2020',
      'Reference': 'Hopfield Networks is All You Need',
      'Link': 'https://arxiv.org/abs/2008.02217',
      'Citations': '5.40E+01',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'EfficientDet',
      'Domain': 'Vision',
      'Task': 'Object detection',
      'Organization(s)': 'Google Research, Brain Team',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Mingxing Tan, Ruoming Pang, Quoc V. Le',
      'Publication date': '27/07/2020',
      'Year': '2020',
      'Reference': 'EfficientDet: Scalable and Efficient Object Detection',
      'Link': 'https://openaccess.thecvf.com/content_CVPR_2020/html/Tan_EfficientDet_Scalable_and_Efficient_Object_Detection_CVPR_2020_paper.html',
      'Citations': '7.06E+02',
      'Inclusion criteria': '',
      'Parameters': '5.20E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '3.25E+11',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'ViT-H/14',
      'Domain': 'Vision',
      'Task': 'Image representation',
      'Organization(s)': 'Google Research, Brain Team',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',
      'Publication date': '28/09/2020',
      'Year': '2020',
      'Reference': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale',
      'Link': 'https://openreview.net/forum?id=YicbFdNTTy',
      'Citations': '1.91E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '1.3E+22',
      'Training dataset': 'Imagenet-1k',
      'Training dataset size (datapoints)': '1.28E+06',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'wave2vec 2.0 LARGE',
      'Domain': 'Speech',
      'Task': 'Speech completion',
      'Organization(s)': 'Facebook',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli',
      'Publication date': '22/10/2020',
      'Year': '2020',
      'Reference': 'wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations',
      'Link': 'https://arxiv.org/abs/2006.11477',
      'Citations': '4.10E+02',
      'Inclusion criteria': 'SOTA Improvement',
      'Parameters': '3.17E+08',
      'Training compute (FLOPs)': '4.3E+20',
      'Training dataset': 'LibriSpeech',
      'Training dataset size (datapoints)': '4.37E+10',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'ViT-Base/32',
      'Domain': 'Vision',
      'Task': 'Image representation',
      'Organization(s)': 'Google Research, Brain Team',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',
      'Publication date': '22/10/2020',
      'Year': '2020',
      'Reference': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale',
      'Link': 'https://arxiv.org/abs/2010.11929',
      'Citations': '7.29E+02',
      'Inclusion criteria': '',
      'Parameters': '8.60E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '0.25 (512px image, one TPUv3 core)',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': 'Score',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'ViT-Huge/14',
      'Domain': 'Vision',
      'Task': 'Image representation',
      'Organization(s)': 'Google Research, Brain Team',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby',
      'Publication date': '22/10/2020',
      'Year': '2020',
      'Reference': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale',
      'Link': 'https://arxiv.org/abs/2010.11929',
      'Citations': '7.29E+02',
      'Inclusion criteria': '',
      'Parameters': '6.32E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '100 (512px image, one TPUv3 core)',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'SimCLRv2',
      'Domain': '',
      'Task': '',
      'Organization(s)': 'Google Research, Brain team',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton',
      'Publication date': '26/10/2020',
      'Year': '2020',
      'Reference': 'Big self- supervised models are strong semi-supervised learners.',
      'Link': 'https://arxiv.org/abs/2006.10029',
      'Citations': '4.57E+02',
      'Inclusion criteria': '',
      'Parameters': '7.95E+08',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.28E+06',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'KEPLER',
      'Domain': 'Language',
      'Task': 'Relation Extraction',
      'Organization(s)': 'Tsinghua University, Princeton, Mila- Quebec AI, University de Montreal, HEC, CIFAR',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan Liu, Juanzi Li, and Jian Tang.',
      'Publication date': '23/11/2020',
      'Year': '2020',
      'Reference': 'KEPLER: A Unified Model for Knowledge Embedding and Pre- trained Language Representation.',
      'Link': 'https://arxiv.org/abs/1911.06136',
      'Citations': '9.60E+01',
      'Inclusion criteria': '',
      'Parameters': '1.10E+08',
      'Training compute (FLOPs)': '1.2E+20',
      'Training dataset': 'Wikipedia+BookCorpus',
      'Training dataset size (datapoints)': '3.30E+09',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'AlphaFold2',
      'Domain': 'Other',
      'Task': 'Protein folding prediction',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Kathryn Tunyasuvunakool, Olaf Ronneberger, Russ Bates, Augustin Žídek, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Anna Potapenko, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Martin Steinegger, Michalina Pacholska, David Silver, Oriol Vinyals, Andrew W Senior, Koray Kavukcuoglu, Pushmeet Kohli, Demis Hassabis.',
      'Publication date': '30/11/2020',
      'Year': '2020',
      'Reference': 'High Accuracy Protein Structure Prediction Using Deep Learning',
      'Link': 'https://www.nature.com/articles/s41586-021-03819-2',
      'Citations': '5.80E+01',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'CPM-Large',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Tsinghua University, BAAI',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'Z Zhang, X Han, H Zhou, P Ke, Y Gu, D Ye, Y Qin, Y Su',
      'Publication date': '01/12/2020',
      'Year': '2020',
      'Reference': 'CPM: A Large-scale Generative Chinese Pre-trained Language Model',
      'Link': 'https://arxiv.org/abs/2012.00413',
      'Citations': '1.00E+01',
      'Inclusion criteria': '',
      'Parameters': '2.60E+09',
      'Training compute (FLOPs)': '1.8E+21',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.67E+10',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '1.20E+07',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'VQGAN + CLIP',
      'Domain': 'Drawing',
      'Task': 'Text-to-image',
      'Organization(s)': 'Heidelberg University',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Patrick Esser, Robin Rombach, Björn Ommer',
      'Publication date': '17/12/2020',
      'Year': '2020',
      'Reference': 'Taming Transformers for High-Resolution Image Synthesis',
      'Link': 'https://github.com/CompVis/taming-transformers',
      'Citations': '3.20E+01',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'AraGPT2-Mega',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'American University of Beirut',
      'Organization Categorization': 'Academia',
      'Author(s)': 'W Antoun, F Baly, H Hajj',
      'Publication date': '31/12/2020',
      'Year': '2020',
      'Reference': 'AraGPT2: Pre-Trained Transformer for Arabic Language Generation',
      'Link': 'https://arxiv.org/abs/2012.15520',
      'Citations': '4.00E+00',
      'Inclusion criteria': '',
      'Parameters': '1.50E+09',
      'Training compute (FLOPs)': '2.0E+21',
      'Training dataset': '',
      'Training dataset size (datapoints)': '8.80E+09',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'NEO (DL:RM-2022)',
      'Domain': 'Recommendation',
      'Task': '',
      'Organization(s)': 'Facebook',
      'Organization Categorization': 'Industry',
      'Author(s)': 'D Mudigere, Y Hao, J Huang, A Tulloch',
      'Publication date': '15/09/2021',
      'Year': '2021',
      'Reference': 'Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models',
      'Link': 'https://arxiv.org/abs/2104.05158',
      'Citations': '2.00E+00',
      'Inclusion criteria': '',
      'Parameters': '3.00E+12',
      'Training compute (FLOPs)': '1.1E+21',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Primer',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Google Research, Brain Team',
      'Organization Categorization': 'Industry',
      'Author(s)': 'DavidR.So, WojciechMan ́ke, HanxiaoLiu, ZihangDai, NoamShazeer, QuocV.Le',
      'Publication date': '24/01/2022',
      'Year': '2022',
      'Reference': 'Primer: Searching for Efficient Transformers for Language Modeling',
      'Link': 'https://arxiv.org/abs/2109.08668',
      'Citations': '1.30E+01',
      'Inclusion criteria': '',
      'Parameters': '1.90E+09',
      'Training compute (FLOPs)': '7.1E+21',
      'Training dataset': 'C4',
      'Training dataset size (datapoints)': '1.73E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'DeBERTa',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Microsoft',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen',
      'Publication date': '06/10/2021',
      'Year': '2021',
      'Reference': 'DeBERTa: Decoding-enhanced BERT with Disentangled Attention',
      'Link': 'https://arxiv.org/abs/2006.03654',
      'Citations': '1.76E+02',
      'Inclusion criteria': '',
      'Parameters': '1.50E+09',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.56E+10',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '160 (pretraining), ',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'BigSSL',
      'Domain': 'Language',
      'Task': 'Audio speech recognition',
      'Organization(s)': 'Google, Apple',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Yu Zhang,  Daniel S. Park, Wei Han,James Qin, Anmol Gulati, Joel Shor, Aren Jansen, Yuanzhong Xu, Yanping Huang, Shibo Wang, Zongwei Zhou, Bo Li, Min Ma, William Chan, Jiahui Yu, Yongqiang Wang, Liangliang Cao, Khe Chai Sim, Bhuvana Ramabhadran, Tara N. Sainath, Françoise Beaufays, Zhifeng Chen, Quoc V. Le, Chung-Cheng Chiu, Ruoming Pang and Yonghui Wu',
      'Publication date': '01/10/2021',
      'Year': '2021',
      'Reference': 'BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition',
      'Link': 'https://arxiv.org/abs/2109.13226',
      'Citations': '4.30E+01',
      'Inclusion criteria': '',
      'Parameters': '8.00E+09',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '2.56E+12',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Japanese dialog transformers',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'NTT Communication Science Laboratories',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Hiroaki Sugiyama, Masahiro Mizukami, Tsunehiro Arimoto, Hiromi Narimatsu, Yuya Chiba, Hideharu Nakajima, Toyomi Meguro',
      'Publication date': '11/09/2021',
      'Year': '2021',
      'Reference': 'Empirical Analysis of Training Strategies of Transformer-based Japanese Chit-chat Systems',
      'Link': 'https://arxiv.org/abs/2109.05217',
      'Citations': '3.20E+01',
      'Inclusion criteria': '',
      'Parameters': '1.60E+09',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '2.10E+09',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'PCL-BAIDU Wenxin (ERNIE 3.0 Titan)',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Baidu',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Shuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng',
      'Publication date': '23/12/2021',
      'Year': '2021',
      'Reference': 'ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation',
      'Link': 'https://arxiv.org/abs/2112.12731',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '2.60E+11',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'CLIP (ViT L/14@336px)',
      'Domain': 'Multimodal',
      'Task': 'Zero-shot image classification',
      'Organization(s)': 'Open AI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever',
      'Publication date': '05/01/2021',
      'Year': '2021',
      'Reference': 'Learning Transferable Visual Models From Natural Language Supervision',
      'Link': 'https://arxiv.org/abs/2103.00020',
      'Citations': '1.30E+02',
      'Inclusion criteria': '',
      'Parameters': '3.70E+08',
      'Training compute (FLOPs)': '1.1E+22',
      'Training dataset': 'Custom image-text pairs from the internet',
      'Training dataset size (datapoints)': '4.00E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '1.10E+08',
      'Equivalent training time (hours)': '86016',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '1.00E+06',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'DALL-E',
      'Domain': 'Drawing',
      'Task': 'Text-to-image',
      'Organization(s)': 'OpenAI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever',
      'Publication date': '05/01/2021',
      'Year': '2021',
      'Reference': 'Zero-Shot Text-to-Image Generation',
      'Link': 'https://openai.com/blog/dall-e/',
      'Citations': '8.00E+01',
      'Inclusion criteria': '',
      'Parameters': '1.20E+10',
      'Training compute (FLOPs)': '4.7E+22',
      'Training dataset': '',
      'Training dataset size (datapoints)': '2.50E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '132000',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'CLIP (ResNet-50)',
      'Domain': 'Multimodal',
      'Task': 'Zero-shot image classification',
      'Organization(s)': 'Open AI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever',
      'Publication date': '05/01/2021',
      'Year': '2021',
      'Reference': 'Learning Transferable Visual Models From Natural Language Supervision',
      'Link': 'https://arxiv.org/abs/2103.00020',
      'Citations': '1.30E+02',
      'Inclusion criteria': '',
      'Parameters': '8.86E+07',
      'Training compute (FLOPs)': '',
      'Training dataset': 'Custom image-text pairs from the internet',
      'Training dataset size (datapoints)': '4.00E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '7.00E+06',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Switch',
      'Domain': 'Language',
      'Task': 'Text autocompletion',
      'Organization(s)': 'Google Brain',
      'Organization Categorization': 'Industry',
      'Author(s)': 'William Fedus, Barret Zoph, Noam Shazeer',
      'Publication date': '11/01/2021',
      'Year': '2021',
      'Reference': 'Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity',
      'Link': 'https://arxiv.org/abs/2101.03961',
      'Citations': '8.00E+01',
      'Inclusion criteria': '',
      'Parameters': '1.60E+12',
      'Training compute (FLOPs)': '8.2E+22',
      'Training dataset': '',
      'Training dataset size (datapoints)': '4.32E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'FLAN',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Google Research',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, Quoc V. Le',
      'Publication date': '12/01/2021',
      'Year': '2021',
      'Reference': 'Finetuned Language Models Are Zero-Shot Learners',
      'Link': 'https://arxiv.org/abs/2109.01652',
      'Citations': '9.00E+00',
      'Inclusion criteria': 'SOTA Improvement',
      'Parameters': '1.37E+11',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.87E+12',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Rational DQN Average',
      'Domain': 'Games',
      'Task': 'Atari Games',
      'Organization(s)': 'TU Darmstadt',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Q Delfosse, P Schramowski, A Molina',
      'Publication date': '18/02/2021',
      'Year': '2021',
      'Reference': 'Recurrent Rational Networks',
      'Link': 'https://openreview.net/forum?id=gnRmI8TatHV',
      'Citations': '1.00E+00',
      'Inclusion criteria': 'SOTA improvement',
      'Parameters': '1.68E+06',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Meta Pseudo Labels',
      'Domain': 'Vision',
      'Task': 'Image Classification',
      'Organization(s)': 'Google AI, Brain team',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Hieu Pham, Zihang Dai, Qizhe Xie, Minh-Thang Luong, and Quoc V. Le',
      'Publication date': '01/03/2021',
      'Year': '2021',
      'Reference': 'Meta pseudo labels',
      'Link': 'https://arxiv.org/abs/2003.10580',
      'Citations': '1.31E+02',
      'Inclusion criteria': 'SOTA Improvement',
      'Parameters': '4.80E+08',
      'Training compute (FLOPs)': '2.1E+23',
      'Training dataset': 'ImageNet',
      'Training dataset size (datapoints)': '1.30E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'M6-10B',
      'Domain': 'Multimodal',
      'Task': '',
      'Organization(s)': 'Tsinghua University, Alibaba Group',
      'Organization Categorization': 'Industry - Academia Collaboration (Industry leaning)',
      'Author(s)': 'J Lin, R Men, A Yang, C Zhou, M Ding, Y Zhang',
      'Publication date': '01/03/2021',
      'Year': '2021',
      'Reference': 'M6: A Chinese Multimodal Pretrainer',
      'Link': 'https://arxiv.org/abs/2103.00823',
      'Citations': '8.00E+00',
      'Inclusion criteria': '',
      'Parameters': '1.00E+10',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.90E+12',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'M6-100B',
      'Domain': 'Multimodal',
      'Task': '',
      'Organization(s)': 'Tsinghua University, Alibaba Group',
      'Organization Categorization': 'Industry - Academia Collaboration (Industry leaning)',
      'Author(s)': 'J Lin, R Men, A Yang, C Zhou, M Ding, Y Zhang',
      'Publication date': '01/03/2021',
      'Year': '2021',
      'Reference': 'M6: A Chinese Multimodal Pretrainer',
      'Link': 'https://arxiv.org/abs/2103.00823',
      'Citations': '8.00E+00',
      'Inclusion criteria': '',
      'Parameters': '1.00E+11',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.90E+12',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Wu Dao - Wen Yuan',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'BAAI',
      'Organization Categorization': 'Industry',
      'Author(s)': '',
      'Publication date': '01/03/2021',
      'Year': '2021',
      'Reference': 'China\'s GPT-3? BAAI Introduces Superscale Intelligence Model \'Wu Dao 1.0\'',
      'Link': 'https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '2.60E+09',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Wu Dao - Wen Lan',
      'Domain': 'Multimodal',
      'Task': '',
      'Organization(s)': 'BAAI',
      'Organization Categorization': 'Industry',
      'Author(s)': '',
      'Publication date': '01/03/2021',
      'Year': '2021',
      'Reference': 'China\'s GPT-3? BAAI Introduces Superscale Intelligence Model \'Wu Dao 1.0\'',
      'Link': 'https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '1.00E+09',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Wu Dao - Wen Hui',
      'Domain': 'Multimodal',
      'Task': '',
      'Organization(s)': 'BAAI',
      'Organization Categorization': 'Industry',
      'Author(s)': '',
      'Publication date': '01/03/2021',
      'Year': '2021',
      'Reference': 'China\'s GPT-3? BAAI Introduces Superscale Intelligence Model \'Wu Dao 1.0\'',
      'Link': 'https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '1.13E+10',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Wu Dao - Wen Su',
      'Domain': 'Other',
      'Task': 'Proteins',
      'Organization(s)': 'BAAI',
      'Organization Categorization': 'Industry',
      'Author(s)': '',
      'Publication date': '01/03/2021',
      'Year': '2021',
      'Reference': 'China\'s GPT-3? BAAI Introduces Superscale Intelligence Model \'Wu Dao 1.0\'',
      'Link': 'https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Seq2Seq',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Facebook AI Research',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston',
      'Publication date': '05/03/2021',
      'Year': '2021',
      'Reference': 'Recipes for building an open-domain chatbot',
      'Link': 'https://arxiv.org/abs/2004.13637',
      'Citations': '2.07E+02',
      'Inclusion criteria': 'SOTA Improvement',
      'Parameters': '9.40E+09',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'M6-T',
      'Domain': 'Multimodal',
      'Task': '',
      'Organization(s)': 'Alibaba Group',
      'Organization Categorization': 'Industry',
      'Author(s)': 'An Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jiamang Wang, Yong Li, Di Zhang, Wei Lin, Lin Qu, Jingren Zhou, Hongxia Yang',
      'Publication date': '05/03/2021',
      'Year': '2021',
      'Reference': 'M6-T: Exploring Sparse Expert Models and Beyond',
      'Link': 'https://arxiv.org/abs/2105.15082',
      'Citations': '2.00E+00',
      'Inclusion criteria': 'SOTA Improvement',
      'Parameters': '1.00E+12',
      'Training compute (FLOPs)': '',
      'Training dataset': 'M6-Corpus',
      'Training dataset size (datapoints)': '1.90E+12',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'GPT-Neo',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'EleutherAI',
      'Organization Categorization': 'Research collective',
      'Author(s)': '',
      'Publication date': '21/03/2021',
      'Year': '2021',
      'Reference': 'GPT-Neo',
      'Link': 'https://www.eleuther.ai/projects/gpt-neo/',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '2.70E+09',
      'Training compute (FLOPs)': '7.9E+21',
      'Training dataset': 'The Pile',
      'Training dataset size (datapoints)': '8.86E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': '',
      'Domain': 'Language',
      'Task': 'Text autocompletion',
      'Organization(s)': 'Stanford, Microsoft Research, NVIDIA',
      'Organization Categorization': 'Industry - Academia Collaboration (Industry leaning)',
      'Author(s)': 'Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, Matei Zaharia',
      'Publication date': '09/04/2021',
      'Year': '2021',
      'Reference': 'Efficient Large-Scale Language Model Training on GPU Clusters',
      'Link': 'https://arxiv.org/abs/2104.04473',
      'Citations': '7.00E+00',
      'Inclusion criteria': '',
      'Parameters': '1.00E+12',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'DLRM-12T',
      'Domain': 'Recommendation',
      'Task': '',
      'Organization(s)': 'Facebook AI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Andrew Tulloch, Srinivas Sridharan, Xing Liu, Mustafa Ozdal, Jade Nie, Jongsoo Park, Liang Luo, Jie Amy Yang, Leon Gao, Dmytro Ivchenko, Aarti Basant, Yuxi Hu, Jiyan Yang, Ehsan K. Ardestani, Xiaodong Wang, Rakesh Komuravelli, Ching-Hsiang Chu, Serhat Yilmaz, Huayu Li, Jiyuan Qian, Zhuobo Feng, Yinbin Ma, Junjie Yang, Ellie Wen, Hong Li, Lin Yang, Chonglin Sun, Whitney Zhao, Dimitry Melts, Krishna Dhulipala, KR Kishore, Tyler Graf, Assaf Eisenman, Kiran Kumar Matam, Adi Gangidi, Guoqiang Jerry Chen, Manoj Krishnan, Avinash Nayak, Krishnakumar Nair, Bharath Muthiah, Mahmoud khorashadi, Pallab Bhattacharya, Petr Lapukhov, Maxim Naumov, Lin Qiao, Mikhail Smelyanskiy, Bill Jia, Vijay Rao',
      'Publication date': '12/04/2021',
      'Year': '2021',
      'Reference': 'Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models',
      'Link': 'https://arxiv.org/abs/2104.05158',
      'Citations': '2.00E+00',
      'Inclusion criteria': '',
      'Parameters': '1.20E+13',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'PanGu-α',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'PanGu-α team',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi LiaoZhiwei WangXin JiangZhenzhang YangKaisheng WangXiaoda ZhangChen LiZiyan GongYifan YaoXinjing HuangJun WangJianfeng YuQi GuoYue YuYan ZhangJin WangHengtao TaoDasen YanZexuan YiFang PengFangqing JiangHan ZhangLingfeng DengYehong ZhangZhe LinChao ZhangShaojie ZhangMingyue GuoShanzhi GuGaojun FanYaowei WangXuefeng JinQun LiuYonghong Tian',
      'Publication date': '25/04/2021',
      'Year': '2021',
      'Reference': 'PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation',
      'Link': 'https://arxiv.org/abs/2104.12369',
      'Citations': '5.00E+00',
      'Inclusion criteria': '',
      'Parameters': '2.07E+11',
      'Training compute (FLOPs)': '5.8E+22',
      'Training dataset': 'Custom dataset',
      'Training dataset size (datapoints)': '2.00E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'GPT-J-6B',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': '',
      'Organization Categorization': 'Research collective',
      'Author(s)': 'Aran Komatsuzaki',
      'Publication date': '01/05/2021',
      'Year': '2021',
      'Reference': 'GPT-J-6B: 6B JAX-Based Transformer',
      'Link': 'https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '6.05E+09',
      'Training compute (FLOPs)': '1.5E+22',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.60E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'ProtT5-XXL',
      'Domain': 'Other',
      'Task': 'Proteins',
      'Organization(s)': 'Technical University of Munich, Med AI Technology, Google AI, NVIDIA, Oak Ridge National Laboratory',
      'Organization Categorization': 'Industry - Academia Collaboration',
      'Author(s)': 'A Elnaggar, M Heinzinger, C Dallago, G Rihawi',
      'Publication date': '04/05/2021',
      'Year': '2021',
      'Reference': 'ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Learning',
      'Link': 'https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3',
      'Citations': '5.70E+01',
      'Inclusion criteria': '',
      'Parameters': '1.10E+10',
      'Training compute (FLOPs)': '7.4E+22',
      'Training dataset': 'UniRef; BDF',
      'Training dataset size (datapoints)': '3.93E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'HyperClova',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Naver Corp',
      'Organization Categorization': 'Industry',
      'Author(s)': '',
      'Publication date': '25/05/2021',
      'Year': '2021',
      'Reference': 'Hyperclova',
      'Link': 'https://www.navercorp.com/promotion/pressReleasesView/30546',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '2.04E+11',
      'Training compute (FLOPs)': '6.3E+22',
      'Training dataset': '',
      'Training dataset size (datapoints)': '5.60E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'CogView',
      'Domain': 'Drawing',
      'Task': 'Text-to-image',
      'Organization(s)': 'Tsinghua University, DAMO academy Alibaba',
      'Organization Categorization': 'Industry - Academia Collaboration (Academia leaning)',
      'Author(s)': 'M Ding, Z Yang, W Hong, W Zheng, C Zhou',
      'Publication date': '26/05/2021',
      'Year': '2021',
      'Reference': 'CogView: Mastering Text-to-Image Generation via Transformers',
      'Link': 'https://arxiv.org/abs/2105.13290',
      'Citations': '1.00E+00',
      'Inclusion criteria': '',
      'Parameters': '4.00E+09',
      'Training compute (FLOPs)': '2.7E+22',
      'Training dataset': 'MS COCO',
      'Training dataset size (datapoints)': '3.00E+07',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Transformer local-attention (NesT-B)',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'Google cloud AI, Google research, Rutgers University',
      'Organization Categorization': 'Industry - Academia Collaboration (Industry Leaning)',
      'Author(s)': 'Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, Tomas Pfister',
      'Publication date': '26/05/2021',
      'Year': '2021',
      'Reference': 'Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding',
      'Link': 'https://arxiv.org/abs/2105.12723v4',
      'Citations': '5.73E+03',
      'Inclusion criteria': 'Highly cited',
      'Parameters': '',
      'Training compute (FLOPs)': '2.4E+19',
      'Training dataset': 'Imagenet-1k',
      'Training dataset size (datapoints)': '1.28E+06',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Wu Dao 2.0',
      'Domain': 'Multimodal',
      'Task': '',
      'Organization(s)': 'BAAI',
      'Organization Categorization': '',
      'Author(s)': 'A Tarantola',
      'Publication date': '01/06/2021',
      'Year': '2021',
      'Reference': 'China\'s gigantic multi-modal AI is no one-trick pony',
      'Link': 'https://www.engadget.com/chinas-gigantic-multi-modal-ai-is-no-one-trick-pony-211414388.html',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '1.75E+12',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'ViT-G/14',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'Google Research, Brain Team',
      'Organization Categorization': 'Industry',
      'Author(s)': 'X Zhai, A Kolesnikov, N Houlsby, L Beyer',
      'Publication date': '08/06/2021',
      'Year': '2021',
      'Reference': 'Scaling Vision Transformers',
      'Link': 'https://arxiv.org/abs/2106.04560',
      'Citations': '6.00E+00',
      'Inclusion criteria': '',
      'Parameters': '1.80E+09',
      'Training compute (FLOPs)': '3.4E+21',
      'Training dataset': 'JFT-3B',
      'Training dataset size (datapoints)': '3.00E+09',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Denoising Diffusion Probabilistic Models (LSUN Bedroom)',
      'Domain': 'Drawing',
      'Task': '',
      'Organization(s)': 'UC Berkeley',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Jonathan Ho, Ajay Jain, Pieter Abbeel',
      'Publication date': '11/06/2021',
      'Year': '2021',
      'Reference': 'Denoising Diffusion Probabilistic Models',
      'Link': 'https://arxiv.org/abs/2006.11239',
      'Citations': '1.36E+02',
      'Inclusion criteria': 'SOTA Improvement',
      'Parameters': '2.56E+08',
      'Training compute (FLOPs)': '1.6E+18',
      'Training dataset': '',
      'Training dataset size (datapoints)': '3.03E+06',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'ALIGN',
      'Domain': 'Multimodal',
      'Task': 'Representation Learning',
      'Organization(s)': 'Google AI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,HieuPham,QuocV.Le,YunhsuanSung, Zhen Li, and Tom Duerig',
      'Publication date': '11/06/2021',
      'Year': '2021',
      'Reference': 'Scaling up visual and vision-language representation learning with noisy text supervision',
      'Link': 'https://arxiv.org/abs/2102.05918',
      'Citations': '7.50E+01',
      'Inclusion criteria': '',
      'Parameters': '8.20E+08',
      'Training compute (FLOPs)': '2.2E+23',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.60E+09',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'ERNIE 3.0',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Baidu Inc. ',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Y Sun, S Wang, S Feng, S Ding, C Pang',
      'Publication date': '05/07/2021',
      'Year': '2021',
      'Reference': 'ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation',
      'Link': 'http://research.baidu.com/Blog/index-view?id=160',
      'Citations': '1.00E+00',
      'Inclusion criteria': '',
      'Parameters': '1.00E+10',
      'Training compute (FLOPs)': '2.4E+18',
      'Training dataset': '',
      'Training dataset size (datapoints)': '6.68E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Codex',
      'Domain': 'Language',
      'Task': 'Code autocompletion',
      'Organization(s)': 'Open AI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Mark Chen , Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,  Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,  Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba ',
      'Publication date': '07/07/2021',
      'Year': '2021',
      'Reference': 'Evaluating Large Language Models Trained on Code',
      'Link': 'https://openai.com/blog/openai-codex/',
      'Citations': '3.52E+02',
      'Inclusion criteria': '',
      'Parameters': '1.20E+10',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '3.18E+10',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'HuBERT',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Facebook AI Research',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed',
      'Publication date': '27/07/2021',
      'Year': '2021',
      'Reference': 'HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units',
      'Link': 'https://arxiv.org/abs/2106.07447',
      'Citations': '3.70E+01',
      'Inclusion criteria': 'SOTA Improvement',
      'Parameters': '1.00E+09',
      'Training compute (FLOPs)': '5.5E+21',
      'Training dataset': 'LibriSpeech',
      'Training dataset size (datapoints)': '7.88E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'GOAT',
      'Domain': 'Games',
      'Task': 'Open ended play',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Open- Ended Learning Team',
      'Publication date': '27/07/2021',
      'Year': '2021',
      'Reference': 'Open-Ended Learning Leads to Generally Capable Agents',
      'Link': 'https://deepmind.com/blog/article/generally-capable-agents-emerge-from-open-ended-play',
      'Citations': '1.02E+02',
      'Inclusion criteria': '',
      'Parameters': '3.50E+06',
      'Training compute (FLOPs)': '7.8E+22',
      'Training dataset': 'XLand',
      'Training dataset size (datapoints)': '3.90E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'SEER',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'Facebook AI Research, Inria',
      'Organization Categorization': 'Industry - Academia Collaboration (Industry leaning)',
      'Author(s)': 'Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, Piotr Bojanowski',
      'Publication date': '29/07/2021',
      'Year': '2021',
      'Reference': 'Self-supervised Pretraining of Visual Features in the Wild',
      'Link': 'https://arxiv.org/abs/2103.01988',
      'Citations': '3.90E+01',
      'Inclusion criteria': 'Important context',
      'Parameters': '1.30E+09',
      'Training compute (FLOPs)': '4.4E+21',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.00E+09',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Perceiver IO',
      'Domain': 'Multimodal',
      'Task': '',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hénaff,Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, João Carreira',
      'Publication date': '08/02/2020',
      'Year': '2020',
      'Reference': 'Perceiver IO: A General Architecture for Structured Inputs & Outputs',
      'Link': 'https://arxiv.org/abs/2107.14795',
      'Citations': '1.04E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Jurassic-1-Jumbo',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'AI21 Labs',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Opher Lieber, Or Sharir, Barak Lenz, Yoav Shoham',
      'Publication date': '11/08/2021',
      'Year': '2021',
      'Reference': 'Jurassic-1: Technical Details and Evaluation',
      'Link': 'https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf',
      'Citations': '9.00E+00',
      'Inclusion criteria': '',
      'Parameters': '1.78E+11',
      'Training compute (FLOPs)': '3.7E+23',
      'Training dataset': '',
      'Training dataset size (datapoints)': '2.25E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'XLMR-XXL',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Facebook AI Research',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau',
      'Publication date': '17/08/2021',
      'Year': '2021',
      'Reference': 'Larger-Scale Transformers for Multilingual Masked Language Modeling',
      'Link': 'https://arxiv.org/abs/2105.00572',
      'Citations': '1.00E+00',
      'Inclusion criteria': 'SOTA Improvement',
      'Parameters': '1.07E+10',
      'Training compute (FLOPs)': '',
      'Training dataset': 'CC100',
      'Training dataset size (datapoints)': '1.25E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'FLAN',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Google Research',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le',
      'Publication date': '03/09/2021',
      'Year': '2021',
      'Reference': 'Fine-tuned Language Models Are Zero-Shot Learners',
      'Link': 'https://arxiv.org/abs/2109.01652',
      'Citations': '1.05E+02',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'MEB',
      'Domain': 'Search',
      'Task': '',
      'Organization(s)': 'Microsoft Bing',
      'Organization Categorization': 'Industry',
      'Author(s)': 'W Liu, Z Wang, X Liu, N Zeng, Y Liu, FE Alsaadi',
      'Publication date': '04/09/2021',
      'Year': '2021',
      'Reference': 'Make Every feature Binary: A 135B parameter sparse neural network for massively improved search relevance',
      'Link': 'https://www.microsoft.com/en-us/research/blog/make-every-feature-binary-a-135b-parameter-sparse-neural-network-for-massively-improved-search-relevance/',
      'Citations': '2.60E+01',
      'Inclusion criteria': '',
      'Parameters': '1.35E+11',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'M6-10T',
      'Domain': 'Multimodal',
      'Task': '',
      'Organization(s)': 'Alibaba',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Junyang Lin, An Yang, Jinze Bai, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Yong Li, Wei Lin, Jingren Zhou, Hongxia Yang',
      'Publication date': '8/10/2021',
      'Year': '2021',
      'Reference': 'M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining',
      'Link': 'https://arxiv.org/abs/2110.03888',
      'Citations': '1.40E+01',
      'Inclusion criteria': '',
      'Parameters': '1.00E+13',
      'Training compute (FLOPs)': '5.5E+21',
      'Training dataset': 'BookCorpus; English Wikipedia',
      'Training dataset size (datapoints)': '8.00E+09',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '122880',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Megatron-Turing NLG 530B',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Microsoft, NVIDIA',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Ali Alvi, Paresh Kharya',
      'Publication date': '11/10/2021',
      'Year': '2021',
      'Reference': 'Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World’s Largest and Most Powerful Generative Language Model',
      'Link': 'https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/',
      'Citations': 'NA',
      'Inclusion criteria': '',
      'Parameters': '5.30E+11',
      'Training compute (FLOPs)': '1.4E+24',
      'Training dataset': '',
      'Training dataset size (datapoints)': '2.03E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'T0-XXL',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Hugging Face, Brown University, BigScience',
      'Organization Categorization': 'Industry - Academia Collaboration (Industry leaning)',
      'Author(s)': 'Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, Alexander M. Rush',
      'Publication date': '15/10/2021',
      'Year': '2021',
      'Reference': 'Multitask Prompted Training Enables Zero-Shot Task Generalization',
      'Link': 'https://arxiv.org/abs/2110.08207',
      'Citations': '1.40E+01',
      'Inclusion criteria': '',
      'Parameters': '1.10E+10',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Cloob',
      'Domain': 'Multimodal',
      'Task': '',
      'Organization(s)': 'Ellis Unit Linz and LIT AI Lab, Johannes Kepler University, IARIA Vienna, HERE Technologies',
      'Organization Categorization': 'Industry - Academia Collaboration (Academia Leaning)',
      'Author(s)': 'Andreas Fürst ∗Elisabeth Rumetshofer ∗Johannes Lehner,Viet Tran,Fei Tang, Hubert Ramsauer, David Kreil, Michael Kopp, Günter Klambauer, Angela Bitto-Nemling, Sepp Hochreiter',
      'Publication date': '21/10/2021',
      'Year': '2021',
      'Reference': 'CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP',
      'Link': 'https://arxiv.org/abs/2110.11316',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.50E+07',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'EfficientZero',
      'Domain': 'Games',
      'Task': '',
      'Organization(s)': 'Tsinghua University, UC Berkeley, Shanghai Qi Zhi institute',
      'Organization Categorization': 'Academia',
      'Author(s)': 'Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, Yang Gao',
      'Publication date': '30/10/2021',
      'Year': '2021',
      'Reference': 'Mastering Atari Games with Limited Data',
      'Link': 'https://arxiv.org/abs/2111.00210',
      'Citations': '6.00E+00',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Academia',
    },
    {
      'System': 'Yuan 1.0',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Inspur',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Shaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang, Chong Shen, Hongli Liu, Feng Li, Hong Zhu, Jiangang Luo, Liang Xu, Xuanwei Zhang, Jun Liu',
      'Publication date': '12/10/2021',
      'Year': '2021',
      'Reference': 'Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning',
      'Link': 'https://arxiv.org/abs/2110.04725',
      'Citations': '4.00E+00',
      'Inclusion criteria': '',
      'Parameters': '2.45E+11',
      'Training compute (FLOPs)': '4.1E+23',
      'Training dataset': '',
      'Training dataset size (datapoints)': '8.35E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Gopher',
      'Domain': 'Language',
      'Task': 'Language modelling',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu and Geoffrey Irving',
      'Publication date': '8/12/2021',
      'Year': '2021',
      'Reference': 'Scaling Language Models: Methods, Analysis & Insights from Training Gopher',
      'Link': 'https://deepmind.com/blog/article/language-modelling-at-scale',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '2.80E+11',
      'Training compute (FLOPs)': '6.3E+23',
      'Training dataset': '',
      'Training dataset size (datapoints)': '2.25E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'RETRO-7B',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Sebastian Borgeaud†, Arthur Mensch†, Jordan Hoffmann†, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero,Karen Simonyan, Jack W. Rae‡, Erich Elsen‡ and Laurent Sifre',
      'Publication date': '7/2/2022',
      'Year': '2022',
      'Reference': 'Improving language models by retrieving from trillions of tokens',
      'Link': 'https://arxiv.org/abs/2112.04426',
      'Citations': '1.20E+01',
      'Inclusion criteria': '',
      'Parameters': '7.50E+09',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.50E+12',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'GLIDE',
      'Domain': 'Drawing',
      'Task': '',
      'Organization(s)': 'OpenAI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam Pamela Mishkin Bob McGrew IlyaSutskever MarkChen',
      'Publication date': '20/12/2021',
      'Year': '2021',
      'Reference': 'GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models',
      'Link': 'https://arxiv.org/abs/2112.10741',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '3.50E+09',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '2.50E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'LaMDA',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'Google',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le',
      'Publication date': '10/02/2022',
      'Year': '2022',
      'Reference': 'LaMDA: Language Models for Dialog Applications',
      'Link': 'https://arxiv.org/abs/2201.08239',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '1.37E+11',
      'Training compute (FLOPs)': '3.6E+23',
      'Training dataset': 'Infiniset',
      'Training dataset size (datapoints)': '1.56E+09',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'InstructGPT',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'OpenAI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Long Ouyang, Pamela Mishkin, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,John Schulman Amanda Askell, Fraser Kelton Peter Welinder, Luke Miller Maddie Simens Paul Christiano,Ryan Lowe,Chong Zhang Jacob Hilton, Sandhini Agarwal Katarina Slama Alex Ray, Jan Leike',
      'Publication date': '27/01/2022',
      'Year': '2022',
      'Reference': 'Training language models to follow instructions with human feedback',
      'Link': 'https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '3.32E+04',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'AlphaCode',
      'Domain': 'Language',
      'Task': 'Code generation',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'The Alpha Code team',
      'Publication date': '02/02/2022',
      'Year': '2022',
      'Reference': 'Competition-Level Code Generation with AlphaCode',
      'Link': 'https://deepmind.com/blog/article/Competitive-programming-with-AlphaCode',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Statement Curriculum Learning',
      'Domain': 'Language',
      'Task': 'Automated theorem proving',
      'Organization(s)': 'OpenAI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, Ilya Sutskever ',
      'Publication date': '02/03/2022',
      'Year': '2022',
      'Reference': 'Formal Mathematics Statement Curriculum Learning',
      'Link': 'https://arxiv.org/abs/2202.01344',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'DeepNet',
      'Domain': 'Language',
      'Task': 'Language modelling',
      'Organization(s)': 'Microsoft Research',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, Furu Wei',
      'Publication date': '01/03/2022',
      'Year': '2022',
      'Reference': 'DeepNet: Scaling Transformers to 1,000 Layers',
      'Link': 'https://arxiv.org/abs/2203.00555',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '3.20E+09',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.20E+10',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'GPT-NeoX-20B',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'EleutherAI',
      'Organization Categorization': 'Research Collective',
      'Author(s)': 'Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach',
      'Publication date': '09/02/2022',
      'Year': '2022',
      'Reference': 'Announcing GPT- NeoX- 20B',
      'Link': 'https://blog.eleuther.ai/announcing-20b/',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '2.00E+10',
      'Training compute (FLOPs)': '9.3E+22',
      'Training dataset': 'The Pile',
      'Training dataset size (datapoints)': '1.77E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Sparse all-MLP',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'MetaAI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Ping Yu, Mikel Artexte, Myle Ott, Sam  Shleifer, Hongyu Gong, Ves Stoyanov',
      'Publication date': '14/4/2022',
      'Year': '2022',
      'Reference': 'Efficient Language Modeling with Sparse all-MLP',
      'Link': 'https://arxiv.org/abs/2203.06850',
      'Citations': '0.00E+00',
      'Inclusion criteria': 'SOTA Improvement',
      'Parameters': '3.50E+09',
      'Training compute (FLOPs)': '2.2E+19',
      'Training dataset': 'RoBERTa dataset',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '20.41',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'BaGuaLu',
      'Domain': '',
      'Task': '',
      'Organization(s)': '',
      'Organization Categorization': '',
      'Author(s)': 'Zixuan Ma, Jiaao He, Jiezhong Qiu, Huanqi Cao, Yunawei Wang',
      'Publication date': '28/3/2022',
      'Year': '2022',
      'Reference': 'BaGuaLu: Targeting Brain Scale Pretrained Models with over 37 Million Cores',
      'Link': 'https://dl.acm.org/doi/abs/10.1145/3503221.3508417',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '1.45E+13',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Chinchilla',
      'Domain': 'Language',
      'Task': 'Language modelling',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals and Laurent Sifre',
      'Publication date': '29/3/2022',
      'Year': '2022',
      'Reference': 'Training Compute-Optimal Large Language Models',
      'Link': 'https://arxiv.org/abs/2203.15556',
      'Citations': '',
      'Inclusion criteria': 'SOTA Improvement',
      'Parameters': '7.00E+10',
      'Training compute (FLOPs)': '5.8E+23',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.05E+12',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'PaLM (540B)',
      'Domain': 'Language',
      'Task': 'Language modelling',
      'Organization(s)': 'Google Research',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev,, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta ,Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel',
      'Publication date': '04/04/2022',
      'Year': '2022',
      'Reference': 'PaLM: Scaling Language Modeling with Pathways',
      'Link': 'https://arxiv.org/abs/2204.02311',
      'Citations': '',
      'Inclusion criteria': 'SOTA Improvement',
      'Parameters': '5.40E+11',
      'Training compute (FLOPs)': '2.5E+24',
      'Training dataset': '',
      'Training dataset size (datapoints)': '5.85E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'DALL·E 2',
      'Domain': 'Drawing',
      'Task': '',
      'Organization(s)': 'OpenAI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen',
      'Publication date': '6/4/2022',
      'Year': '2022',
      'Reference': 'Hierarchical Text-Conditional Image Generation with CLIP Latents',
      'Link': 'https://cdn.openai.com/papers/dall-e-2.pdf',
      'Citations': '',
      'Inclusion criteria': 'SOTA Improvement',
      'Parameters': '3.50E+09',
      'Training compute (FLOPs)': '',
      'Training dataset': 'CLIP, DALL-E',
      'Training dataset size (datapoints)': '6.50E+08',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': 'Industry',
    },
    {
      'System': 'Flamingo',
      'Domain': 'Multimodal',
      'Task': '',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan',
      'Publication date': '29/4/2022',
      'Year': '2022',
      'Reference': 'Flamingo: a Visual Language Model for Few-Shot Learning',
      'Link': 'https://arxiv.org/abs/2204.14198',
      'Citations': '',
      'Inclusion criteria': 'SOTA Improvement',
      'Parameters': '8.00E+10',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': '',
    },
    {
      'System': 'OPT-175B',
      'Domain': 'Language',
      'Task': 'Language modelling',
      'Organization(s)': 'Meta AI',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer',
      'Publication date': '02/05/2022',
      'Year': '2022',
      'Reference': 'OPT: Open Pre-trained Transformer Language Models',
      'Link': 'https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '1.75E+11',
      'Training compute (FLOPs)': '7.6E+23',
      'Training dataset': '',
      'Training dataset size (datapoints)': '1.35E+11',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': '',
    },
    {
      'System': 'Jurassic-X',
      'Domain': 'Language',
      'Task': '',
      'Organization(s)': 'AI21labs',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon Shashua, Moshe Tenenholtz',
      'Publication date': '03/05/2022',
      'Year': '2022',
      'Reference': 'MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning',
      'Link': 'https://www.ai21.com/blog/jurassic-x-crossing-the-neuro-symbolic-chasm-with-the-mrkl-system',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '7.00E+09',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': '',
    },
    {
      'System': 'Gato',
      'Domain': 'AGI',
      'Task': '',
      'Organization(s)': 'DeepMind',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Scott Reed, Konrad Żołna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Giménez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, Nando de Freitas',
      'Publication date': '12/05/2022',
      'Year': '2022',
      'Reference': 'A Generalist Agent',
      'Link': 'https://www.deepmind.com/publications/a-generalist-agent',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '1.18E+09',
      'Training compute (FLOPs)': '5.4E+21',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': '',
    },
    {
      'System': 'Imagen',
      'Domain': 'Vision',
      'Task': '',
      'Organization(s)': 'Google Research',
      'Organization Categorization': 'Industry',
      'Author(s)': 'Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li',
      'Publication date': '23/05/2022',
      'Year': '2022',
      'Reference': 'Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding',
      'Link': 'https://imagen.research.google/',
      'Citations': '',
      'Inclusion criteria': '',
      'Parameters': '',
      'Training compute (FLOPs)': '',
      'Training dataset': '',
      'Training dataset size (datapoints)': '',
      'Hidden layers': '',
      'Inference compute (FLOPs)': '',
      'Equivalent training time (hours)': '',
      'Inference time (ms)': '',
      'Training dataset size (GB)': '',
      'Approach': '',
      'Dense or sparse model': '',
      'Training objective': '',
      'Training cost (2020 USD)': '',
      'Compute Sponsor Categorization': '',
    },
  ],
};